<!doctype html><html lang=en dir=ltr><head><title>Managing Cluster API with Kluctl</title>
<meta charset=utf-8><meta name=description content="A tutorial on how to use Kluctl to manage Cluster API based clusters.
"><meta name=viewport content="width=device-width,initial-scale=1"><meta property="og:type" content="article"><meta property="og:title" content="Managing Cluster API with Kluctl"><meta property="og:image" content="https://kluctl.io/images/blog/2024-03-13-cluster-api-kluctl.png"><meta property="og:url" content="https://kluctl.io/blog/2024/03/13/cluster-api-kluctl/"><meta property="og:description" content="A tutorial on how to use Kluctl to manage Cluster API based clusters.
"><meta property="og:image:alt" content="Managing Cluster API with Kluctl"><meta property="twitter:title" content="Managing Cluster API with Kluctl"><meta property="twitter:description" content="A tutorial on how to use Kluctl to manage Cluster API based clusters.
"><meta name=twitter:card content="summary_large_image"><meta property="article:published_time" content="2024-03-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-13T00:00:00+00:00"><meta name=author content="Alexander Block (@codablock)"><meta name=website content="https://kluctl.io/"><meta name=keywords content="Kluctl,Kubernetes,deployments"><link rel=icon href=https://kluctl.io/favicons/favicon.ico sizes=any><link rel=apple-touch-icon sizes=180x180 href=https://kluctl.io/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=512x512 href=favicons/android-chrome-512x512><link rel=icon type=image/png sizes=192x192 href=favicons/android-icon-192x192.png><link rel=icon type=image/png sizes=32x32 href=https://kluctl.io/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://kluctl.io/favicons/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://kluctl.io/favicons/site.webmanifest><link rel=stylesheet href=/scss/style.min.6e649311e3a4154385f84aa7643429e92f9251dee440cec29a40119d6d7d14c3af19ef1fab064c4e7b84f35687558b70.css integrity=sha384-bmSTEeOkFUOF+EqnZDQp6S+SUd7kQM7CmkARnW19FMOvGe8fqwZMTnuE81aHVYtw><script src=/js/bootstrap.7f02a1ed69df76defafd6ba1fc436689843464087ef358af5fd061b68c074a8e5d883c6b9c95ef38fcc9ca523db901a8.js integrity=sha384-fwKh7Wnfdt76/Wuh/ENmiYQ0ZAh+81ivX9BhtowHSo5diDxrnJXvOPzJylI9uQGo defer></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css><div id=cookie-notice><span>We would like to use third party cookies and scripts to improve the
functionality and user experience of this website.</span>
<a id=cookie-notice-accept class="btn btn-primary btn-sm">Approve</a>
<a id=cookie-notice-deny class="btn btn-primary btn-sm">Deny</a>
<a href=/privacy/ class="btn btn-primary btn-sm">More info</a></div><script>function createCookie(e,t,n){var s,o="";n&&(s=new Date,s.setTime(s.getTime()+n*24*60*60*1e3),o="; expires="+s.toUTCString()),document.cookie=e+"="+t+o+"; path=/"}function readCookie(e){for(var t,s=e+"=",o=document.cookie.split(";"),n=0;n<o.length;n++){for(t=o[n];t.charAt(0)==" ";)t=t.substring(1,t.length);if(t.indexOf(s)==0)return t.substring(s.length,t.length)}return null}function eraseCookie(e){createCookie(e,"",-1)}var _paq=window._paq=window._paq||[];_paq.push(["requireCookieConsent"]),readCookie("cookie-notice-option")=="true"&&_paq.push(["setCookieConsentGiven"]),_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//klutomo.kluctl.io/",_paq.push(["setTrackerUrl",e+"klutomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"klutomo.js",s.parentNode.insertBefore(t,s)}(),readCookie("cookie-notice-option")=="true"?localStorage.setItem("doNotTrack","0"):readCookie("cookie-notice-option")!="false"&&(document.getElementById("cookie-notice").style.display="block"),document.getElementById("cookie-notice-accept").addEventListener("click",function(){createCookie("cookie-notice-option","true",31),document.getElementById("cookie-notice").style.display="none",location.reload()}),document.getElementById("cookie-notice-deny").addEventListener("click",function(){createCookie("cookie-notice-option","false",31),document.getElementById("cookie-notice").style.display="none",location.reload()})</script><noscript><img referrerpolicy=no-referrer-when-downgrade src="https://klutomo.kluctl.io/klutomo.php?idsite=1&amp;rec=1&_action_name=Managing+Cluster+API+with+Kluctl+%7C+Kluctl+documentaion&amp;url=https%3A%2F%2Fkluctl.io%2Fblog%2F2024%2F03%2F13%2Fcluster-api-kluctl%2F" style=border:0 alt></noscript></head><body><div><header id=Navigation><nav class="navbar navbar-expand-xl"><div class=container><a class=logo aria-label=Home href=/><img src=/icons/logo.svg alt=logo>
</a><button class="navbar-toggler collapsed" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class="toggler-icon top-bar"></span>
<span class="toggler-icon middle-bar"></span>
<span class="toggler-icon bottom-bar"></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav align-items-center pl-md-5 ms-auto"><li class=nav-item><a class=nav-link href=/>Home</a></li><li class=nav-item><a class=nav-link href=/blog/>Blog</a></li><li class=nav-item><a class=nav-link href=/community/>Community</a></li><li class="nav-item ps-xl-4"><a class=nav-link aria-label="GitHub link" href=https://github.com/kluctl/kluctl><img src=/icons/github-icon.svg alt=github-icon></a></li><li class="nav-item docs-btn pt-3 pt-xl-0 ps-xl-2"><a class=nav-link href=/docs/kluctl>Docs</a></li></ul></div></div></nav></header></div><div class=mb-4 id=content><div class="py-xl-9 py-4"><div class=container><div class=row><article class="col-lg-8 docs-content offset-lg-2"><header><h1>Managing Cluster API with Kluctl</h1><div class="d-flex align-items-center mt-lg-6 mt-4"><div class=me-5><span class=fs-6>Written by</span><div class="d-flex align-items-center mt-2"><img src=/images/authors/alexander.jpg alt="Alexander Block" class="avatar avatar-xs rounded-circle"><div class=ms-2><p class="text-reset fs-6">Alexander Block</p></div></div></div><div><span class=fs-6>Published on</span><div class="mt-2 text-dark"><span class=fs-6>March 13, 2024</span></div></div></div></header><div class="blog-content main-content mt-5"><p><img src=/images/blog/2024-03-13-cluster-api-kluctl.png alt=image></p><p>Kubernetes started as a very promising container orchestrator and in my opinion it was very clear at day one that it would establish itself and take the market. What was not so obvious to me, was that Kubernetes would also morph into some kind of &ldquo;API Platform&rdquo;.</p><p>With the introduction of <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ rel=external target=_blank>Custom Resource Definitions<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>, all kinds of resources could now be managed by Kubernetes. <a href=https://book.kubebuilder.io/cronjob-tutorial/controller-overview.html#whats-in-a-controller rel=external target=_blank>Controllers and Operators<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> take these Custom Resources and use the reconcile pattern to constantly reconcile the desired state with the real world.</p><p>The next step was obvious in hindsight, but still a surprise for me personally: Why not manage Kubernetes Clusters itself from inside Kubernetes Clusters. <a href=https://cluster-api.sigs.k8s.io/ rel=external target=_blank>Cluster API<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> was born.</p><h1 id=implications-of-custom-resources>Implications of Custom Resources</h1><p>Having something in the form of a Custom Resource also means that it becomes a regular Kubernetes Resource that can be managed with all available tooling in the Kubernetes ecosystem. It can be managed with plain Kubectl, but also with more advances tools like <a href=https://helm.sh/ rel=external target=_blank>Helm<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>, <a href=https://fluxcd.io/ rel=external target=_blank>Flux<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>, <a href=https://argo-cd.readthedocs.io/en/stable/ rel=external target=_blank>ArgoCD<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> or <a href=https://kluctl.io rel=external target=_blank>Kluctl<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>.</p><h1 id=so-why-kluctl>So, why Kluctl?</h1><p>Kluctl is general purpose deployment tool for Kubernetes. It allows you to define Kubernetes deployments of any complexity and manage them via a <a href=https://kluctl.io/docs/kluctl/commands/>unified CLI</a> and/or an optional <a href=https://kluctl.io/docs/gitops/>GitOps controller</a>. Here a are a few features that make Kluctl interesting for the management of Cluster API based clusters.</p><ol><li><a href=https://kluctl.io/docs/kluctl/kluctl-project/targets/>Targets</a> allow you to manage multiple workload clusters with the same Kluctl deployment project.</li><li><a href=https://kluctl.io/docs/kluctl/templating/>Templating</a> allows you to follow a natural project structure, without the need to use overlays and patching to meet simple requirements.</li><li><a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/>Deployment projects</a> allow you to reuse parametrised and templated subcomponents without copy-paste.</li><li><a href=https://kluctl.io/docs/kluctl/templating/variable-sources/>Variable sources</a> give you easy to understand and structured configuration for the workload clusters.</li><li>The <a href=https://kluctl.io/docs/kluctl/commands/diff/>Kluctl diff</a> command will always tell you if you&rsquo;re good or not when you change things (because it&rsquo;s based on a server-side dry-run).</li><li><a href=https://kluctl.io/docs/gitops/>GitOps</a> is fully supported but also optional. It can even be <a href=https://kluctl.io/docs/kluctl/commands/gitops-deploy/>mixed</a> with a classical push style CLI.</li></ol><h1 id=installing-kluctl>Installing Kluctl</h1><p>For this tutorial, you&rsquo;ll need the Kluctl CLI installed. Please follow the instructions <a href=https://kluctl.io/docs/kluctl/installation/#installing-the-cli>here</a>. There is no need to install the GitOps controller or the Webui, but feel free to try these out as well after the tutorial.</p><h1 id=lets-setup-cluster-api>Let&rsquo;s setup cluster-api</h1><p>In this tutorial, we&rsquo;ll work completely locally without any cloud resources being involved. This means, we&rsquo;re using <a href=https://kind.sigs.k8s.io/ rel=external target=_blank>Kind<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> and the CAPD (Cluster API Docker) infrastructure provider. In the real world, you&rsquo;ll need to adapt the principles learned here to a proper Cluster API infrastructure provider.</p><p>First, lets set up a local Kind cluster. If you don&rsquo;t have Kind installed yet, read through the <a href=https://kind.sigs.k8s.io/#installation-and-usage rel=external target=_blank>installation instructions<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>.</p><p>The CAPD provider will need access to the host Docker daemon from inside the Kind cluster. To give access, you&rsquo;ll need to pass through the Docker unix socket. This can be done by using a custom Kind configuration:</p><div class=prism-codeblock><pre id=3ee0b5a class=language-yaml>
  <code># contents of kind-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  ipFamily: dual
nodes:
- role: control-plane
  extraMounts:
    - hostPath: /var/run/docker.sock
      containerPath: /var/run/docker.sock</code>
  </pre></div><p>Now create the cluster with the above config:</p><div class=prism-codeblock><pre id=5dd8ba3 class=language-bash>
  <code>$ kind create cluster --config kind-config.yaml
Creating cluster &#34;kind&#34; ...
 ‚úì Ensuring node image (kindest/node:v1.29.2) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
Set kubectl context to &#34;kind-kind&#34;
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Have a nice day! üëã</code>
  </pre></div><p>The current Kubernetes Context will be set to kind-kind, which is what we&rsquo;ll from now on use to install Cluster API to. Let&rsquo;s do that:</p><div class=prism-codeblock><pre id=3646def class=language-bash>
  <code>$ clusterctl init --infrastructure docker
Fetching providers
Installing cert-manager Version=&#34;v1.13.2&#34;
Waiting for cert-manager to be available...
Installing Provider=&#34;cluster-api&#34; Version=&#34;v1.6.1&#34; TargetNamespace=&#34;capi-system&#34;
Installing Provider=&#34;bootstrap-kubeadm&#34; Version=&#34;v1.6.1&#34; TargetNamespace=&#34;capi-kubeadm-bootstrap-system&#34;
Installing Provider=&#34;control-plane-kubeadm&#34; Version=&#34;v1.6.1&#34; TargetNamespace=&#34;capi-kubeadm-control-plane-system&#34;
Installing Provider=&#34;infrastructure-docker&#34; Version=&#34;v1.6.1&#34; TargetNamespace=&#34;capd-system&#34;

Your management cluster has been initialized successfully!

You can now create your first workload cluster by running the following:

  clusterctl generate cluster [name] --kubernetes-version [version] | kubectl apply -f -</code>
  </pre></div><p>We now have a fully functional Cluster API installation that is able to provision and manage workload clusters in the form of Docker Containers.</p><h1 id=basic-project-structure>Basic project structure</h1><p>Let&rsquo;s talk about the basic Kluctl project structure that we&rsquo;ll follow for this tutorial. You can find the full project at <a href=https://github.com/kluctl/cluster-api-demo rel=external target=_blank>https://github.com/kluctl/cluster-api-demo<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>. This repository contains multiple subdirectories with different versions of the project. The first version, as described in this and the next section, is inside <code>1-initial</code>.</p><p>The root directory will contain 2 files.</p><p>The first one is the <a href=https://kluctl.io/docs/kluctl/kluctl-project/>.kluctl.yaml</a> file, which specifies which <a href=https://kluctl.io/docs/kluctl/kluctl-project/targets/>targets</a> exists. A target defines where/what to deploy with a Kluctl project and can be anything you want. In a classical application deployment, it would be the target environment. In this case, a target represents a Cluster API workload cluster, deployed to a Cluster API management cluster (our Kind cluster). It serves as the entrypoint to configuration management and will later allow us to load cluster specific configuration.</p><div class=prism-codeblock><pre id=2301c23 class=language-yaml>
  <code># content of .kluctl.yaml
targets:
  - name: demo-1
    context: kind-kind
  - name: demo-2
    context: kind-kind

discriminator: capi-{{ target.name }}</code>
  </pre></div><p>You can also see the first use of templating here in the discriminator. The discriminator is later used to differentiate resources that have been applied to the cluster before. This is useful for cleanup tasks like pruning or deletion.</p><p>The second file is the <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/>deployment.yaml</a>, which defines the actual deployment project. It includes Kustomize deployments, Helm Charts and other sub-deployment projects.</p><div class=prism-codeblock><pre id=ab0284c class=language-yaml>
  <code># content of deployment.yaml
deployments:
  - include: clusters

commonAnnotations:
  kluctl.io/force-managed: &#34;true&#34;</code>
  </pre></div><p>This will include a sub-deployment in the directory &ldquo;clusters&rdquo;. Inside this directory, there must be another deployment.yaml. The annotation found in <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#commonannotations>commonAnnotations</a> will cause Kluctl to <a href=https://kluctl.io/docs/kluctl/deployments/annotations/all-resources/#kluctlioforce-managed>always consider</a> resources as managed by Kluctl. This is required because Cluster API <a href=https://github.com/kubernetes-sigs/cluster-api/issues/5487#issuecomment-950824947 rel=external target=_blank>claims ownership of resources<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> even though it is not in control of those.</p><div class=prism-codeblock><pre id=3b673d0 class=language-yaml>
  <code># content of clusters/deployment.yaml
deployments:
  - path: {{ target.name }}</code>
  </pre></div><p>This will include a <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#kustomize-deployments>Kustomize</a> deployment from the directory that is resolved via the template <code>{{ target.name }}</code>. &ldquo;target&rdquo; is a global variable that is always present, and it allows you to access the properties used in the current target, defined in the <code>.kluctl.yaml</code> from above. This means, if you later deploy the target &ldquo;demo-1&rdquo;, Kluctl will load the Kustomize deployment found in the &ldquo;clusters/demo-1&rdquo; folder.</p><h1 id=creating-a-workload-cluster>Creating a workload cluster</h1><p>Now, create the following files in the clusters/demo-1 directory:</p><div class=prism-codeblock><pre id=3bfae44 class=language-yaml>
  <code># contents of clusters/demo-1/kustomization.yaml
resources:
  - namespace.yaml
  - cluster.yaml
  - control-plane.yaml
  - workers.yaml</code>
  </pre></div><p>The above file is a regular <code>kustomization.yaml</code> that includes the actual resources. Kluctl fully supports <a href=https://kluctl.io/docs/kluctl/deployments/kustomize/>Kustomize</a> and all its features. You can also omit the <code>kustomization.yaml</code> in most cases, causing Kluctl to <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#simple-deployments>auto-generate</a> the kustomization.yaml. In this case however, this is not recommended as the order is important here: The namespace must be deployed before anything else.</p><div class=prism-codeblock><pre id=c904d13 class=language-yaml>
  <code># contents clusters/demo-1/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: cluster-demo-1</code>
  </pre></div><p>We create a dedicated namespace for this cluster. We will also create more namespaces later for every other cluster.</p><div class=prism-codeblock><pre id=7e4afda class=language-yaml>
  <code># contents of clusters/demo-1/cluster.yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: &#34;demo-1&#34;
  namespace: &#34;cluster-demo-1&#34;
spec:
  clusterNetwork:
    services:
      cidrBlocks: [&#34;10.128.0.0/12&#34;]
    pods:
      cidrBlocks: [&#34;192.168.0.0/16&#34;]
    serviceDomain: &#34;cluster.local&#34;
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: DockerCluster
    name: &#34;demo-1&#34;
    namespace: &#34;cluster-demo-1&#34;
  controlPlaneRef:
    kind: KubeadmControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    name: &#34;demo-1-control-plane&#34;
    namespace: &#34;cluster-demo-1&#34;
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerCluster
metadata:
  name: &#34;demo-1&#34;
  namespace: &#34;cluster-demo-1&#34;</code>
  </pre></div><p>The above file describes a <a href=https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/cluster.x-k8s.io/Cluster/v1beta1@v1.6.2 rel=external target=_blank>Cluster<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> and a <a href=https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/infrastructure.cluster.x-k8s.io/DockerCluster/v1beta1@v1.6.2 rel=external target=_blank>DockerCluster<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>. Please note that we are not using Cluster Topology (<a href=https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/ rel=external target=_blank>ClusterClass<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>) features. I will later explain why.</p><div class=prism-codeblock><pre id=2e2e9c9 class=language-yaml>
  <code># contents of clusters/demo-1/control-plane.yaml
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &#34;demo-1-control-plane&#34;
  namespace: &#34;cluster-demo-1&#34;
spec:
  replicas: 1
  machineTemplate:
    infrastructureRef:
      kind: DockerMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      name: &#34;demo-1-control-plane&#34;
      namespace: &#34;cluster-demo-1&#34;
  kubeadmConfigSpec:
    clusterConfiguration:
      controllerManager:
        extraArgs:
          enable-hostpath-provisioner: &#39;true&#39;
      apiServer:
        certSANs: [localhost, 127.0.0.1, 0.0.0.0]
    initConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cgroup-driver: systemd
          eviction-hard: &#39;nodefs.available&lt;0%,nodefs.inodesFree&lt;0%,imagefs.available&lt;0%&#39;
    joinConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cgroup-driver: systemd
          eviction-hard: &#39;nodefs.available&lt;0%,nodefs.inodesFree&lt;0%,imagefs.available&lt;0%&#39;
  version: &#34;1.29.0&#34;
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: &#34;demo-1-control-plane&#34;
  namespace: &#34;cluster-demo-1&#34;
spec:
  template:
    spec:
      extraMounts:
        - containerPath: &#34;/var/run/docker.sock&#34;
          hostPath: &#34;/var/run/docker.sock&#34;</code>
  </pre></div><p>The above file describes a <a href=https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/controlplane.cluster.x-k8s.io/KubeadmControlPlane/v1beta1@v1.6.2 rel=external target=_blank>KubeadmControlPlane<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> and a <a href=https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/infrastructure.cluster.x-k8s.io/DockerMachineTemplate/v1beta1@v1.6.2 rel=external target=_blank>DockerMachineTemplate<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> for the control plane nodes.</p><div class=prism-codeblock><pre id=a236065 class=language-yaml>
  <code># contents of clusters/demo-1/workers.yaml
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: &#34;demo-1-md-0&#34;
  namespace: &#34;cluster-demo-1&#34;
spec:
  template:
    spec: {}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: &#34;demo-1-md-0&#34;
  namespace: &#34;cluster-demo-1&#34;
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cgroup-driver: systemd
            eviction-hard: &#39;nodefs.available&lt;0%,nodefs.inodesFree&lt;0%,imagefs.available&lt;0%&#39;
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: &#34;demo-1-md-0&#34;
spec:
  clusterName: &#34;demo-1&#34;
  replicas: 1
  selector:
    matchLabels:
  template:
    spec:
      clusterName: &#34;demo-1&#34;
      version:  &#34;1.29.0&#34;
      bootstrap:
        configRef:
          name: &#34;demo-1-md-0&#34;
          namespace: &#34;cluster-demo-1&#34;
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: &#34;demo-1-md-0&#34;
        namespace: &#34;cluster-demo-1&#34;
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: DockerMachineTemplate</code>
  </pre></div><p>The above file describes everything needed to create a pool of nodes. This includes a <a href=https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/infrastructure.cluster.x-k8s.io/DockerMachineTemplate/v1beta1@v1.6.2 rel=external target=_blank>DockerMachineTemplate<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>, a <a href=https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/bootstrap.cluster.x-k8s.io/KubeadmConfigTemplate/v1beta1@v1.6.2 rel=external target=_blank>KubeadmConfigTemplate<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> and a <a href=https://doc.crds.dev/github.com/kubernetes-sigs/cluster-api/cluster.x-k8s.io/MachineDeployment/v1beta1@v1.6.2 rel=external target=_blank>MachineDeployment<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>.</p><h1 id=deploying-the-workload-cluster>Deploying the workload cluster</h1><p>We now have a working Kluctl Deployment Project that can be deployed via the <a href=https://kluctl.io/docs/kluctl/commands/>Kluctl CLI</a> (we will later also explore GitOps). Execute the following command:</p><div class=prism-codeblock><pre id=cffc261 class=language-bash>
  <code>$ kluctl deploy -t demo-1</code>
  </pre></div><p>This will perform a dry-run, show the diff and then after confirmation do the actual deployment. The dry-run will produce a few errors as the underlying server-side dry-run is not perfect in combination with Cluster API, you can ignore these errors and simply confirm.</p><p>After a few minutes, the workload cluster should be ready with one control-plane node and one worker node, all running as Docker containers. We now need to get the kubeconfig of this cluster.</p><div class=prism-codeblock><pre id=45378e4 class=language-bash>
  <code>$ kind get kubeconfig --name demo-1 &gt; demo-1.kubeconfig</code>
  </pre></div><p>You can now test access to the workload cluster:</p><div class=prism-codeblock><pre id=5cb4198 class=language-bash>
  <code>$ kubectl --kubeconfig=demo-1.kubeconfig get node
NAME                         STATUS     ROLES           AGE   VERSION
demo-1-control-plane-bjfvn   NotReady   control-plane   47m   v1.29.0
demo-1-md-0-mtcpn-wnb8v      NotReady   &lt;none&gt;          21m   v1.29.0</code>
  </pre></div><p>This will reveal that the cluster is currently not fully functional, simply because a working <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ rel=external target=_blank>CNI<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> is missing. To install a CNI, run:</p><div class=prism-codeblock><pre id=cf7b2c8 class=language-bash>
  <code>$ kubectl --kubeconfig=./demo-1.kubeconfig \
    apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml</code>
  </pre></div><p>After a few seconds, re-running the above <code>get node</code> command will show that nodes are ready.</p><h1 id=modifying-the-workload-cluster>Modifying the workload cluster</h1><p>You can now try to modify something in the workload cluster manifests.</p><p>Lets increase the workers <code>MachineDeployment</code> replicas to 2. You can do this by editing <code>clusters/demo-1/workers.yaml</code> with your favorite editor, search for the <code>MashineDeployment</code> resource and replace <code>replicas: 1</code> with <code>replicas: 2</code>.</p><p>Now, let&rsquo;s <a href=https://kluctl.io/docs/kluctl/commands/deploy/>deploy</a> this change. We will now start to see the first benefits from Kluctl, specifically the dry-run and diff that happens before we deploy something. You will need to confirm the deployment by pressing <code>y</code>.</p><div class=prism-codeblock><pre id=c1dc0ab class=language-bash>
  <code>$ kluctl deploy -t demo-1
‚úì Loading kluctl project-api-demo/1-initial
‚úì Initializing k8s client
‚úì Rendering templates
‚úì Rendering Helm Charts
‚úì Building kustomize objects
‚úì Postprocessing objects
‚úì Writing rendered objects
‚úì Getting remote objects by discriminator
‚úì Getting namespaces
‚úì demo-1: Applied 8 objects.

Changed objects:
  cluster-demo-1/MachineDeployment/demo-1-md-0


Diff for object cluster-demo-1/MachineDeployment/demo-1-md-0
&#43;---------------&#43;----------------------------------------------------------------------------------&#43;
| Path          | Diff                                                                             |
&#43;---------------&#43;----------------------------------------------------------------------------------&#43;
| spec.replicas | -1                                                                               |
|               | &#43;2                                                                               |
&#43;---------------&#43;----------------------------------------------------------------------------------&#43;
‚úì The diff succeeded, do you want to proceed? (y/N) y
‚úì demo-1: Applied 8 objects.
‚úì Writing command result

Changed objects:
  cluster-demo-1/MachineDeployment/demo-1-md-0


Diff for object cluster-demo-1/MachineDeployment/demo-1-md-0
&#43;---------------&#43;----------------------------------------------------------------------------------&#43;
| Path          | Diff                                                                             |
&#43;---------------&#43;----------------------------------------------------------------------------------&#43;
| spec.replicas | -1                                                                               |
|               | &#43;2                                                                               |
&#43;---------------&#43;----------------------------------------------------------------------------------&#43;</code>
  </pre></div><p>If you check the Cluster API management cluster, you will see that a new node will appear now.</p><div class=prism-codeblock><pre id=19f558f class=language-bash>
  <code>$ kubectl --kubeconfig=demo-1.kubeconfig get node
demo-1-control-plane-bjfvn   Ready      control-plane   12h   v1.29.0
demo-1-md-0-mtcpn-n2jdt      NotReady   &lt;none&gt;          20s   v1.29.0
demo-1-md-0-mtcpn-wnb8v      Ready      &lt;none&gt;          12h   v1.29.0</code>
  </pre></div><h1 id=add-and-remove-node-pools>Add and remove node pools</h1><p>You can also try more types of modifications. It gets especially interesting when you start to add or remove resources, for example if you add another node pool by copying <code>workers.yaml</code> to <code>workers-2.yaml</code> (don&rsquo;t forget to also update <code>kustomization.yaml</code>) and replace all occurrences of <code>md-0</code> with <code>md-1</code>. When you deploy this, Kluctl will show you that new resources will be created and actually create these after confirmation.</p><p>If you tried this, also try to delete <code>workers-2.yaml</code> again and then see what <code>kluctl deploy -t demo-1</code> will do. It will inform you about the orphaned resources, which you then can <a href=https://kluctl.io/docs/kluctl/commands/prune/>prune</a> via <code>kluctl prune -t demo-1</code>. Pruning can also be combined with deploying via <code>kluctl deploy -t demo-1 --prune</code>. We won&rsquo;t get into more detail at this point, because this will get more clear and powerful when we combine this with templating in the next section.</p><h1 id=introducing-templating>Introducing templating</h1><p>So far, we&rsquo;ve only used very static manifests. To introduce new clusters, or even node pools, we&rsquo;d have to do a lot of copy-paste while replacing names everywhere. This is of course not considered best practice and we should seek for a better way. Cluster API has an experimental feature called <a href=https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/ rel=external target=_blank>cluster classes<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> which tries to solve this problem. We&rsquo;ll however not use these in this tutorial and instead rely on Kluctl&rsquo;s templating functionality to solve the same requirements. A later section will also explain why templating is a viable alternative to ClusterClass.</p><p>The following changes to the project structure and files can also be found in the same <a href=https://github.com/kluctl/cluster-api-demo rel=external target=_blank>repository<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> already mentioned before, but inside the <code>2-templating</code> directory.</p><h1 id=preparing-some-templated-deployments>Preparing some templated deployments</h1><p>We will now introduce two reusable and templated Kustomize deployments for the cluster iteself and its workers. The cluster deployment is meant to be included once for per cluster. The workers deployment can be included multiple times, depending on how many different worker node pools you need.</p><p>Let&rsquo;s start by moving <code>kustomization.yaml</code>, <code>namespace.yaml</code>, <code>cluster.yaml</code> and <code>control-plane.yaml</code> into <code>templates/cluster/</code>. Also remove <code>workers.yaml</code> from the resources list in <code>kustomization.yaml</code>. This will be the cluster deployment.</p><p>Now, replace all occurrences of <code>demo-1</code> with <code>{{ cluster.name }}</code> in all the manifests found in the <code>templates/cluster</code> directory. Also, in the <code>KubeadmControlPlane</code> inside <code>control-plane.yaml</code>, replace <code>replicas: 1</code> with <code>{{ cluster.replicas }}</code>. This introduces some simple <a href=https://kluctl.io/docs/kluctl/templating/>Jinja2 templating</a> to inject the cluster name. The global <code>cluster</code> variable seen here will be introduced later.</p><p>Next, move the <code>workers.yaml</code> manifest into <code>templates/workers</code>. This time, there is no need for a <code>kustomization.yaml</code> as we don&rsquo;t care about deployment order here (there is no namespace involved here), which means we can allow Kluctl to <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#simple-deployments>auto-generate</a> it. Then, replace all occurences of <code>demo-1</code> with <code>{{ cluster.name }}</code> and all occurrences of <code>md-0</code> with <code>{{ workers.name }}</code>. Finally, find <code>replicas: 1</code> (or whatever you set it to before) and replace it with <code>replicas: {{ workers.replicas }}</code>.</p><p>Please note that this tutorial keeps the amount of configuration possible in these deployments to a minimum. You can maybe imagine that a lot can be achieved via templating here. For example, AWS or Azure instance types could be configured via <code>{{ workers.instanceType }}</code>.</p><p>Also, a real world example might consider putting the cluster/worker templates in seprate git repositories and including them via <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#git-includes>git</a> or <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#oci-includes>oci</a> includes. Both will allow you to implement versioning and other best practices for the templates.</p><h1 id=using-the-templated-deployments>Using the templated deployments</h1><p>The previously prepared templated deployments can now be included as often as you want, with different configuration.</p><p>For this to work, we must however change the <code>clusters/demo-1</code> Kustomize deployment to become an <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#includes>included sub-deployment</a>. Replace <code>path</code> with <code>include</code> inside <code>clusters/deployment.yaml</code>:</p><div class=prism-codeblock><pre id=5f6778a class=language-yaml>
  <code># content of clusters/deployment.yaml
deployments:
  - include: {{ target.name }}</code>
  </pre></div><p>Now, create a <code>deployment.yaml</code> inside <code>clusers/demo-1</code>:</p><div class=prism-codeblock><pre id=5f1ff02 class=language-yaml>
  <code># content of clusters/demo-1/deployment.yaml
vars:
  - values:
      cluster:
        name: demo-1
        replicas: 1

deployments:
  - path: ../../templates/cluster
  - barrier: true
  - path: ../../templates/workers
    vars:
      - values:
          workers:
            name: md-0
            replicas: 1
  - path: ../../templates/workers
    vars:
      - values:
          workers:
            name: md-1
            replicas: 2</code>
  </pre></div><p>The above sub-deployment defines some global configuration (e.g. <code>cluster.name</code>) and includes the two previously prepared Kustomize deployments. The cluster level configuration is loaded on sub-deployment level so that all items in <code>deployments</code> have access to the configuration found there. The worker specific configuration is specified in-line as part of the deployment item itself. This way, each workers item can have its own configuration (e.g. own name and replicas), which is also demonstrated here by introducing a new node pool.</p><p>You&rsquo;ll also find a <a href=https://kluctl.io/docs/kluctl/deployments/deployment-yml/#barriers>barrier</a> in the list of deployment items. This barrier ensures that Kluctl does not continue deploying worker resources before the cluster resources are applied already.</p><h1 id=deploying-the-refactored-workload-cluster>Deploying the refactored workload cluster</h1><p>Simply re-run the deploy command:</p><div class=prism-codeblock><pre id=2366203 class=language-bash>
  <code>$ kluctl deploy -t demo-1
‚úì Loading kluctl project
‚úì Initializing k8s client
‚úì Rendering templates
‚úì Rendering Helm Charts
‚úì Building kustomize objects
‚úì Postprocessing objects
‚úì Writing rendered objects
‚úì Getting remote objects by discriminator
‚úì Getting namespaces
‚úì ../../templates/workers: Applied 3 objects.
‚úì ../../templates/cluster: Applied 5 objects.

Changed objects:
  Namespace/cluster-demo-1
  cluster-demo-1/KubeadmConfigTemplate/demo-1-md-0
  cluster-demo-1/Cluster/demo-1
  cluster-demo-1/MachineDeployment/demo-1-md-0
  cluster-demo-1/KubeadmControlPlane/demo-1-control-plane
  cluster-demo-1/DockerCluster/demo-1
  cluster-demo-1/DockerMachineTemplate/demo-1-control-plane
  cluster-demo-1/DockerMachineTemplate/demo-1-md-0

Diff for object Namespace/cluster-demo-1
&#43;-------------------------------------------------------&#43;------------------------------------------&#43;
| Path                                                  | Diff                                     |
&#43;-------------------------------------------------------&#43;------------------------------------------&#43;
| metadata.annotations[&#34;kluctl.io/deployment-item-dir&#34;] | -1-initial/clusters/demo-1               |
|                                                       | &#43;2-templating/templates/cluster          |
&#43;-------------------------------------------------------&#43;------------------------------------------&#43;
| metadata.labels[&#34;kluctl.io/tag-0&#34;]                    | -clusters                                |
|                                                       | &#43;demo-1                                  |
&#43;-------------------------------------------------------&#43;------------------------------------------&#43;
...</code>
  </pre></div><p>You&rsquo;ll see a lot of changes in regard to <a href=https://kluctl.io/docs/kluctl/deployments/tags/>tags</a> and the <code>kluctl.io/deployment-item-dir</code> annotation. These are happening due to the movement of manifests and can be ignored for this tutorial. Simply confirm and let it deploy it.</p><p>You should also see that the new workers are being created. You could now try to experiment a little bit by adding more workers or removing old ones. Kluctl will always support you by showing what is new and what got orphaned, allowing you to prune these either via <code>kluctl prune -t demo-1</code> or via <code>kluctl deploy -t demo-1 --prune</code>.</p><h1 id=adding-more-clusters>Adding more clusters</h1><p>Adding more clusters is hopefully self-explanatory at this point. It&rsquo;s basically just copying the <code>demo-1</code> directory, changing the cluster name in <code>deployment.yaml</code> and adding a new target in <code>.kluctl.yaml</code>.</p><h1 id=introducing-gitops>Introducing GitOps</h1><p>If you prefer to manage your workload clusters via GitOps, the same Kluctl project can be re-used via a simple <a href=https://kluctl.io/docs/gitops/spec/v1beta1/kluctldeployment/>KluctlDeployment</a> pointing to your Git repository. We won&rsquo;t go into more detail about GitOps here, but feel free to read the documentation and try it on your own. Moving to GitOps doesn&rsquo;t mean that you have to do a full buy-in, as you&rsquo;ll always be able to mix non-GitOp related workflows with GitOps workflows. For example, a <code>kluctl diff</code> / <code>kluctl gitops diff</code> can always be used even if the same deployment is already managed via GitOps.</p><h1 id=kluctl-vs-clusterclass>Kluctl vs. ClusterClass</h1><p>You might ask why one would use Kluctl instead of simply relying on <a href=https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/ rel=external target=_blank>ClusterClass<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>, which is a cluster-api native way of achieving reusability. There are multiple reasons why I believe that Kluctl is a good alternative to ClusterClass, let&rsquo;s go through a few of them.</p><h4 id=generic-solution>Generic solution</h4><p>Kluctl is a very generic solution for templated deployments. This means, you can implement a lot of different ways and scenarios that meet different needs. If you already use Kluctl somewhere else, or consider using it somewhere else, you&rsquo;ll easily get used to managing Cluster API via Kluctl. With ClusterClass, you have to learn a new and very Cluster API specific way of templating.</p><p>I also believe that it&rsquo;s very likely that you will end up using at least some additional tool on top of the Cluster API manifests, simply because plain <code>kubectl apply -f ...</code> is not the best way to do it. Classically, this would be Kustomize or Helm. If GitOps is desired, you might also end up using Flux or ArgoCD. So, if this additional layer of tooling is already required, why not give Kluctl a try and while at it, completely avoid uses of ClusterClass with it.</p><h4 id=not-limited-to-cluster-api-resources>Not limited to Cluster API resources</h4><p>With ClusterClass, you can only glue together Cluster API related resources. A cluster might however need much more, for example an instance of <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler rel=external target=_blank>Cluster Autoscaler<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>. With ClusterClass, the only option you have is to use a <a href=https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-resource-set rel=external target=_blank>ClusterResourceSet<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> that deploys plain manifests to the workload cluster. These CRSs are however not templated, which will limit you quite a bit in what can be achieved. Also, you must use plain manifests and can&rsquo;t use Helm Charts, which means that the burden of keeping manifests up-to-date is on you. Also, CRSs only allow to deploy additional resource to the workload cluster, but not into the management cluster itself.</p><p>With Kluctl, you can use whatever resources you want for the cluster and/or worker templates. Adding Cluster Autoscaler becomes as easy as adding a <a href=https://kluctl.io/docs/kluctl/deployments/helm/>Helm Chart</a> with proper Helm values (which can also use the <code>cluster</code> configuration via templating).</p><h4 id=migrationsmodifications-to-cluster-templates>Migrations/Modifications to cluster templates</h4><p><a href=https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/change-clusterclass rel=external target=_blank>Changing a ClusterClass<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> is a risky thing and in my opinion it is crucial to have proper dry-run and diff capabilites. With ClusterClass, this is <a href=https://cluster-api.sigs.k8s.io/clusterctl/commands/alpha-topology-plan#clusterctl-alpha-topology-plan rel=external target=_blank>supported<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> to some degree but hard to use and <a href=https://cluster-api.sigs.k8s.io/clusterctl/commands/alpha-topology-plan#limitations-server-side-apply rel=external target=_blank>not 100% reliable<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a>. With Kluctl, testing changes becomes as easy as changing something and then running <code>kluctl diff -t demo-1</code>.</p><h1 id=wrapping-it-up>Wrapping it up</h1><p>If you want to try out the results of this tutorial without copy-pasing all the YAML, simply clone <a href=https://github.com/kluctl/cluster-api-demo rel=external target=_blank>https://github.com/kluctl/cluster-api-demo<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> and follow the instructions in the README.md.</p><p>For a more generic explanation of what Kluctl can do, watch <a href="https://www.youtube.com/watch?v=fJgLOyEHmN8" rel=external target=_blank>this live demo<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> at the <a href=https://www.youtube.com/@RawkodeAcademy rel=external target=_blank>Rawkode Academy<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> YouTube channel. The documentation at https://kluctl.io/docs/ is also worthwhile to read.</p><p>You can also join the projects #kluctl channel in the <a href=https://cloud-native.slack.com/ rel=external target=_blank>CNCF Slack<svg width="16" height="16" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path fill="currentcolor" d="M14 5c-.552.0-1-.448-1-1s.448-1 1-1h6c.552.0 1 .448 1 1v6c0 .552-.448 1-1 1s-1-.448-1-1V6.414l-7.293 7.293c-.391.39-1.024.39-1.414.0-.391-.391-.391-1.024.0-1.414L17.586 5H14zM5 7c-.552.0-1 .448-1 1v11c0 .552.448 1 1 1h11c.552.0 1-.448 1-1v-4.563c0-.552.448-1 1-1s1 .448 1 1V19c0 1.657-1.343 3-3 3H5c-1.657.0-3-1.343-3-3V8c0-1.657 1.343-3 3-3h4.563c.552.0 1 .448 1 1s-.448 1-1 1H5z"/></svg></a> and get in contact with existing users and maintainers.</p></div><div id=remark42></div></article></div></div></div><script type=text/javascript src=https://kluctl.io/blog/js/bundle.min.246c1542eb27e5663faafdfe8c04627ae1108d6fbce58b544391adbfb97190b7.js integrity="sha256-JGwVQusn5WY/qv3+jARieuEQjW+85YtUQ5Gtv7lxkLc=" crossorigin=anonymous defer></script></div><section id=Get-started><div class=get-started-wrapper><div class=container><div class=get-started-container><div class=get-started-content><h2>Ready to Solve Your Kubernetes Deployment Issues?</h2><div class=hero-btn><a href=/docs>Get started</a></div></div></div></div></div></section><footer id=Footer><div class=footer-wrapper><div class=container><div class=row><div class=col-lg-5><div class=footer-about><a href=https://kluctl.io/><img height=40px src=/icons/logo-white.svg alt=logo-light></a><p>Kluctl is the missing glue to put together large Kubernetes deployments.</p></div></div><div class="col-lg-7 pt-4 pt-md-0"><div class=footer-menu><h6>Links</h6><ul class=footer-nav><li><a href=/>Home</a></li><li><a href=/docs/>Docs</a></li><li><a href=/blog/>Blog</a></li><li><a href=/community/>Community</a></li><li><a href=/privacy/>Privacy</a></li><li><a href=/impressum/>Impressum</a></li></ul></div></div></div><div class=footer-copyright><p>¬© 2024 The Kluctl Authors. All Rights Reserved.</p></div></div></div></footer><script>var remark_config={host:"https://remark42.kluctl.io",site_id:"kluctl.io",components:["embed","last-comments"],max_shown_comments:100,theme:"light",page_title:"Managing Cluster API with Kluctl",locale:"en",show_email_subscription:!1,simple_view:!0,no_footer:!1}</script><script>!function(e,t){for(s=0;s<e.length;s++){var s,n=t.createElement("script"),o=".js",i=t.head||t.body;"noModule"in n?(n.type="module",o=".mjs"):n.async=!0,n.defer=!0,n.src=remark_config.host+"/web/"+e[s]+o,i.appendChild(n)}}(remark_config.components||["embed"],document)</script><script type=text/javascript src=https://kluctl.io/js/bundle.min.2b61f1a287c7bbfc90f724191495007ca8d997a79462409ab65f8906809c41f8f1de3192d511b3ad7375549c5b073dae.js integrity=sha384-K2HxoofHu/yQ9yQZFJUAfKjZl6eUYkCatl+JBoCcQfjx3jGS1RGzrXN1VJxbBz2u></script><script>const viewers=document.querySelectorAll(".image-compare");let configs=[{addCircle:!0,addCircleBlur:!1,labelOptions:{after:"Light",before:"Dark",onHover:!1},showLabels:!0,startingPoint:50},{addCircle:!0,addCircleBlur:!1,controlColor:"#3C4257",labelOptions:{after:"Life Saver",before:"Inter",onHover:!1},showLabels:!0,startingPoint:25},{addCircle:!0,addCircleBlur:!0,labelOptions:{after:"Cardinal",before:"Blue",onHover:!1},showLabels:!0,startingPoint:25}];viewers.forEach((e,t)=>{let n=new ImageCompare(e,configs[t]).mount()})</script></body></html>