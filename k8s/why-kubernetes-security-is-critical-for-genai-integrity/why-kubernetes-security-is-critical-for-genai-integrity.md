
<!--
title: Kubernetes安全：生成式AI完整性的关键基石
cover: https://cdn.thenewstack.io/media/2025/10/b05455af-security.jpg
summary: Kubernetes是生成式AI的优选平台，但76%的组织担忧其安全性。传统K8s安全工具不足以应对AI工作负载的复杂挑战，需AI感知安全方案以防数据泄露和知识产权盗窃。
-->

Kubernetes是生成式AI的优选平台，但76%的组织担忧其安全性。传统K8s安全工具不足以应对AI工作负载的复杂挑战，需AI感知安全方案以防数据泄露和知识产权盗窃。

> 译自：[Why Kubernetes Security Is Critical for GenAI Integrity](https://thenewstack.io/why-kubernetes-security-is-critical-for-genai-integrity/)
> 
> 作者：Utpal Bhatt

随着组织越来越多地采用生成式AI，Kubernetes已成为编排这些高要求工作负载的首选平台。然而，AI驱动应用程序的兴起为安全格局带来了新的复杂性。

[Cloud Native Computing Foundation (CNCF)](https://cncf.io/?utm_content=inline+mention) 在4月发布的一份报告发现，[76%的组织在运行Kubernetes时认为安全性是他们最大的担忧](https://www.cncf.io/reports/cncf-annual-survey-2024/)，其中未经授权的访问和错误配置是主要风险。

当应用于高价值的[生成式AI](https://thenewstack.io/genai-is-quickly-reinventing-it-operations-leaving-many-behind/)工作负载时，这些漏洞可能导致知识产权盗窃或数据泄露，这凸显了Kubernetes的灵活性虽然非常适合AI，但也创造了传统安全模型无法解决的关键盲点。

虽然Kubernetes提供了生成式AI应用程序所需的扩展性和弹性，但它也引入了传统安全模型未曾考虑的复杂安全和合规挑战。其动态和短暂的特性使得保持一致的可观测性和控制变得困难。

对于通常涉及敏感数据和专有模型的AI工作负载而言，这种缺乏可观测性不仅仅是性能问题。它制造了攻击者可以利用的关键盲点，尤其是在横向移动或数据外泄期间。

## 生成式AI为何在Kubernetes上运行

生成式AI工作负载，如模型训练、推理和微调，以高要求而闻名。这些管道通常涉及大量数据处理、分布式计算协调和高度动态的扩展行为。有效支持它们需要：

*   **高性能计算（尤其是GPU）：** 例如，训练像[GPT-J](https://thenewstack.io/7-guiding-principles-for-working-with-llms/)这样的大型语言模型可能需要数百个GPU并行工作数天或数周。
*   **分布式数据访问：** 考虑一个推荐引擎，它从分布在不同区域的多个来源（用户点击、购买和实时活动流）中提取行为数据。
*   **基于不可预测使用模式的弹性扩展：** 一个由生成式AI驱动的医疗保健聊天机器人可能在公共卫生事件期间需求突然激增，需要跨集群快速自动扩展。

Kubernetes提供了所有这些，使团队能够将AI工作负载部署到跨公共云、私有数据中心和边缘位置的集群中。但同样的灵活性引入了新的攻击面，并使保护环境变得更加困难。

挑战的根源在于Kubernetes的瞬态和去中心化性质。Pod不断启动和停止，服务是短暂的，网络流量，特别是服务之间的东西向流量，难以监控，甚至更难控制。这使得实时检测威胁或在集群和团队之间强制执行一致策略变得困难。

## 剖析生成式AI生命周期及其风险

### 阶段1：数据摄取和准备

此阶段涉及从外部存储库或API收集和预处理大量数据。这里的主要威胁是出口风险。如果安全控制过于宽松，敏感数据可能会被无意泄露或外泄。

出口控制必须足够细粒度，以区分合法的API调用和未经授权的外部通信。通用防火墙规则是行不通的；AI工作负载通常需要基于FQDN（完全限定域名）对OpenAI或Hugging Face等API进行选择性访问。

### **阶段2：模型训练**

模型训练是内部流量爆发的阶段。数十或数百个Pod可能协同工作以优化和验证模型，同时访问敏感数据存储。

这种横向通信为攻击者创造了绝佳机会。如果一个Pod被攻陷，攻击者可能会在集群内横向移动以访问有价值的训练数据或凭据。这使得微隔离和东西向流量监控变得至关重要。

### **阶段3：模型部署和推理**

一旦模型上线，它就成为用户和应用程序的API端点。这为OWASP风格的威胁打开了大门，例如SQL注入、提示注入或未经授权的推理请求。如果没有强大的入口控制和WAF保护，模型可能会被访问、操纵或逆向工程。

## **为何Kubernetes原生安全不足以应对**

Kubernetes确实提供了基本的安全原语，例如NetworkPolicy。但这些工具在设计时并未考虑AI。它们缺乏：

*   应用程序层感知。
*   基于FQDN的过滤。
*   跨集群的策略执行。
*   对AI特定流量模式的可观测性。

随着AI工作负载的扩展，这些差距变成了安全隐患。

例如，一个训练Pod可能需要访问外部模型存储库，但应阻止其将数据上传到未经授权的域。Kubernetes NetworkPolicy无法区分这两者。同样，Kubernetes不原生支持跨多个集群的统一策略，这使得从开发到生产难以确保一致的安全态势。

## **所需：AI感知的Kubernetes安全**

要真正保护Kubernetes上的生成式AI，安全解决方案需要超越基于IP的防火墙。它们必须了解AI管道的行为方式、它们与哪些服务通信、处理哪些数据以及如何扩展。

所需的一些关键能力包括：

*   **零信任微隔离**，将Pod通信限制在必要范围内——甚至跨不同的命名空间或租户。
*   **细粒度出口控制**，带有基于域的过滤，以防止数据外泄和保护知识产权。
*   **用于入口和出口流量的集中式网关**，以监控和控制所有外部通信点。
*   **多集群策略管理**，以确保在分布式训练、推理和开发环境中一致的执行。
*   **AI专用[可观测性工具](https://thenewstack.io/how-ai-log-analysis-is-shaping-observabilitys-future/)**，记录DNS查询、API调用和服务交互，以检测异常并支持事件响应。

## **风险太高**

最新的行业数据突显了与AI工作负载相关的风险不断升级。根据[IBM 2025年《数据泄露成本》](https://www.ibm.com/reports/data-breach)报告，13%的组织曾经历涉及AI模型或应用程序的泄露，其中97%的组织缺乏适当的AI专用访问控制。在这些事件中，60%导致数据泄露，31%导致运营中断。

随着生成式AI越来越多地为关键任务系统提供支持，从财务预测到临床决策支持，这些泄露事件的影响不容小觑。AI集成到核心工作流程中，提高了数据泄露、知识产权盗窃和法规不合规的风险。

Kubernetes可能是这场AI革命的引擎，但如果没有AI感知的安全控制，其灵活性就变成了负债。传统的Kubernetes安全工具从未被设计来保护动态的、高价值的AI管道，使得组织面临他们可能甚至没有意识到的风险。

## **前进之路：超越原生Kubernetes的AI安全**

生成式AI为企业提供了变革性的潜力，但前提是它建立在安全合规的基础之上。Kubernetes实现了这种转型，尽管并非没有权衡。突发工作负载、跨集群复杂性和敏感数据移动的结合意味着生成式AI管道需要一种新的安全模型。

通过采用超越Kubernetes原生功能的[专门为AI管道构建的安全工具](https://www.tigera.io/learn/guides/llm-security/generative-ai-security-risks/#Generative_AI_Security_with_Calico)，组织可以在不牺牲控制、可观测性或完整性的情况下安全地扩展其AI计划。