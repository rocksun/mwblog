AI adoption is surging as businesses seek to unlock fresh productivity gains. According to [McKinsey & Company](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai), 78% of business decision-makers report that their organizations use AI in at least one business function.

At the forefront of this shift is the adoption of [large language models (LLMs)](https://thenewstack.io/what-is-a-large-language-model/). Organizations are increasingly using third-party LLMs, such as [GPT](https://thenewstack.io/openai-launches-new-chatgpt-interface-designed-for-coding/) and [Claude](https://thenewstack.io/anthropics-claude-sonnet-4-model-gets-a-1m-token-context-window/), to assist with tasks like data analysis and content creation, without having to break the bank by adopting their own models. As both sanctioned and unsanctioned use of AI skyrockets, LLMs are quickly becoming business-critical systems.

One consequence of this reliance on LLMs is that organizations must ensure their continued trustworthiness. Without adequate oversight, LLMs can produce content based on outdated or biased training data that harms customer trust or damages brand reputation.

To counteract the potential harmful effects of data drift, organizations must introduce an LLM operations (LLMOps) framework. These standardized processes will tackle the challenges posed by LLMs in an enterprise context and are exemplified by four pillars:

## **1.** **Implement Clear Boundaries**

The first step in LLMOps is to set the core objectives of LLM usage.

LLMs must be kept out of high-stakes decision-making. Sensitive tasks such as pricing strategies, hiring or legal consultation should be left for a human to make the final call. Even the most advanced fine-tuned models can produce hallucinations, miss context or incorporate biases that go unnoticed until they cause problems.

In-house experts can also fine-tune the model to the business domain or have guidelines for optimal context engineering. By placing more emphasis on certain instructions or restrictions, developers can steer the accuracy and balance of responses. Weighting the right tokens or instructions reduces the ambiguity of responses and avoids common pitfalls like overly confident hallucinations.

## **2.** **Control Access and Define Use Cases**

Once correct usage is in place, user access must be regulated and use cases set aligned with company policies. Not every employee should be able to prompt the LLM with proprietary or sensitive data, especially with unvetted third-party LLMs. Assigning user permissions provides a safety net so that employees won’t accidentally expose sensitive information or misuse the model.

As with any tool adopted by an enterprise, although LLMs must have clearly defined approved use cases, we should also allow space for experimentation. The balance between approved use cases and the level of experimentation will be different for each company.

Access to sensitive business information, such as customer data, should be limited to those who only need to access it to [mitigate the risk](https://thenewstack.io/want-to-mitigate-risk-invest-in-automation/) of data breaches. Without clear guardrails, teams risk running into compliance-related or ethical issues.

## **3.** **Test Regularly to Prevent Data Drift**

It is often assumed that an LLM will continue to deliver consistent performance throughout its lifespan. In reality, the outputs generated by LLMs gradually lose relevance as their training data becomes outdated in a process known as “data drift.” An extreme example would be using an outdated version of ChatGPT like GPT-1, which only provides answers based on training data available up to pre-2018. In practice, data drift is less obvious, but it may lead teams to mistakenly use inaccurate outputs.

It is vital for organizations to test the LLMs they use for degradation over time as a result of data changes. If the model is providing inaccurate outputs, adopting a newer model or fine-tuning LLMs to respond to a specific topic or domain can further improve output accuracy without the major investment of training a foundational, proprietary model. This ensures the model aligns with the data in their environment and provides an additional layer of security against misleading outputs.

## **4.** **Monitor Performance With Real-Time Availability**

Once an LLM is in use, developers must continuously monitor its performance to ensure it meets their expectations. Performance issues, such as high latency, severely impair LLM responsiveness, which is especially problematic in time-sensitive applications such as customer support, real-time chat interfaces or incident resolution.

Monitoring dashboards that track key metrics such as latency, token usage and accuracy rates are key to ensuring LLMs remain highly performing. When LLM response times consistently exceed a defined threshold, automated alerts can flag the issue before it affects end users. Remediation actions include reviewing context to ensure an optional path to response, adjusting the model size, scaling up infrastructure or caching common responses to ensure the LLM’s ongoing performance and stability.

## **The Missing Link to AI Success**

Adopting LLMs is only part of the equation. Without a clear LLMOps strategy, organizations risk performance issues, compliance failures and reputational damage. As AI becomes more embedded in everyday workflows, clear guardrails and policies aren’t optional. They are essential to ensuring the LLM delivers optimal value.

By putting these four pillars into practice, organizations can build trust in their outputs, scale usage safely and maximize the return on their AI investments. Ultimately, a well-defined LLMOps strategy will separate organizations that lead in AI from those that fall behind.

[YOUTUBE.COM/THENEWSTACK

Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.

SUBSCRIBE](https://youtube.com/thenewstack?sub_confirmation=1)

Group
Created with Sketch.

[![](https://cdn.thenewstack.io/media/2025/04/42adea86-joaofreitas.png)

João Freitas is general manager and engineering lead for AI at PagerDuty. With about 20 years of experience in software development, machine learning and as a people manager, he was previously CTO at an AI startup and has taken several...

Read more from João Freitas](https://thenewstack.io/author/joao-freitas/)