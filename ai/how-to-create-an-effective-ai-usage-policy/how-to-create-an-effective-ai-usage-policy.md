<!--
title: 打造有效的AI使用政策
cover: https://cdn.thenewstack.io/media/2025/12/930fe9a0-create-ai-usage-policy2.jpg
summary: 企业应制定AI政策。组建跨职能团队，参考ISO标准与欧盟AI法案，分类风险。重视数据隐私，清晰沟通政策，确保政策易懂并适应各业务领域。
-->

企业应制定AI政策。组建跨职能团队，参考ISO标准与欧盟AI法案，分类风险。重视数据隐私，清晰沟通政策，确保政策易懂并适应各业务领域。

> 译自：[How To Create an Effective AI Usage Policy](https://thenewstack.io/how-to-create-an-effective-ai-usage-policy/)
> 
> 作者：Jennifer Riggins

*此为《[企业人工智能：制定和扩展人工智能战略的行动指南](https://thenewstack.io/ebooks/generative-ai/ai-for-the-enterprise-the-playbook-for-developing-and-scaling-your-ai-strategy/)》（ acclaimed tech journalist Jennifer Riggins 撰写，[Red Hat](https://www.openshift.com/try?utm_content=inline+mention) 和 Intel 赞助的新电子书）第二章的摘录。*

*从使用“双速”人工智能投资模型的优势，到衡量人工智能的实际影响，这本[现可免费下载](https://thenewstack.io/ebooks/generative-ai/ai-for-the-enterprise-the-playbook-for-developing-and-scaling-your-ai-strategy/)的免费书籍，旨在帮助企业领导者制定人工智能战略，以实现生产力提升，解决以前不可能解决的问题，并获得真正的竞争优势。*

---

大多数组织昨天就需要人工智能使用政策，然而只有大约一半的组织拥有这些政策。

“有多少产品在完全忽视其产品和平台的主要痛点的情况下，就被强行塞入了人工智能？”AI for the Rest of Us 的创始人 Hannah Foxwell 问道。“你的人工智能战略需要立足于当下的业务，而不是某些虚构的未来业务。”

为了制定你的人工智能政策，请建立一个跨职能、跨组织的人工智能赋能团队——也许设在新成立的 [首席人工智能官办公室](https://thenewstack.io/do-you-need-a-caio-the-rise-of-the-chief-ai-officer-in-2025/)。

[国际标准化组织的人工智能标准](https://www.iso.org/home.html?utm_source=the+new+stack&utm_medium=ebook&utm_campaign=Series13Book2)是开始考虑你的人工智能政策的好地方：

*   **ISO/IEC 42001 – 人工智能治理和风险管理：** 建立问责制和监督机制，设置风险管理协议以识别、评估和缓解潜在风险和影响。
*   **ISO 27001 – 信息安全：** 推荐对人员、流程和技术进行全面审查的工具。这包括隐私管理扩展。
*   **ISO 37301 – 合规和道德文化：** 定义合规管理系统的要求，以及培养人工智能使用中诚信、正直和公平文化的指南。

[欧盟人工智能法案](https://artificialintelligenceact.eu/ai-act-explorer/?utm_source=the+new+stack&utm_medium=ebook&utm_campaign=Series13Book2)提供了另一种分类人工智能工具风险的好方法：

*   **不可接受的人工智能：** 包括抓取互联网或安全摄像头以建立面部识别数据库。
*   **高风险人工智能：** 包括用于关键基础设施和公用事业、教育和就业管理的任何人工智能。
*   **通用人工智能模型：** 默认透明的人工智能系统，允许用户做出明智的决策。

这种分类可能因你业务的不同部分而异，具体取决于它们对风险的接受或规避程度。

明智的做法是，让组织中的每个人都温习一下本地和国际数据隐私法规，例如欧盟的 GDPR 和加州隐私权法案 (CPRA)。生成式人工智能 (GenAI) 工具的易用性和普及性已经导致了[意外数据泄露](https://www.metomic.io/resource-centre/the-hidden-data-leakage-crisis-how-genai-tools-compromise-enterprise-security?utm_source=the+new+stack&utm_medium=ebook&utm_campaign=Series13Book2)——你不会想成为下一个。

## 沟通你的人工智能政策

一旦你的人工智能赋能团队就人工智能使用政策达成一致，请务必将其传达给组织内外的所有利益相关者。

Grammarly 在其产品中使用人工智能已有 15 年，其[用户信任中心](https://www.grammarly.com/trust?utm_source=the+new+stack&utm_medium=ebook&utm_campaign=Series13Book2)已成为清晰人工智能沟通的行业标准。它将风险分为四个方面：

*   隐私
*   安全
*   合规
*   负责任的人工智能

“经理和领导者与他们的团队一起经历这个学习过程，所以你不能指望你的经理给出具体的建议，”Foxwell 说。“如果你想获得一致的信息，一个集中的赋能团队或外部培训伙伴是一个不错的选择。”

别忘了你的人工智能战略还必须考虑到你已经在使用的 SaaS 产品。如果它们在产品中注入了某种人工智能，你的公司是否对其合规性进行了审查？

由于这些风险和用例因领域而异，你不可能制定一项政策来统管所有。可以考虑一些总体性的[数据和安全规则](https://thenewstack.io/your-top-2026-priority-prepare-your-data-for-ai/)，然后允许主题专家和领域专家就部门特定方面发表意见。并将任何政策都以基于领域或部门的示例为基础。

当你创建这项人工智能使用政策时，请记住将其从法律术语“翻译”成通俗易懂的语言，以便所有员工都能理解。这是一个很好的生成式人工智能用例——只要有人工审核员参与其中。

---

欲了解更多信息，请立即下载《[企业人工智能：制定和扩展人工智能战略的行动指南](https://thenewstack.io/ebooks/generative-ai/ai-for-the-enterprise-the-playbook-for-developing-and-scaling-your-ai-strategy/)》！