# Upstage为何构建小型语言模型

![Featued image for: Why Upstage Builds Small Language Models](https://cdn.thenewstack.io/media/2025/01/19e82884-lucy-park-upstage-2-1024x576.png)

拉斯维加斯——Upstage是一家韩国企业AI公司，构建小型语言模型 (SLM) 以帮助公司解决文档处理问题。它最初是一家使用光学字符识别 (OCR) 为韩国大型公司扫描文档的公司。

当ChatGPT出现时，客户开始询问Upstage关于大型语言模型 (LLM) 的问题。Upstage 使用其 OCR 功能实现了 95% 的准确率，但客户希望达到 100% 的准确率。因此，Upstage团队开始寻找能够满足获得更高准确率要求的模型。LLM 具有通用性，但较小的模型更适用于文档处理所需的狭窄焦点。

小型语言模型 (SLM) 没有得到太多关注，但它们的功能包括提供公司专用甚至国家专用的LLM。

“客户想要一个适合他们自己使用的语言模型，”联合创始人兼首席产品官在AWS re:Invent的一次采访中说道。“所以这就是我们开始构建小型语言模型的原因之一。所以现在我们正在研究文档处理引擎和大型语言模型。”

## 模型融合以创建SLM

Upstage，一家AWS生成式AI加速器参与者，使用开源模型，允许在单个GPU上运行。其旗舰模型Solar，与其他在单个GPU上运行的小型模型相当，包括Llama 3.81 B、Mistral Small Instruct 2409和Hugging Face的ExaOne 3.0 7.8B Instruct。

表示Upstage将两个小型LLM的副本合并成一个大型LLM。例如，它会将一个70亿参数的模型集成到一个100亿参数的模型中。“如果我们有一个140亿参数的模型，我们会将其扩展到一个220亿参数的模型，”她说道。“这就是我们最近一直在做的。”

模型融合，一种组合LLM的技术，已在AI社区获得认可。实现包括诸如权重平均之类的实践，这是一种用不同能力的多个独立模型的参数进行合并的方法。根据南洋理工大学、东北大学和中山大学的研究人员8月份发表的一篇论文，模型融合允许数据科学家“在无需访问原始训练数据或昂贵的计算的情况下构建通用模型”。

表示Upstage已经发现使用组合模型方法可以提高其基准测试结果。根据Upstage网站，Solar Pro是一个小型语言模型，与Solar Pro预览版相比，其东亚语言掌握能力提高了64%。

SLM在语言方面的改进反映了它们日益普及的趋势。SLM训练较小的数据集，使其能够灵活地用于Upstage这样的领域中心方法。

表示大型语言模型专注于通用智能。小型语言模型也提供了更窄的焦点。

例如，Upside为泰语构建了一个特定模型。对于泰语，它类似于GPT 4，OpenAI的模型。

SLM的开发成本也低得多。假设，表示，想象一下一个构建成本为10美元的SLM。一个大10倍的LLM可能要花费100美元。

她表示，客户将采用三种选择来部署模型。如果他们是在本地部署模型，他们可以使用Upstage控制台，该控制台通过AWS市场提供API。例如，Solar Pro模型现在已在Amazon Bedrock Marketplace上提供。

[YOUTUBE.COM/THENEWSTACK 技术发展迅速，不要错过任何一集。订阅我们的YouTube频道，收看我们所有的播客、访谈、演示等等。](https://youtube.com/thenewstack?sub_confirmation=1)