# The Rise of Community-Driven Data Analysis in the Age of AI
![Featued image for: The Rise of Community-Driven Data Analysis in the Age of AI](https://cdn.thenewstack.io/media/2024/07/081e8dfa-the-rise-of-community-driven-data-analysis-in-the-age-of-ai-1024x576.jpg)
[Dana Calacci](https://x.com/dcalacci) cares about data analysis â€” but even more, about the people who are affected by it.
Or, as [Calacciâ€™s webpage](https://www.dcalacci.net/one-offs/attention-economy-1/) puts it, â€œI study how [data stewardship and analysis](https://thenewstack.io/taking-data-curation-to-a-new-level/) can impact community governance. Right now, Iâ€™m focused on how algorithmic management is changing the reality of work, how data stewardship and participatory design can help create alternate working futures, and on the data rights of platform workers.â€

Calacci provided an example in [a July article in IEEE Spectrum](https://spectrum.ieee.org/shipt) â€” a kind of cautionary tale that arrives at an inspiring conclusion.

But along the way, Calacci also gave hints on new ways of thinking about data, showing how some communities are pioneering a new approach to the ways that data is gathered â€” and then an even newer approach to how that data gets put to use.

## When Gig Workers Unite
Calacci wrote that in early 2020, gig workers for the delivery app [Shipt](https://www.shipt.com/) â€œfound that their paychecks had becomeâ€¦ unpredictable.â€ Previously, they had been paid $5 per delivery plus 7.5% of the order amount, but Shipt then added additional factors to their pay algorithm â€” including mileage driven and shopping time required.

â€œSince Shipt didnâ€™t release detailed information about the algorithm, it was essentially a black box that the workers couldnâ€™t see inside,â€ they wrote.

But instead of taking it, Calacci wrote, the workers â€œbanded together, gathering data and forming partnerships with researchers and organizations to help them make sense of their pay data.â€

![Shipt worker Willie Solits (screenshot from a video by MIT)](https://cdn.thenewstack.io/media/2024/07/0c954fed-willy-solis-screenshot-from-mit-media-lab-video-2023.png)
â€œIt was very clear that something else needed to happen â€” something above just complaining on Facebook groups,â€ says Shipt worker Willie Solits, who organized their early efforts. In a [video from MIT Media Lab](https://www.youtube.com/watch?v=naOKu92aiig), Solits recalled that â€œwe needed to organize and do something about it.â€

The workers started by taking pictures of their pay receipts and collating the data. And by that summer, Calacci was brought in to build an SMS-based tool that could scale their data collection faster. â€œBy October of 2020,â€ they wrote, â€œwe had received more than 5,600 screenshots from more than 200 workers.â€

Calacci wrote that the project was exciting because â€œit was worker-led. Itâ€™s driven by measuring the experience of workers themselves â€¦ With the help of that tool, the organized workers and their supporters essentially audited the algorithm and found that it had given 40 percent of workers substantial pay cuts.â€

But more importantly, â€œThe workers showed that itâ€™s possible to fight back against the opaque authority of algorithms, creating transparency despite a corporationâ€™s wishes.â€

## What the Shipt Episode Means
The new data led to [worker protests](https://techcrunch.com/2020/09/29/shipt-shoppers-are-organizing-a-walkout-in-protest-of-new-pay-model/) and media coverage â€” though Calacciâ€™s article concedes thereâ€™s no answer as to whether it ultimately improved their condition. â€œWe donâ€™t know, and thatâ€™s disheartening.â€

In a [discussion on Hacker News](https://news.ycombinator.com/item?id=40845628), some commenters argued that the data ultimately showed that more than half the Shipt workers *didnâ€™t* experience a cut in pay. But Calacci [responded on social media](https://x.com/dcalacci/status/1811064293460152366) that â€œthe real issue was that pay was suddenly opaque and unpredictable.â€

So regardless of the fairness of the new algorithm, Calacci thinks whatâ€™s important is the power dynamic between workers and the app.

Or, as they put it in the article, â€œIn a fairer world â€¦ this transparency would be available to workers by default.â€

And in the end, â€œour experiment served as an example for other gig workers who want to use data to organize, and it raised awareness about the downsides of algorithmic management. Whatâ€™s needed is wholesale changes to platformsâ€™ business models. â€

Happy to tell the story of how workers helped drive an audit of Shiptâ€™s algorithmic pay system in

[@IEEESpectrum]! some discussion on hacker news that I want to highlight (ğŸ§µ):[https://t.co/29Z7SERID2]â€” dr. dana calacci (@dcalacci)

[July 10, 2024]
And in [an article published in April](https://www.nelp.org/story/willy-solis-shipt-shopper/) by the [National Employment Law Project](https://www.nelp.org/), Willie Solis, a Shipt worker, wrote that â€œwe were able to prove through crowdsourcing information in our Facebook group that our tips werenâ€™t all coming to us like they should.â€

## Other Community-Sourced Data Projects
Calacciâ€™s article cited a [2021 event](https://digitalworkerinquiry.com/#about) that showcased other inspiring worker-led data projects â€” as a reminder that â€œthere are researchers and technologists who are interested in applying their technical skills to such projects.â€

And in an email interview with The New Stack, Calacci shared more examples of community-sourced data projects.

**Cornell** has a [Citizens and Technology Lab](https://citizensandtech.org/) (led by [J. Nathan Matias](https://www.linkedin.com/in/natematias/), an assistant professor of communications), created specifically to work with communities â€œto study the effects of technology on societyâ€ â€” and to test ideas â€œfor changing digital spaces to better serve the public interest.â€
In January, the labâ€™s researchers [studied](https://citizensandtech.org/research/2024-algorithm-transparency-law/) how hundreds of employers in New York City responded to a law requiring them to test their hiring algorithms for bias â€” with the help of 155 undergraduates. The results? â€œOut of 391 employers, â€ the report read, â€œ18 employers published hiring algorithm audit reports, and 13 posted transparency notices informing job-seekers of their rights.â€

But â€œmost employers implemented the law in ways that make it practically impossible for job-seekers to learn about their rights,â€ the researchers concluded, writing that the law â€œgives employers extreme discretion over compliance and strong incentives to avoid transparency.â€

**Mozilla** has an [advocacy/research team](https://foundation.mozilla.org/en/campaigns/meet-mozillas-advocacy-and-research-team/), and among its projects is a browser extension that lets people donate their data about the YouTube videos theyâ€™ve regretted watching.
In 2022 [Jesse McCrosky](https://www.linkedin.com/in/jesse-mccrosky/), a [data scientist](https://thenewstack.io/how-to-build-a-data-science-enablement-team/), teamed up for a study with [Becca Ricks](https://foundation.mozilla.org/en/research/browse-authors/becca-ricks-69/), the [Mozilla Foundationâ€™s](https://foundation.mozilla.org/) head of open source research and investigations, to analyze the data donated by 22,722 people. Their conclusion? â€œPeople donâ€™t feel they have much control over their YouTube recommendations â€” and our study demonstrates they actually donâ€™t,â€ they wrote.

â€œWe found that YouTubeâ€™s user controls influence what is recommended, but this effect is meager and most unwanted videos still slip through.â€

Their report ultimately recommended that YouTube should â€œprovide researchers with access to better tools,â€ and that policymakers should â€œpass and/or clarify laws that provide legal protections for public interest research.â€

Calacci said there are numerous additional examples in the 2020 book â€œ[Data Feminism](https://data-feminism.mitpress.mit.edu/),â€ which promised â€œa new way of thinking about data science and [data ethics](https://thenewstack.io/data-ethics-researcher-cautions-against-algorithmic-reordering-of-society/) that is informed by the ideas of intersectional feminism.â€

## Starving the LLMs
Itâ€™s a topic that Calacci takes very seriously. â€œI would love to see more researchers treat communitiesâ€™ questions and concerns as serious sources of inquiry,â€ they said in our email interview. â€œMany communities have real questions they want answered about their environment, the technology they use and how itâ€™s impacting their well-being and health.

â€œThese can be translated into rigorous research questions by treating communities as co-researchers â€” like we did with the Shipt work.â€

And Calacci believes the issue is about to become even more important. Their article ends with the possibility of even more inequity when worker-monitoring tools are powered by AI. â€œThe battles that gig workers are fighting are the leading front in the larger war for workplace rights, which will affect all of us.â€

Calacci elaborated in [an article published last December](https://interactions.acm.org/archive/view/november-december-2023/building-dreams-beyond-labor-worker-autonomy-in-the-age-of-ai) in ACM Interactions magazine, noting we were already living in a world with â€œalgorithmically determined productivity scores, tracking text communications in the workplace, and software that takes regular screenshots of workersâ€™ computer screens for employer review.â€

â€œIn the [[large language model](https://thenewstack.io/llm/)] era, all of this surveillance â€” and the data it creates â€” turns into potential training material for future AI systems.â€

But in a section titled â€œStarving the System as a Labor Strategy,â€ Calacci noted â€œa glimmer of hopeâ€ in the way LLMs need human content to train on â€” â€œfor workers seeking to exert some control over the direction of future AI model development. If workers choose to withhold their labor, they can effectively starve these systems of the data they need to improve.â€

automation keeps getting billed as an unstoppable force, but this is SO FAR from the reality. in a new piece in

[@interactionsMag], I argue that workersâ€”and researcher+advocate alliesâ€”can & should collectively help direct automation at work:[https://t.co/iXwyZIbl82][pic.twitter.com/nzgGxBjrqm]â€” dr. dana calacci (@dcalacci)

[November 16, 2023]
In our email interview, Calacci described the outlines of a larger struggle. â€œTech workers, along with creative workers more broadly, need to start considering how their data is being used to train systems that control workers in other offices and create new, valuable content through [generative AI]. Their data helps determine what AI systems can do. That means that gaining some legal control over how itâ€™s used is crucial.

â€œWorkers should try and follow in the footsteps of the WGA and SAG-AFTRA in negotiating data protection clauses into their bargaining agreements, or unionizing in order to gain the power to make one,â€ they told The New Stack. â€œWithout federal or state data protection for workers, this is the best step they can take to help control and direct how technology might impact their workplace in the future. â€

Calacciâ€™s article even lays out the potential for automation to be an *empowering* tool, calling for the development of â€œparticipatory AI and governance.â€ But, they wrote, â€œachieving these goals requires more than developing new technologies. It asks for a paradigm shift in how we approach technology in the workplace and automation as a whole.

â€œRather than treating automation as an inevitable force that workers must adapt to, we need to recognize it as a social process that can and should be shaped by those it affects most â€” workers themselves.â€

And Calacci will continue this work in their new position as an assistant professor at Penn State in human-centered AI. â€œFor me, human-centered AI means first, surfacing the impacts that new AI tools have on people,â€ Calacci said in their email.

But in addition, â€œit means creating systems that mitigate those harms, ranging from designing alternative algorithms and interfaces to deploying adversarial tools to resist or break existing AIs.â€

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)