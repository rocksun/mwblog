
<!--
title: 从虚拟机到人工智能：边缘计算的演变
cover: https://cdn.thenewstack.io/media/2024/09/fa83a360-edge-computing-evolving-vm-ai.jpg
-->

Kubernetes 是人工智能革命的核心，它允许传统虚拟机和新的 AI 应用程序在统一的环境中共存。

> 译自 [From VMs to AI: How Edge Computing Is Evolving](https://thenewstack.io/from-vms-to-ai-how-edge-computing-is-evolving/)，作者 Ben Cohen。

引用 Aerosmith 的话：我们生活在边缘。全球的企业和组织现在已经习惯了边缘计算的概念，无论是工厂车间的事件跟踪还是嵌入我们购物的零售店，地球现在都被 [边缘计算](https://www.redhat.com/en/topics/edge-computing) 所覆盖。

边缘计算的扩展暗示了使用这项技术的企业在几个方面的增长。首先，它需要传感器或物联网 (IoT) 设备。由此推断，第二项是：更多需要处理的 [数据](https://thenewstack.io/data/)。大多数边缘计算计划的全部目的是快速、本地地收集、监控和分析数据。这些工厂组件显示了它们在效率（或故障率）方面的年龄，客户可以获得专门的优惠或新的体验，而这些优惠或体验如果没有本地资源是不可能实现的。

所有这些都引出了一个问题：现在怎么办？现在传感器都已到位，数据正在流动，边缘的下一步是什么？答案肯定在头条新闻中，但边缘不是一个流行语的附加物，而是运行 [人工智能应用程序](https://thenewstack.io/ai/) 的完美场所。毕竟，人工智能非常适合分析数据，而边缘无疑会产生大量需要分析的数据。

那么人工智能和边缘如何结合在一起呢？答案实际上需要绕道而行，通过一些你可能已经非常熟悉的技术：虚拟机 (VM)。

## 边缘的挑战

是什么 [使边缘计算](https://thenewstack.io/edge-computing/) 独一无二？因为它不在数据中心的范围内，除了日常挑战之外，还有一些变量。其中一些包括：

* **非标准服务器**：有时没有空间容纳半机架（甚至 4U 节点）。
* **不可靠的网络连接**：并非每个位置都拥有稳定、快速的网络。
* **不稳定的电源**：即使电力也不能保证 24/7 不间断供应。
* **不存在 HVAC**：有些地方又热又冷、潮湿或多尘（不利于硬件）。
* **海量数据**：生成的设备和传感器用户数据量可能达到千兆字节。

这些只是你在边缘可能遇到的几个例子。当然，解决这些问题是保持数据流动的基础。

## 当今的内部挑战

技术一直在变化，但对于 IT 团队来说，一个不变的因素始终如一：在保持今天赚钱的所有旧事物的同时，平衡未来推动收入的令人兴奋的新事物。

应用程序有多种形状和大小——大/小、虚拟化/容器化/裸机，现在还有近/远。对于许多组织来说，这确实是一个混合包——毕竟，谁有资源重写每个应用程序？即使拥有容器的所有优势，虚拟机仍然非常流行，并且将在未来几年内继续存在。

使用 [混合解决方案](https://www.redhat.com/en/technologies/cloud-computing/openshift/virtualization) 可以将现代容器化基础设施的所有优势应用于现场的虚拟机。对于所有运行各种类型应用程序的混合环境，混合解决方案可以让你的 [虚拟机和容器](https://thenewstack.io/virtualization-and-containers-better-together) 并排运行，使用相同的工具和流程，并允许团队在单个应用程序平台上一起管理所有应用程序，包括裸机。

## 明天的内部挑战

很难找到 [一篇不提人工智能的最新科技博客](https://www.redhat.com/en/blog/buzzwords-unite-6-reasons-run-ai-edge)，这也不例外。现在我们所有现有的应用程序都有了归宿，让我们展望一下使用人工智能的一些新用例，这些用例可以提供新的见解或决策。无论是工厂的质量控制、零售的特殊优惠还是运输中的船上运营，快速、本地和私密的智能见解可以为现有行业带来真正的价值。

无论你对人工智能有什么想法，构建人工智能工作流、开发流程以及最终的人工智能应用都需要一些基本且不可否认的东西。这些要求甚至并不令人震惊：如果你见过一个人工智能工作流图，你基本上就都见过。唯一的区别往往是实际的软件实现。但工作流通常是相同的，并且与传统的软件开发工作流非常相似，如果你对定义稍微宽松一点的话。

- 数据存在。
- 一些软件分析这些数据并根据这些数据生成某种形式的输出。
- 评估输出的有效性。
- 软件更新，数据更新。
- 返回步骤 1。

软件开发非常相似，它也是一个反馈循环。编写的软件越多，收集的数据就越多，输出的评估就越准确，软件就越好。这是一个自我延续的循环，但如果没有稳定的数据流和软件创新，这个过程就会停滞不前。

因此，从业人员有责任将数据持续流入集群。这就是为什么 [Kubernetes](https://roadmap.sh/kubernetes) 是人工智能革命的核心：它允许所有基础设施在所有相关人员的同一思维空间中存在。

在边缘位置添加虚拟机只会增强平台的功能。随着这些边缘系统在现场摄取数据，有时需要比某些开发人员年龄更大的应用程序，虚拟机可以为那些并非完全云原生的分布式应用程序覆盖传统工作负载。Red Hat OpenShift 允许组织在一个环境中使用这些新的 AI 应用程序和传统的虚拟机，使用一组工具在一个地方。
