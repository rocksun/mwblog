
<!--
title: 具身智能伦理速成：自主性与信任的终极较量
cover: https://cdn.thenewstack.io/media/2025/10/50a708e8-jakub-zerdzicki-wd7s-lz12es-unsplash-scaled.jpg
summary: AI自主性与可信度需平衡。理解AI自主光谱，构建可信赖AI的六支柱。通过风险评估、设计信任、渐进自主、持续监控及跨职能团队实现最佳实践。
-->

AI自主性与可信度需平衡。理解AI自主光谱，构建可信赖AI的六支柱。通过风险评估、设计信任、渐进自主、持续监控及跨职能团队实现最佳实践。

> 译自：[An Ethics Crash Course for Agentic AI: Autonomy Versus Trust](https://thenewstack.io/an-ethics-crash-course-for-agentic-ai-autonomy-versus-trust/)
> 
> 作者：Vrushali Sawant, Manisha Khanna

今天，领导者和技术从业者都发现自己正驾驶着强大的AI引擎。这些“车辆”最热门的新功能之一是什么？那就是智能体AI的自动驾驶能力。

随着每一次新功能的推出都比上一次提供更强的自主能力，各个层级的团队都应该在[最佳自主性与信任](https://thenewstack.io/why-trust-and-autonomy-matter-for-cloud-optimization/)策略上保持一致，就像他们建造汽车时一样。AI车辆应该是自动驾驶的、拥有高级驾驶辅助系统，还是依赖人类驾驶方法？组织又该如何衡量每种选择对所有利益相关者的可信度呢？

制定这些最佳[实践对于AI的可持续部署至关重要](https://thenewstack.io/10-essential-practices-for-building-software-for-cloud-scale/)。AI自主性与可信度之间的平衡不仅影响技术实现，还应塑造组织构建AI系统的方式。然后，所有人都可以自信地将这种平衡整合到运营中，以符合法规要求。

首先，了解AI自主性、AI可信度以及如何实现两者的平衡以校准组织进行负责任的创新会有所帮助。系好安全带，我们开始吧。

## **AI自主性光谱中的循环**

在确定最合适的AI自主性级别并制定其部署策略时，请将自主性本身视为一个连续体。正如自动驾驶汽车以不同程度的独立性运行一样，AI系统也存在于一个自主性光谱中。

在该光谱的一端，“人在回路（human-in-the-loop）”系统提供被动辅助和建议，同时让人类控制最终决策。例如，考虑那些标记可疑交易以供人工审查的欺诈检测系统。这就像汽车中的车道偏离警告或[盲点监测](https://thenewstack.io/cloud-monitorings-blind-spot-the-user-perspective/)等功能。

在中间，“人在环中（human-on-the-loop）”系统在人类监督下自主执行任务，同时保持监督机制。高级驾驶辅助系统就是这种方法的例证：AI处理日常驾驶任务，就像巡航控制一样，而人类则保持监督控制。

在该连续体的另一端，自主或“人外回路（human-out-of-the-loop）”系统在定义参数内独立运行，无需实时人工干预即可做出决策。考虑算法交易系统、自主无人机、受控环境中的自动驾驶汽车和先进制造机器人。

## **工程化可靠的自主性**

团队还需要审视其组织是否能够可靠地在生产环境中部署AI系统。可信赖AI的六大支柱是必不可少的“安全功能”——就像汽车中的那些功能一样——从一开始就被设计到系统中。这些包括：

1. **算法公平性和偏见缓解**，跨越不同人群和使用场景，就像汽车中的高级制动系统或精密传感器，确保一致和公正的性能。
2. **透明度和可解释AI**，因为组织、利益相关者以及国内外法律日益要求AI系统分解决策过程。这类似于一个全面的汽车诊断系统。
3. **生产环境中的可靠性和鲁棒性**，以确保系统在各种条件下（包括边缘案例和网络攻击）都能持续稳定运行，就像汽车坚固的发动机一样。
4. **清晰的问责框架**，包括所有权结构、错误处理程序和符合法规要求及内部政策的合规机制，类似于汽车所有权或驾驶执照记录。
5. **优先考虑数据安全和保障**，通过隐私设计和网络安全措施保护敏感数据，就像一个上锁的存放个人物品的储物箱。
6. **以人为本**，即设计AI系统以促进人类福祉、维护人类能动性并促进公平，这类似于优先考虑驾驶员舒适度和安全的人体工程学汽车设计。

## **融合信任与自主性以驱动AI引擎**

在选择AI系统时，请记住，每种实现都需要不同级别的监督。这包括确定复杂或黑盒技术带来的性能提升是否足以抵消可解释性的降低，尤其是在受监管的行业中。

最强大的实现，例如深度学习、[自然语言处理](https://thenewstack.io/lambeq-a-toolkit-for-quantum-natural-language-processing/)和生成式AI，都伴随着更高的风险。AI智能体代表了最高的自主性级别和巨大的自动化潜力。它们也带来了最复杂的可信度挑战。

风险缓解——包括特定行业和基于用例的方法、严格的测试协议、验证流程和内容审查——是领导者和从业者工具包中最有效的工具之一，用于创建足够的防护措施。

## **驾驭信任与自主性的5大最佳实践**

当全力以赴时，平衡信任与自主性会是怎样的景象？它包括组建一支世界级的AI伦理“维修团队”，并在每一个转折点都谨慎地、通过风险评估来应对高风险决策。

1. **通过情境驱动的风险评估进行更好的制动**

领导者应优先考虑将自主性水平与应用关键性相匹配的AI部署策略。消费者推荐系统可以容忍更高的自主性，同时对可信度进行适度监督；而医疗保健或金融[应用则需要广泛的验证](https://thenewstack.io/progress-360-how-panera-fixed-its-application-validation-gap/)和人工监督。

2. **实施信任设计（trust-by-design）方法**

将可信度要求整合到AI开发生命周期的从概念到部署的各个阶段。这包括建立[数据治理协议](https://thenewstack.io/how-bluesky-was-influenced-by-scuttlebutt-a-p2p-protocol/)、实施偏见检测机制以及创建与业务需求相符的可解释性要求，同时不断追问：目的是什么？最终目标是什么？谁可能因此失败？

3. **通过增量自主性扩展谨慎加速**

对于高风险应用，从“人在回路”实现开始，随着系统在生产环境中证明其可靠性和可信度，逐步增加自主性。这种方法使组织能够在最大程度降低风险的同时建立信心。

4. **目光聚焦仪表盘：持续监控和治理**

整合全面的AI监控系统，跟踪性能指标、检测异常并识别新出现的偏见。治理框架需要包括定期审计、性能评估和更新程序，以长期保持可信度。

5. **组建AI伦理专家跨职能团队**

组建多学科团队，包括技术专家、领域专家、法律顾问和伦理专业人士，以指导AI部署决策并确保与组织价值观和法规要求保持一致。

## **驾驭可信赖AI时代**

就像自动驾驶汽车一样，AI智能体以高度独立性运行，这导致了共享且常常模糊的责任。随着自主性的增加，除非明确界定责任，否则信任就成为一个盲点。

智能体AI最重大的争议之一是为自主行为分配责任。当一个高自主性AI智能体做出导致不良结果的决策或行动时，无论是错误、伤害还是违法行为，确定最终责任变得棘手。谁应该承担责任：开发者、部署者、用户还是其他人？

缺乏明确的责任会造成问责真空，侵蚀公众信任，并使组织陷入伦理困境和法律麻烦。

最终，AI的部署策略应平衡更高的自主性与更强的可信度控制。如果AI被赋予高度独立性，它就需要高水平的治理、透明度、对边缘案例的严格测试以及明确的责任模型。最关键的[应用需要最多的人工监督，而低风险应用可以在受监控的](https://thenewstack.io/why-upgrade-to-observability-from-application-monitoring/)自由度和更高的自主性下运行。

最重要的是，领导者必须抵制为了追求竞争优势而将AI做得尽可能智能的诱惑。相反，在人类退出驾驶座之前，每一个战略决策都应以AI必须有多可信以及谁将承担责任为依据。通过设置防护措施，领导者可以使组织安全且战略性地向前发展。