
<!--
title: 大规模RAG：打破精度与时延的取舍魔咒
cover: https://cdn.thenewstack.io/media/2025/10/a3e73903-data1-2.jpg
summary: RAG系统通过多阶段排名、分层检索和语义分块技术，有效平衡了精确度、速度和可扩展性，优化了检索效率和输出质量。
-->

RAG系统通过多阶段排名、分层检索和语义分块技术，有效平衡了精确度、速度和可扩展性，优化了检索效率和输出质量。

> 译自：[Eliminating the Precision–Latency Trade-Off in Large-Scale RAG](https://thenewstack.io/eliminating-the-precision-latency-trade-off-in-large-scale-rag/)
> 
> 作者：Bonnie Chase

[检索增强生成 (RAG)](https://thenewstack.io/why-rag-is-essential-for-next-gen-ai-development/) 系统持续面临一个权衡：精确的结果通常意味着更高的延迟和成本，而更快的响应则有丢失上下文和准确性的风险。解决方案并非选择其一，而是重新设计检索。让我们探讨三项共同消除这种权衡的技术：多阶段排名、分层检索和语义分块。

当它们结合在一起时，将创建一个平衡速度、可扩展性和精确度的检索栈。

## 多阶段排名：结果的增量优化

排名的核心在于检索。对整个候选集运行深度机器学习 (ML) 排名会增加延迟并提高基础设施成本。另一方面，仅靠轻量级评分方法无法捕获足够的上下文，从而导致精确度下降。这就是为什么多阶段方法提供了一种平衡的替代方案。

多阶段排名不是在昂贵的深度模型或快速但肤浅的启发式方法之间进行选择，而是将评分从廉价到昂贵分阶段进行。轻量级过滤器（词法、近似最近邻或ANN）迅速修剪候选池，而逐步更重的ML功能仅应用于顶部结果。这在保持精确度的同时，控制了延迟和计算。

多阶段排名提供了一种平衡的替代方案：

*   **阶段1：** 使用关键词匹配或ANN搜索进行快速过滤。
*   **阶段2：** 使用密集嵌入、混合相似性度量或自定义排名表达式进行重新排名。
*   **阶段3+：** 高级机器学习模型、个性化信号或领域特定评分规则。

这种分阶段优化确保了昂贵的模型仅在它们能带来最大价值的地方应用。

优点包括：

*   **成本敏感的精确度：** 在不同阶段战略性地分配计算资源。
*   **混合逻辑：** 融合符号规则、语义相似性和行为数据。
*   **个性化：** 根据个体用户或会话调整结果。

通过在大规模搜索和推荐中借鉴这些最佳实践，多阶段排名使RAG系统能够在不超出延迟预算的情况下提供准确的结果。

## 分层检索：排名质量的基础

即使是最复杂的多阶段排名栈也无法弥补糟糕的检索单元或嘈杂的输入。排名质量在很大程度上取决于您选择的检索单元：

*   **细粒度分块**（段落或滑动窗口）可最大限度地提高召回率，因为即使是短查询也可能匹配。但它们也带来了权衡：
    *   **上下文碎片化：** 关键信号被分割到不同的分块中。
    *   **冗余：** 重叠的分块会膨胀索引大小并导致重复。
    *   **下游负担：** 排名和[大型语言模型 (LLM)](https://thenewstack.io/what-is-a-large-language-model/) 必须将碎片化的证据拼接在一起，从而增加token使用量和延迟。
*   **整文档检索** 保留了全局上下文并减少了冗余，但往往牺牲了精确度。大量不相关的文本被拉入提示，稀释了相关性信号，增加了token成本，并降低了重新排名的效果。

精心设计的检索策略通常介于两者之间：定义一个语义检索单元，该单元捕获足够的局部上下文以实现自包含，同时保留下游排名可以利用的结构化元数据（标题、章节、时间戳）。这种平衡确保排名在高质量候选之上运行，最大限度地减少了计算浪费，并最大化了馈送给LLM的信噪比。

分层检索通过结合这两个层面的相关性来实现这种平衡：

1.  对最相关的文档进行排名和选择。
2.  在这些文档中，仅检索前K个分块。

这种分层过程保留了文档级信号的更广泛上下文，同时缩小到重要的特定范围。

优点包括：

*   减少token使用量和更低的提示成本。
*   为LLM提供更清晰、更连贯的上下文。
*   在不牺牲召回率的情况下提高精确度。

## 语义分块：精确度始于预处理

最后，检索质量取决于您如何索引数据。存储为整体的冗长文档通常会产生嘈杂的检索，因为只有部分内容与给定查询相关。

语义分块通过将文档拆分为有意义、自包含的单元（如段落或逻辑章节）来解决此问题，同时保留上下文元数据（如标题、作者或时间戳）。

优点包括：

*   **更高的召回率：** 文档的更细粒度入口点。
*   **更好的精确度：** 在查询时可以排除不相关的部分。
*   **元数据丰富：** 支持符号过滤和下游排名。

分块可能会增加索引大小并需要仔细的提示组装，但当与分层检索和多阶段排名结合使用时，它将成为精确度的强大基础。

## 构建用于RAG的生产级检索栈

这三项技术共同解决了扩展RAG的最大痛点：

*   因包含过多内容而导致的过长提示。
*   孤立分块导致的上下文碎片化。
*   忽略领域逻辑的僵化排名管道。

因此，一个强大的检索栈应该：

*   通过语义分块索引文档，同时保留元数据。
*   通过分层检索进行分层检索。
*   通过多阶段排名高效地优化结果。

这种组合能够实现更准确、更经济高效、更值得信赖的LLM输出，特别是当与检索感知提示工程结合使用时。

## 最终思考

随着[RAG系统扩展](https://thenewstack.io/a-blueprint-for-implementing-rag-at-scale/)，检索设计成为关键的差异化因素。[超越简单的向量或ANN搜索](https://thenewstack.io/beyond-vector-search-the-move-to-tensor-based-retrieval/)，引入多阶段排名、分层检索和语义分块，将显著提高效率和输出质量。

Vespa旨在处理企业规模的这些检索挑战。其张量原生架构直接在集群中支持多阶段排名、分层检索和语义分块，消除了外部瓶颈和昂贵的变通方案。通过在数据所在位置运行检索和排名，Vespa能够以低延迟、高精确度在数十亿文档和每秒数千次查询中提供结果。

无论您是构建知识助手、研究代理还是大规模生产RAG系统，Vespa都提供检索基础，使生成式AI保持准确、高效并随时准备扩展。

了解[Vespa 如何为 Perplexity 提供检索能力](https://vespa.ai/perplexity/)。