# 强化学习先驱荣获 ACM 图灵奖

![Featued image for: Reinforcement Learning Pioneers Honored With ACM Turing Prize](https://cdn.thenewstack.io/media/2025/03/65b7ed30-gemini_generated_image-1024x768.jpg)

两位研究人员早期在强化学习方面的理论工作于周三获得认可，[美国计算机协会](https://www.acm.org/about-acm) 宣布 [Andrew G. Barto](https://people.cs.umass.edu/~barto/) 和 [Richard S. Sutton](http://incompleteideas.net/) 两位研究人员荣获 2024 年 [ACM A.M. 图灵奖](https://amturing.acm.org/)。

![ACM Logo](https://cdn.thenewstack.io/media/2025/03/9711b7f5-acm_logo_tablet.png)
ACM

两位研究人员对于开发[强化学习](https://thenewstack.io/reinforcement-learning-ready-real-world/)的概念和算法基础至关重要，强化学习是当前[基于 AI 的代理技术](https://thenewstack.io/ai-agents-a-comprehensive-introduction-for-developers/)的基石。

他们将共同获得 100 万美元的奖金（由 [Google](https://cloud.google.com/?utm_content=inline+mention) 赞助），以表彰他们的辛勤工作。

ACM A.M. 图灵奖通常被称为“计算机领域的诺贝尔奖”，以英国数学家 [Alan M. Turing](https://thenewstack.io/happy-birthday-alan-turing-also-sorry/) 的名字命名，他阐明了计算的数学基础，并创造了 [图灵测试](https://www.coursera.org/articles/what-is-the-turing-test)，这是一个用于评估机器是否已实现类人智能行为的思想实验（以及当前基准）。

因此，今年的奖项非常适合其同名者。

Google DeepMind 首席科学家 [Jeff Dean](https://www.linkedin.com/in/jeff-dean-8b212555/) 在一份声明中指出：“Alan Turing 在 1947 年的一次演讲中表示，‘我们想要的是一台可以从经验中学习的机器。’” “由 Barto 和 Sutton 开创的强化学习直接回答了 Turing 的挑战。他们的工作是过去几十年人工智能进步的关键。”

Barto 是马萨诸塞大学阿默斯特分校信息与计算机科学荣誉退休教授。Sutton 是阿尔伯塔大学的计算机科学教授，也是 [Keen Technologies](https://keenagi.com/)（“John Carmack 的 AGI 努力”）的研究科学家，以及 [阿尔伯塔机器智能研究所](https://www.amii.ca/)的研究员。

### 完全代理
强化学习受到神经科学甚至心理学思想的启发，构成了[代理 AI](https://thenewstack.io/agentic-ai-is-the-new-web-app-and-your-ai-strategy-must-evolve/) 的基础，或者说是计算机实体感知和行动的基础，最好是以满足用户意图的方式行动。为此，代理依赖于“奖励”，或对其行为质量的反馈。

Barto 和 Sutton 开发了强化学习的许多基础知识，并在 1998 年的开创性教科书“[Reinforcement Learning: An Introduction](https://amzn.to/3DmD6j5)”中分享了他们的学习成果。

这项工作建立在 [马尔可夫决策过程](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-markov-decision-process/) (MDP) 的基础上，其中代理在随机环境中做出决策，并在每次行动后获得奖励信号，目标是最大化其奖励。

MDP 假设代理了解其环境。强化学习更进一步，假设代理对环境或其奖励一无所知。

“[ACM 公告](https://www.acm.org/media-center/2025/march/turing-award-2024) 总结道：“强化学习的最小信息要求，加上 MDP 框架的通用性，使得强化学习算法可以应用于范围广泛的问题。”

两人是第一个发现神经网络可以代表学习函数，并且代理可以将学习和规划结合起来的人。然后，获得环境知识可以成为规划的基础。

两人开创的其他一些技术——与其他研究人员合作——包括[时间差分学习](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html)，这有助于解决奖励预测问题，以及[策略梯度方法](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)，以解决强化学习不足的那些高维动作空间。

## 成功的应用
强化学习在 2016 年和 2017 年通过 [AlphaGo 计算机程序](https://thenewstack.io/alphagos-win-human-go-champion-means-ai/) 击败了最优秀的人类 *围棋 * 选手，从而获得了首次重大胜利。

源自 AlphaGo 的 AI 系统已被调整以解决其他问题。2022 年，研究人员使用其中一个系统发现了一种称为矩阵乘法的基本数学任务的新算法。
[https://t.co/9Yku0j8C6H](https://t.co/9Yku0j8C6H) [pic.twitter.com/pjpeBczc1M](pic.twitter.com/pjpeBczc1M) — Quanta Magazine (@QuantaMagazine)

[2025年3月5日]

OpenAI 的 [ChatGPT](https://thenewstack.io/how-to-build-web-components-using-chatgpt/) 同样也将其成功归功于强化学习。根据 ACM 的说法，为了训练其大型语言模型，该服务使用了一种称为“从人类反馈中进行强化学习 (RLHF)”的技术来捕捉人类的期望。

[技术发展迅速，不要错过任何一集。订阅我们的 YouTube 频道，即可观看我们所有的播客、访谈、演示等。](https://youtube.com/thenewstack?sub_confirmation=1)