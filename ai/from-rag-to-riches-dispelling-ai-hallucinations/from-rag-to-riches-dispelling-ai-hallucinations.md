<!--
title:  从RAG到财富：驱散人工智能的幻觉
cover: https://cdn.thenewstack.io/media/2023/11/06db055c-ai-hallucination-rag-1024x688.jpg
-->

检索增强生成被认为是使AI模型改善数据、减轻幻觉的最有前途的技术之一。

> 译自 [From RAG to Riches: Dispelling AI Hallucinations](https://thenewstack.io/from-rag-to-riches-dispelling-ai-hallucinations/)，作者 Rahul Pradhan 拥有超过16年经验，是 Couchbase 的产品和战略副总裁。

Generative AI（GenAI）和大语言模型（LLM）毫无疑问是2023年最热门的科技，而这种势头在2024年及以后也不会减缓。企业将继续投资数十亿美元用于这些技术，富裕的组织将沉溺于[并购](https://www.gartner.com/en/newsroom/press-releases/2023-11-15-gartner-announces-top-trends-in-mergers-and-acquisitions-for-2024)狂潮，以确保他们处于创新的前沿。

作为一种商业工具，GenAI完全合乎逻辑 —— 它可以使员工更加高效，增加理解力和技能，并开拓新的机会。组织对人工智能的依赖增加的危险在于你需要相信它做出正确的决策的能力。没有这一点，组织可能会花费大部分人工智能投资来对每个提示和答案进行双重甚至三重检查，以确保它们是可信的。此外，[人工智能很容易陷入幻觉](https://thenewstack.io/deep-learning-neural-networks-google-deep-dream/)，导致混淆组织或使它们完全走上错误的道路。

## 宏伟的幻觉

[LLM是概率引擎](https://thenewstack.io/what-is-a-large-language-model/)，分析输入和可用数据，然后计算在回复中下一个单词（或单词序列）应该是什么。这是一种双刃剑的方法。它使组织能够用自然、易懂和语法正确的语言回答可能涉及任何主题的查询。

然而，归根结底，大语言模型（LLMs）是在打赌。如果它们的学养 —— 以及它们学习和使用的数据集 —— 不能匹配一个查询，那么它们唯一的选择就是虚张声势。答案看起来准确，而且信心十足，但它并不基于现实或任何可以添加上下文的学到的知识。对于需要根据事实证据做出业务决策并遵循最佳实践的组织而言，这极大地降低了人工智能的可信度，从而降低了其有效性。

## 幕后

人工智能幻觉是由各种因素引起的，但归根结底，问题在于，虽然一个人拥有一生的知识和经验可供借鉴，但人工智能模型只能像它们的数据集一样聪明。

例如，导致人工智能幻觉的最常见挑战之一是**数据稀疏性**。如果数据集存在缺失或不完整的值，人工智能将别无选择，只能填补这些空缺。一个人将具有处理这一情况的上下文、判断和批判性思维能力，但人工智能可能轻易得出不准确的结论。例如，即使它从未看过他的任何电影，大多数人都会认为汤姆·汉克斯是一位优秀、甚至是伟大的演员。然而，只有少数表演在其数据集中的人工智能可能会得出相反的结论。

与缺失数据相关的是不正确的数据。导致信息被错误分类或标记的低**质量数据**，或者人工智能（AI）从不可靠的来源学习，都可能导致AI无意中传播错误信息。这不仅仅是分享单一不准确的事实的情况；例如，[声称](https://petapixel.com/2023/02/08/googles-bard-ai-chatbot-shares-false-photo-fact-in-its-debut/)詹姆斯·韦伯太空望远镜在发射前17年拍摄照片。无法交叉参考相关数据或理解偏见会导致越来越不准确的结论 —— 比如使用[不具代表性的医疗数据](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(21)00252-1/fulltext)来预测、检测和治疗皮肤癌。

最后，还有AI模型的培训方式的问题。如果训练数据中没有足够的样本使模型能够概括，如果存在太多不相关的“噪音”数据，如果模型在单一样本数据集上训练时间过长，或者如果模型过于复杂以至于从不相关和相关数据中学到，结果就是**过拟合**。AI模型在其训练样本上运行良好，但在真实世界中具有极差的模式识别能力，导致不准确和错误。

## 打破魔咒

[消除人工智能的幻觉](https://thenewstack.io/how-to-reduce-the-hallucinations-from-large-language-models/)是确保其充分发挥潜力的关键。解决数据稀疏性、质量和过拟合是关键的第一步。与任何其他业务功能或员工一样，企业不能指望人工智能在没有正确信息和培训的情况下有效运行。微调或重新训练模型也有助于生成相关、准确的内容。问题在于，如果没有持续的培训，数据可能会过时。所有这些都可能意味着巨大的成本和延迟的投资回报。

[提示工程](https://thenewstack.io/prompt-engineering-get-llms-to-generate-the-content-you-want/)是避免幻觉的另一种方法，并且迅速成为预期的人工智能技能之一。然而，这带来了确保模型始终接收高度描述性提示和额外培训投资的负担。

## 跳出数据集的盒子

理想情况下，通过适当的帮助，人工智能模型应该能够改进其数据并减轻幻觉。[检索增强生成（RAG）](https://thenewstack.io/improving-chatgpts-ability-to-understand-ambiguous-prompts/)是实现这一目标最有前途的技术之一。通过根据需要从外部来源获取数据，RAG人工智能框架为大语言模型提供了它们改进回应所需的重要上下文，关键的是，避免了幻觉。

为了帮助应用程序如虚拟助手、聊天机器人和其他内容创作者生成精确、相关的回应，组织需要确保检索增强生成（RAG）能够提供引用多个信息源和深刻理解上下文的能力。与任何人工智能应用一样，这是一个信任的问题 —— 通过从相关、可靠且最新的来源提取信息，并为用户提供对这些来源的访问，RAG有助于消除人们对人工智能可靠性的疑虑。

迫切的是，RAG需要访问实时数据，以确保所有信息尽可能当前、完整且准确。例如，在销售旺季期间，任何旨在向用户提供最佳、最个性化产品优惠的应用程序或聊天机器人，除非能够根据每个用户的个人资料和用户会话上下文定制建议，否则将毫无价值。此外，它需要访问实时数据以获取价格的动态变化，以制定最佳的用户优惠。毕竟，没有人想要已经购买的商品的优惠，发现他们的推荐并不是在正确的时间以正确的价格购买正确的产品，或者因为其他地方提供的价格更低而支付过高的费用。

此外，检索增强生成（RAG）应与操作数据存储结合，以提高其效果。为了有效地查询数据，数据需要存储在高维数学向量中，使模型能够使用数值向量而不是特定术语或语言进行搜索。然后，人工智能可以在正确的上下文中找到相关信息，而无需依赖于找到相同的术语。通过使用支持向量的高效存储和搜索，并且可以将模型的查询转换为这些数值向量的数据库，人工智能模型可以实时保持其理解的最新状态：始终学习，始终适应，极大地降低了过时或不完整信息导致昂贵幻觉的机会。
