# AI Agents Must Learn From ChatGPT’s Data Wrongs
![Featued image for: AI Agents Must Learn From ChatGPT’s Data Wrongs](https://cdn.thenewstack.io/media/2025/03/e35b1022-gustas-brazaitis-xnky-cu20d4-unsplash-1024x683.jpg)
[Gustas Brazaitis](https://unsplash.com/@gustasbrazaitis?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)on
[Unsplash](https://unsplash.com/photos/turned-on-macbook-pro-xNKy-Cu20d4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
Large language models (LLMs) set a dangerous data precedent, entering the age of artificial intelligence (AI). ChatGPT and other generative platforms train on information without user consent or compensation, creating significant copyright and ownership issues. The output is “new,” but the input has been copied and pasted from undisclosed sources.

This is a data rights issue that we must address on the eve of AI agents. Despite promising to become superhuman helpers in personal and professional tasks, agents won’t be worthy of trust if we continue to create black boxes with little regard for intellectual property.

Instead, especially in these early days, we [must prefer infrastructure](https://thenewstack.io/why-infrastructure-must-be-serverless-in-the-ai-age/) that tracks information, recognizes input, and rewards contributors. This is how we learn from ChatGPT’s data wrongs and enable the new wave of agents to operate with verification, permission, and privacy at their core.

**LLMs Set a Dangerous Data Precedent**
Even when the content generated by Claude, ChatGPT, or Gemini feels original, it’s actually scraped from billions of data points without explicit permission or subsequent compensation to the owners. These platforms essentially take copyrighted materials, move ahead without consent, and fail to attribute sourcing.

To make matters worse, we usually don’t know how these models make decisions. They’re closed [source with data](https://thenewstack.io/decoding-kafka-why-its-worth-the-complexity/) going in, commands coming out, and zero transparency about what happens in between. This black box [approach creates both ethical and practical](https://thenewstack.io/microsoft-takes-practical-approach-to-kubernetes-management/) problems.

I’ve previously compared models to humans in that they are what they eat. If we only eat junk food, we’re slow and sluggish. If agents only consume copyright and second-hand material, they’re inaccurate, unreliable, and general rather than specific. Their [data “diet”](https://crypto.news/ai-agents-need-better-data-diets-opinion/) determines performance and we can’t expect quality outputs from systems built on problematic inputs.

A new era [needs a new approach](https://thenewstack.io/why-you-need-a-centralized-approach-to-monitoring/). [AI agents have a chance to bake in data](https://thenewstack.io/agentic-ai-for-enterprises-4-key-benefits-driving-innovation/) rights from day one by leveraging blockchain to track information and strong data infrastructure to dictate its use. By [building data provenance and respect into the foundations](https://thenewstack.io/distributed-data-not-apps-build-the-foundation-for-web3/), we can arm agents with consented information and bring users in on the value it generates.

**Building Data Guardrails With Infrastructure**
The good news is that it’s not too late to change the data status quo. Three technical guardrails are emerging to ensure agent behavior moves toward transparent infrastructure and improved data rights.

First, we need clear pipelines that track attribution. Another much-hyped and much-misunderstood technology, blockchain, is helping with this. Blockchain-based [data frameworks create immutable records of what information agents access](https://thenewstack.io/a-look-at-datastaxs-ai-and-push-cache-for-data-access-at-scale/). Unlike today’s opaque sourcing, we can [build accountability into the infrastructure](https://thenewstack.io/building-an-integrated-infrastructure-for-real-time-business/) with verifiable credentials systems and decentralized identifiers. For example, [Kite AI](https://gokite.ai/) is building a modular layer-1 blockchain that tracks proof of attributed intelligence. This way, developers can configure incentives, coordinate collaboration across subnets, and [build ideal AI tech stacks across customized data](https://thenewstack.io/how-llms-helped-me-build-an-odbc-plugin-for-steampipe/), models, and agents.

Second, privacy-preserving computation technologies allow data processing without exposure. Zero-knowledge proofs, homomorphic encryption, and [secure multiparty computation create foundations of consent and keep data](https://thenewstack.io/container-security-a-troubling-tale-but-hope-on-the-horizon/) safe. These technologies, implemented in various blockchain systems and Trusted Execution Environment (TEE) computing platforms, enable computation over sensitive data without revealing it.

Third, we [need to return confidence to these systems](https://thenewstack.io/devs-need-system-design-tools-not-diagramming-tools/) with proper credit. If user information or copyrighted material is used, agents and their respective models must reward rather than ignore attribution. [Story Protocol](https://learn.story.foundation/intellectual-property-101) is another web3 project furthering this concept — using blockchain to allow creators to establish ownership of their work, setting rules for how it can be used, and ensuring they get paid when their content is utilized. [CARV ID](https://eips.ethereum.org/EIPS/eip-7231) achieves something similar — tying online identities to a single source of truth where users decide if their information is available and payable for model training. Not only does this bring users in on the AI revolution, but it goes some way to restoring trust in the overall system.

**Trust Makes or Breaks Agent Adoption**
Agents will stumble at the first hurdle if they [don’t engender trust](https://thenewstack.io/dont-trust-security-in-ai-generated-code/). Remember, these are platforms that promise autonomous, intelligent assistance across contexts. Mainstream acceptance is unlikely at home or work if our industry continues to rip off data.

We have already seen the dangers of this with Samsung meeting notes and source code [leaking after use](https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/) in ChatGPT. This kind of [data handling isn’t good enough](https://thenewstack.io/if-your-data-isnt-answering-why-it-isnt-working-hard-enough/) for corporate proprietary data and organizations bound by GDPR or HIPAA. Users and enterprises alike need to know their information isn’t only safe with these models but that it’s using top-of-the-line privacy standards and backend guardrails.

There are also [operational benefits to proper data](https://thenewstack.io/fermyon-cloud-save-your-webassembly-serverless-data-locally/) use. AI agents built on data sovereignty infrastructure produce more [reliable results by accessing](https://thenewstack.io/developers-guide-to-the-built-in-tools-of-openai-agents-sdk/) higher-quality, properly attributed information. They can also be audited for biases and inaccuracies. And perhaps most importantly, they earn trust by operating transparently rather than as inscrutable black boxes.

Agent success depends on creating a system where data isn’t just abundant and accredited but enriched with actionable metrics and validated through trustless consensus. When agents access high-quality, verified information — and only with explicit user consent and attribution mechanisms — they can deliver on their lofty goals. Now’s the time to learn from ChatGPT’s data wrongs and chart a new path toward safe and validated agentic infrastructure.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)