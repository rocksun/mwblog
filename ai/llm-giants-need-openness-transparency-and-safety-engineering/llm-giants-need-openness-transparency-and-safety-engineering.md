<!--
title: LLM巨头需要开放、透明和安全的工程
cover: https://cdn.thenewstack.io/media/2023/12/9920ff78-ai-fring-2-1024x576.png
-->

随着英国副首相支持开源人工智能，专家警告潜在风险和挑战。

> 译自 [LLM Giants Need Openness, Transparency and Safety Engineering](https://thenewstack.io/llm-giants-need-openness-transparency-and-safety-engineering/)，作者 Joe Fay 在科技行业担任记者已有30年，并且曾在伦敦和旧金山编辑过刊物。他还是GigaOm的一位贡献分析师。

开源的透明度和协作优势，以及经典的安全工程实践，将在世界学会如何与[生成式人工智能](https://thenewstack.io/generative-ai-cloud-services-aws-azure-or-google-cloud/)和前沿模型共存的过程中变得至关重要，专家们在上个月的伦敦[AI Fringe](https://aifringe.org/)活动上表示。

在英国副首相[奥利弗·道登（Oliver Dowden）](https://www.gov.uk/government/people/dowden)在[布莱切利公园举办的AI安全峰会](https://thenewstack.io/an-ai-safety-institute-benefits-big-tech-but-little-else/)上力挺开源[人工智能](https://thenewstack.io/ai/)的呼声不绝于耳。该次集结了世界领导人、政策制定者和人工智能公司的盛会通过了[布莱切利宣言](https://www.gov.uk/government/news/countries-agree-to-safe-and-responsible-development-of-frontier-ai-in-landmark-bletchley-declaration)，确定了关键风险，并承诺在人工智能[安全研究方面进行合作](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)。

然而，在峰会上，[“开源”人工智能模型](https://thenewstack.io/why-open-source-developers-are-using-llama-metas-ai-model/)的前景成为一个分歧点，各国政府和公司担心它们可能使不良分子获得先进技术并造成广泛危害。

## 开源框架促进了发展

[道登告诉Politico](https://www.politico.eu/article/british-deputy-pm-throws-backing-behind-open%20source-ai-downplays-risks/)，开源有益于初创企业，并且很可能对确保发展中国家从人工智能发展中受益至关重要。“因此，我认为限制开源的门槛非常高。”

在AI Fringe的一次交流中，OpenUK首席执行官[阿曼达·布洛克（Amanda Brock）](https://www.linkedin.com/in/amandabrocktech/?originalSubdomain=uk)和GitHub高级政策经理彼得·西翰（[Peter Cihon](https://www.linkedin.com/in/pcihon/)）指出，诸如[TensorFlow](https://thenewstack.io/serve-tensorflow-models-with-kserve-on-google-kubernetes-engine/)和[PyTorch](https://thenewstack.io/pytorch-lightning-and-the-future-of-open-source-ai/)之类的开源框架已经推动了人工智能的发展。

然而，他们继续表示，开放式方法是关于人工智能危害和监管的持续对话中不可或缺的一部分。

“越来越明显的是，尤其是在过去的一年半中，开源在模型方面也变得越来越普遍，” Cihon说道。“这在技术堆栈中形成了一种中间层。”

## 协作与透明度

他表示，在开放中进行的协作开发可以使人们以新的方式构建和扩展模型。开放的方法还可以帮助消除使用人工智能时的一些成本和其他障碍。

但当涉及到“透明度”时，开放的模型和生态系统尤其重要，他说。Cihon说，布莱切利公园讨论的前沿模型由少数几家公司主导，实际上只由两个国家主导。“房间里的大象是，大型语言模型在很大程度上是黑盒，我们并不充分理解。”

正是开源社区在“模型、文档、模型卡片、数据、文档数据集”方面“真正在开创最佳实践”。

他指出[EleutherAI](https://pile.eleuther.ai/)的[The Pile](https://pile.eleuther.ai/)是“一个经典例子”，说明如何进行这些讨论并展示模型开发的过程。他还表示，这支持“开放科学”，因为“你看到了如何创建某物的详尽文档，可以进行迭代并朝着新的方向发展。”

## 管理风险

Brock支持布莱切利宣言，称其为灵活、能够随着技术和创新的演变而灵活调整的监管的起点。她说，监管机构必须从过去吸取教训，避免过于详细和不灵活。

“我考虑的是责任方面的问题，”她补充说。“我们如何管理这种风险……如果你不理解它，就无法管理风险。”

Cihon表示，重要的是要超越炒作和悲观情绪，“真正集中注意力于人工智能如何今天伤害人们以及需要规范来解决这个问题。”

当涉及到人们“发表意见”关于开源人工智能的风险时，他说，有必要质疑在提供模型方面真正的边际风险是什么。“为了在世界上造成伤害，借助人工智能系统的支持，你需要在世界上采取行动。而这些行动是可被执法机关看到并可起诉的。”

在涉及现实世界的危害时，技术界有很多可以从古典安全工程中学到的东西，“构建人工智能安全文化”的一场讨论听到了这样的说法。

约克大学软件工程教授John McDermid表示，“在安全工程中，通常会识别潜在的危险，人们可能受到伤害的方式，然后定义消除这些危险的手段，或者控制它们以降低风险。”

一些直接的危害可能更容易识别，比如在贷款申请中的歧视。然而，开发人员需要考虑系统可能失败的所有方式。他说，如果系统在查看癌细胞图像时“漏掉了那些，那么就有人将得不到相关的治疗。”

将这一原则应用于存在风险以及基础或前沿模型可能会更加困难，他说，“因为系统的可见性或透明度不足。” 但是，他继续说，“如果我对这些问题的判断是正确的，那么我们应该避免在可能造成巨大危害的关键应用中部署这类系统。”

## 负责任的计算

英国计算机学会首席执行官[Rashik Parmar](https://www.linkedin.com/in/rashikparmar/?originalSubdomain=uk)表示，该协会正在制定一个“负责任计算”的框架，借鉴了这些方法并寻求建立最佳实践。

“这种硅谷文化的做坏事然后再想办法修复的方式可能在过去有效，但我们认为这对未来并不合适，”他说。

但他承认，仅有最佳实践是不够的。他提到了2010年代[剑桥分析丑闻](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)，表示那里的工程师曾提出担忧，但却被告知：“如果你不想做，那么你知道门在哪。”

Parmar表示，工程师和专业人士通常会理解他们所做的事情的后果，并希望做正确的事情。“但除非我们能够有其他机制来进行监管、治理，这是不够的。那么我们如何追究高管的责任呢？”
