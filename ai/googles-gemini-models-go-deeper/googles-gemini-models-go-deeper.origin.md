# Google’s Gemini Models Go Deeper
![Featued image for: Google’s Gemini Models Go Deeper](https://cdn.thenewstack.io/media/2025/05/bbd93769-img_0902-1024x768.jpg)
Google’s [I/O developer conference](https://io.google/2025/) this year is all about AI. That’s really not that much of a surprise, given that the company got most of the traditional Android updates it would typically feature at the event out of the way a few weeks ago. Last year, Google CEO [Sundar Pichai](https://www.linkedin.com/in/sundarpichai) declared the beginning of the Gemini era and in some ways, we’ve now hit the Gemini 2.0 era (or maybe 2.5, given the current version numbers of its models).

In many ways, today’s Gemini announcements aren’t revolutionary but build upon the foundations Google has been building over the last year. We’re not getting a Gemini 3.0 model, for example, but a better version of the 2.5 Flash and Pro model, which now scores even higher on some of the standard benchmarks.

![](https://cdn.thenewstack.io/media/2025/05/fef263c1-99a2eb37-4a6c-4b17-98c2-e5f946a6c84e-scaled.jpg)
Image credit: The New Stack.

In a press briefing ahead of today’s keynotes, Pichai noted as much by pointing out that in other years, Google would’ve kept quiet in the weeks before the event and kept the launch of its best models for I/O, but since the technology is moving so fast, that’s just not an option in this “Gemini era.”

And to be fair, they are getting some new features, like native audio output for more natural conversational experiences, new security safeguards and more.

Interestingly, consumers will get these updated models earlier than developers. It’s now available in the Gemini app, with Google AI Studio and Vertex AI getting support for 2.5 Flash in early June, with 2.5 Pro coming later.

## Going Deeper
The real innovation, however, is happening by putting these models to use, though.

Google, for example, showcased an enhanced “Deep Think” reasoning model for the 2.5 Pro model, for example, which builds upon today’s research model, but with the ability to go even deeper and test multiple hypotheses before responding. This new mode, Google says, pretty much bests all of the current frontier models in math and coding benchmarks.

However, that mode isn’t available to consumers or developers just yet, but it will soon become available to some trusted testers through the Gemini API.

![](https://cdn.thenewstack.io/media/2025/05/dc0e57c3-6638bd37-fd0e-48fd-9425-c490a8d7813f-scaled.jpg)
Image credit: The New Stack.

All of that depth means the model will use quite a few tokens, though, and will cost accordingly. Coming soon, developers will be able to set a thinking budget for Gemini 2.5 Pro, allowing them to set a maximum amount of tokens generated during the thinking project.

There’s another caveat here, too: Deep Think will be part of Google’s new Gemini AI Ultra plan, which will cost $249 per month — more than even OpenAI’s $200 per month [Pro plan](https://openai.com/chatgpt/pricing/). Google’s plan will include access to Deep Think, early access to [Gemini in Chrome](https://gemini.google/cms/login?continue=%2Foverview%2Fgemini-in-chrome%2F%3Fpreview%3Dtrue), higher usage limits for NotebookLM, as well as the video-centric Whisk and Flow generative AI tools (Flow is Google’s new AI filmmaking tool). YouTube Premium and 30TB of online storage are also included.

## Going Faster With Diffusion
Google also launched a new Gemini diffusion model today. Typically, diffusion models are used for image generation and not text generation, generating the image in multiple steps, getting more detailed over time, Google found a way to use this technique for generating text as well, with the model going back and forth in the text as it generates it.

In the demo the company showed today, the model generated mathematical solutions in under a second. For the time being, though, the company didn’t offer any other additional details of this model.

![Google's Gemini Diffusion model](https://cdn.thenewstack.io/media/2025/05/b72864cc-img_0932-scaled.jpg)
Image credit: The New Stack.

## Project Mariner Takes Over the Browser
Maybe the most interesting of these new Ultra features that are in the pipeline is Project Mariner, though. Announced last year and now ready for wider use, Project Mariner is Google’s agentic computer-use tool — meaning it can use a browser on behalf of a user. Mariner can handle up to 10 tasks simultaneously, ranging from looking up information to making bookings and buying things for the user. Those capabilities are coming to the Gemini API at some point later this year.

OpenAI, with Operator, and others have recently announced similar capabilities.

In Google Search, Google is also surfacing Project Mariner. Last year, Google announced the “AI Mode” in Search. This mode, which combines Google Search and its latest large language models, was previously only available as an experiment, but now, it can go out and do research on behalf of all Google users (in the U.S.).

Google says that since many of these searches are about getting something done, it is integrating Project Mariner into AI Mode to help users buy tickets, make restaurant reservations and make local appointments. Support for other use cases will come later.

AI Mode will also provide personalized suggestions, which will take data from Gmail and other Google apps into account. This is an opt-in feature and while Google stresses that it is doing all of this in a privacy-first way, not everyone is going to feel comfortable with this.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)