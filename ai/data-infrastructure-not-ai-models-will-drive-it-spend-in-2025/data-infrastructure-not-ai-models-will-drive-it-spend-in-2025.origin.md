# Data Infrastructure, Not AI Models, Will Drive IT Spend in 2025
![Featued image for: Data Infrastructure, Not AI Models, Will Drive IT Spend in 2025](https://cdn.thenewstack.io/media/2025/02/0e942505-logan-voss-lex2rihvms4-unsplash-1024x576.jpg)
[Logan Voss](https://unsplash.com/@loganvoss?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)on
[Unsplash](https://unsplash.com/photos/a-black-and-yellow-plaid-pattern-is-shown-lex2rihvms4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash).
As organizations race to implement Artificial Intelligence (AI) initiatives, they’re encountering an unexpected bottleneck: the massive cost of data infrastructure required to support AI applications.

While [Gartner projects a 9.8% increase in IT spending for 2025](https://www.gartner.com/en/newsroom/press-releases/2025-01-21-gartner-forecasts-worldwide-it-spending-to-grow-9-point-8-percent-in-2025), the real story isn’t about models or compute resources — it’s about the exponential growth in data infrastructure costs that threatens to make AI initiatives economically unsustainable.

**The Scale of the Financial Challenge **
Traditional data architectures weren’t designed to handle the volume and velocity of data that AI applications require. In streaming scenarios, networking costs alone can account for up to 80% of total infrastructure expenses. This becomes particularly problematic when organizations must move data between different zones or regions for processing or training.

Consider a typical enterprise AI initiative: Companies often start with modest pilots with one specific business challenge, such as implementing AI for fraud detection or predictive planning. These projects initially appear manageable from a cost perspective. However, as data volumes grow and use cases expand, infrastructure costs scale exponentially rather than linearly. This is especially true for mission-critical applications like autonomous driving or manufacturing sensors, where real-time data processing is non-negotiable and downtime isn’t an option.

**The cost challenge stems from three primary factors:**
**First**, traditional architectures often require multiple copies of the same data across different systems — one copy for streaming, another for batch processing, and another for AI training.**Second**, moving this data between different zones in cloud environments incurs significant networking costs.**Third**, maintaining separate infrastructure for real-time and batch processing creates operational overhead and inefficiencies.
This multiplication of data and infrastructure leads to the “AI data tax” — the hidden costs that emerge when scaling AI initiatives beyond pilot projects. For many organizations, these costs can exceed the expenses of the AI models and compute resources themselves.

**Rethinking Data Architecture **
I’m seeing organizations address these challenges through innovative architectural approaches. One promising direction is the adoption of leaderless architectures combined with object storage. This approach eliminates the need for expensive data movement by leveraging cloud-native storage solutions that simultaneously serve multiple purposes.

Another key strategy involves rethinking how data is organized and accessed. Rather than maintaining separate infrastructures for streaming and batch processing, companies are moving toward unified platforms that can efficiently handle both workloads. This reduces infrastructure costs and simplifies data governance and access patterns.

Cloud providers’ pricing models significantly impact the economics of AI infrastructure. While cloud services offer flexibility and scalability, their network egress fees and data transfer costs can quickly become prohibitive for data-intensive AI workloads. Organizations must carefully architect solutions to minimize data movement between zones and regions.

**Cost-Effectiveness in Motion**
As we move into 2025, successful AI initiatives will depend less on choosing the right models and more on building cost-effective data infrastructure.

Organizations should focus on:

- Implementing architectures that minimize data duplication and movement;
- Leveraging object storage and headless designs to reduce infrastructure costs;
- Unifying streaming and batch processing to simplify operations and
- Optimizing cloud resource usage to control networking expenses.
While Gartner’s projected increase in IT spending reflects the growing importance of AI, organizations that fail to address these infrastructure challenges risk seeing their AI initiatives stall due to unsustainable costs. The key to success lies not in throwing more resources at the problem but in fundamentally rethinking how we architect data infrastructure for the AI era.

The next wave of AI innovation will not come solely from better models — it will also come from more efficient ways to store, move, and process the massive amounts of data that make AI possible. Organizations that solve this infrastructure challenge will be best positioned to scale their AI initiatives successfully.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)