一只雄伟的蓝色老虎骑在一艘帆船上。老虎非常大。图像使用PonyXL生成。

AI模型可以变得非常庞大。更大的模型似乎比更小的模型性能更好，但我们并不完全清楚为什么。我的工作MacBook有64GB内存，在进行AI推理时，我几乎可以使用所有内存。不知何故，这些40多GB的浮点数能够回答关于天空颜色这样的问题。在某种程度上，这是一个技术奇迹，但它是如何工作的呢？

今天，我将介绍AI模型究竟是什么以及构成它的部分。我不会介绍其中涉及的线性代数或任何神经网络。无论如何，大多数人都想从现成的模型开始。

## AI模型由什么构成？

核心上，AI模型只是一个浮点数的集合，输入数据通过它来获得输出。模型主要有两种：语言模型和图像扩散模型。它们非常相似，但也有一些不同之处。

文本生成模型有一些基本组成部分：

- 一个分词器模型，用于将输入分解成单词、语法分隔符和表情符号。
- 一个嵌入模型，用于获取标记之间关系的频率并生成“概念”，这使得模型能够识别“热”和“温暖”是相似的。
- 标记预测权重，嵌入数据将通过它来确定接下来最有可能出现的标记。

请注意，这实际上是三个独立的模型[stacked](https://bojackhorseman.fandom.com/wiki/Vincent_Adultman)堆叠在一起，但它们只有组合在一起才有意义。你不能将它们分开，也不能互换部件。

在所有这些中，标记预测权重是最大的一部分。语言模型的参数数量指的是标记预测权重中浮点数的数量。一个80亿参数的语言模型有80亿个浮点参数。

图像扩散模型与语言模型的大部分组成部分相同：

- 一个分词器，用于获取你的输入并将其分解成单词、语法分隔符和表情符号。
- 一个嵌入模型，用于将这些标记转换成潜在空间，这是一种更适合潜在扩散的嵌入方式。
- 一个去噪模型（unet），它逐渐去除潜在空间中的噪声，使图像显现出来。
- 一个变分自动编码器（VAE），用于将潜在空间编码成图像。

大多数情况下，单个模型（例如Stable Diffusion XL、PonyXL或微调模型）会将所有这四个模型包含在一个`.safetensors`文件中。

在Stable Diffusion 1.5的早期，你通常可以通过将VAE模型替换为你最合适的变体来获得巨大的质量提升（一个更擅长制作动漫风格的图像，一个针对使手看起来正确而进行了优化，一个针对特定类型的柔和水彩风格进行了优化，等等）。Stable Diffusion XL及更高版本在很大程度上使得模型中内置的VAE足够好，你不再需要关心它。

在这三个堆叠的模型中，去噪模型是所引用的尺寸。分词器、嵌入模型和变分自动编码器是附加的组件。Stable Diffusion XL的去噪模型有66亿个参数，Flux [dev]的去噪模型有120亿个参数，其他模型的尺寸约占模型总尺寸的5-10%，并且参数数量中没有计算在内，但它们确实会影响最终的模型尺寸。

我们目前认为，模型的参数越多，它就能更准确地表示细微差别。这通常意味着，一个700亿参数的语言模型能够处理一个80亿参数的语言模型无法处理的任务，或者一个700亿参数的语言模型能够比一个80亿参数的语言模型更好地完成任务。

最近，较小的模型正在迎头赶上，[bigger isn't always better](https://www.scientificamerican.com/article/when-it-comes-to-ai-models-bigger-isnt-always-better/)。更大的模型需要更多的计算资源，并会引入性能瓶颈。现实情况是人们会使用大型模型，因此我们需要设计能够处理它们的系统。

## 量化

如果你有幸能够廉价地访问高vram GPU，则无需担心量化。量化是一种压缩形式，你将模型的浮点权重转换为较小的数字，例如将一个具有140GB float16参数（16位浮点数）的700亿参数模型转换为35GB的4位参数（Q4）。这是一个有损操作，但它将节省docker镜像中的宝贵GB，并使更大的模型能够适应较小的GPU。

当你阅读模型量化级别，例如`Q4`或`fp16`/`float16`时，你可以这样理解：

| 首字母 | 数字 | 含义 |
|---|---|---|
| `Q` 或 `I` | `4` | 四位整数 |
|---|---|---|
| `f`、`fp` 或 `float` | `16` | 16 位 |

使用量化是在拥有的视频内存（GPU 内存）量和所需任务之间的权衡。使用 Q4 量化的 70B 模型与使用完整的 float16 量化相比，会损失一些质量，但您可以在一块 GPU 上运行该 70B 模型，而无需两到四块 GPU 才能运行它。

大多数情况下，您不需要对图像扩散模型进行量化就能运行它们（对于在低端消费级 GPU 上运行 Flux [dev] 有一些例外）。这几乎只在语言模型中进行。

为了计算 float16 量化模型需要多少内存，请遵循以下经验法则：

`(参数数量 * 每个参数的大小) * 1.25`

这意味着一个 80 亿参数的模型，在 16 位浮点精度下，大约需要 20GB 的视频内存，但根据上下文窗口的大小，它可能需要更多内存。

## 存储位置

您的 AI 模型越大，权重就越大。

AI 模型是需要加载到 GPU 内存中才能使用的大块数据（模型权重和开销）。大多数情况下，AI 模型的运行时需要模型的字节在加载之前存在于磁盘上。这就提出了“我应该把这些东西存储在哪里？”的问题。

人们在生产中使用几种选项：

- Git LFS，例如与 HuggingFace 一起使用。
- 将模型权重放入对象存储（如 Tigris）并在应用程序启动时下载它们。
- 将模型权重放入 Docker 镜像的专用层（例如使用 [depot.ai](https://depot.ai/)）。
- 挂载已经包含模型的远程文件系统并直接使用它。

所有这些都有其自身的优缺点。Git LFS 已成熟，但如果您想在自己的硬件上运行它，则需要设置一个专用的 git forge 程序，例如 Gitea。使用远程文件系统可能会将您锁定到提供商对该文件系统的实现（例如使用 AWS Elastic FileSystem）。将模型权重放入 Docker 镜像可能会导致提取时间增加，并可能超过您选择的 Docker 注册表的限制。使用 Tigris（或其他对象存储）时，您需要在启动时将模型权重下载到磁盘，或者设置高性能共享文件系统，例如 [GeeseFS](https://www.tigrisdata.com/docs/training/geesefs-linux/)。

在比较选项时，请记住所有这些。

## 总结

作为行业，我们花费了大量时间来考虑 Docker 构建的效率以及将代码作为不可变工件移动。AI 模型具有相同的经典问题，但工件大小更大。许多系统的设计假设您的镜像大小在未记录的“合理”限制内，可能小于 140GB 的浮点数。

如果您的系统难以跟上图像快速增长的速度，请不要感到难过。它并非设计用来处理我们今天面临的问题，因此我们可以使用新工具进行构建。但是，如果您处理的是 80 亿参数的 Q4 量化或更低量化的模型，那么将模型权重塞入 Docker 镜像中会很好用。

超过该阈值，您需要将模型分解成更小的块 [就像上游模型那样](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-70B/tree/main)。如果这是一个问题，您可以将模型存储在 Tigris 中。我们将处理所有大型文件，而无需文件类型限制或限制性限制。我们的文件大小限制为 5TB。如果您的模型大于 5TB，请联系我们。我们很想知道我们如何提供帮助。

# 想试试吗？

创建一个没有出口费的全局存储桶，并在世界各地存储所有模型。