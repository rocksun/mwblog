<!--
title: 告别AGI炒作，LLM迈向工程现实的未来
cover: https://cdn.thenewstack.io/media/2025/12/f702dffa-getty-images-itbnjq7h1-4-unsplashb.jpg
summary: 大模型未来将通过知识图谱减少幻觉，借助中心辐射模式与本地化提升效率，并利用生活流实现个性化理解，重心在于现有技术改进。
-->

大模型未来将通过知识图谱减少幻觉，借助中心辐射模式与本地化提升效率，并利用生活流实现个性化理解，重心在于现有技术改进。

> 译自：[From AGI Hype to Engineering Reality: The Future of LLMs](https://thenewstack.io/from-agi-hype-to-engineering-reality-the-future-of-llms/)
> 
> 作者：David Eastman

如果我们想知道大语言模型（LLM）提供商在未来几年将如何改进其服务，我们可以从预测当前局限性将如何得到解决开始。尽管大语言模型在聊天框形式上取得了相当大的成功，但它们在能源消耗方面既昂贵，又存在无休止的幻觉问题。软件开发者们努力通过增加token的使用量来获得更集中的结果。

关于规模与训练如何真正影响输出，仍存在一些猜测，但能源和幻觉问题限制了其扩张。因此，本文探讨了大语言模型提供商可能选择的发展方向。

但首先，我们必须验证 Yann LeCun 关于[大语言模型是死胡同](https://www.businessinsider.com/meta-ai-yann-lecun-llm-world-model-intelligence-criticism-2025-11)的预测的有效性。尽管这对于“人工通用智能”来说最终可能是正确的，但人工智能公司投入的巨额资金和发展势头确保我们将在未来一段时间内继续使用大语言模型。LeCun 本人也[创办了一家初创公司](https://www.linkedin.com/posts/yann-lecun_as-many-of-you-have-heard-through-rumors-activity-7397020300451749888-2lhA/)，旨在“继续我过去几年一直在追求的‘高级机器智能研究计划（AMI）’”，但这在短期内不会结果。

## 本体论

许多旧的AI方法已被大语言模型的成功所取代，但我仍然记得，过去人们曾认为人工智能将由大型[本体论](https://www.shepbryan.com/blog/ontologies-101)组成——可以把它们想象成概念图，非常类似于话题标签，用于在某种形式结构内连接思想。由于大语言模型在大量信息上进行训练，它们以某种随机的方式内化概念，却似乎理解事物之间的关系。但我们知道大语言模型可以帮助创建知识图谱；而[检索增强生成](https://thenewstack.io/a-blueprint-for-implementing-rag-at-scale/)（RAG）是一种重要的方法，通过向大语言模型提供格式化的专业知识，来保持其响应的真实性。

对抗幻觉的一种可能方法是专注于在特定主题领域维护大量大型知识图谱，并在其他提供商服务之间共享这些图谱。

这样做的压力可能来自监管。例如，我们最近看到澳大利亚[对社交网络施加了年龄限制](https://www.theguardian.com/australia-news/2025/dec/13/will-other-countries-follow-australia-social-media-ban-under-16s)，原因是屏幕成瘾对儿童造成了各种负面影响。因此，可能需要创建相当于“儿童不列颠百科全书”的东西——一套不泄露来自问题领域事实的大量信息。由第三方维护的、更受监管的信息可能会说服各国政府，大语言模型不会传播有偏见的事实。

## 中心辐射模式

正式共享大量信息可能会与竞争对手提供商的商业模式相悖，但合作仍能带来效率提升。

我们在此已经看到了一些希望：Anthropic 的[模型上下文协议](https://thenewstack.io/model-context-protocol-a-primer-for-the-developers/)（MCP）作为“大语言模型的USB接口”在早期获得了略微令人惊讶的普遍接受，这可能告诉我们，只要一个想法足够好，竞争对手提供商（例如本例中的 OpenAI）就会采用它。

OpenAI 已经通过其[应用SDK](https://thenewstack.io/openais-apps-sdk-a-developers-guide-to-getting-started/)及其 Atlas 浏览器如何协同工作，强调了一种可能的发行模式。这里的想法是，字面上将本地知识视为一种大语言模型可以调用的 MCP 服务器。通过这种方式，OpenAI 正试图取代网络——通过其 ChatGPT 模型回答一般查询，但调用用户应用程序服务器以获取本地专业信息。例如，这与 OpenAI 使用 MCP 工具访问你的硬盘驱动器的方式完全相同。

## 本地大语言模型

许多人已经在本地运行大语言模型，过去几年我们向读者展示了[如何设置和运行本地大语言模型，使用Ollama和Llama 2](https://thenewstack.io/how-to-set-up-and-run-a-local-llm-with-ollama-and-llama-2/)。虽然大型尖端模型仍将留在云端，但有大量较小的预加权开源模型，用户可以在其笔记本电脑上运行。本地运行仍然有点技术性，但像 Ollama 这样的应用程序使其变得更简单。当然，最终的本地机器很可能是你的手机。

我们已经看到代理式 CLI 系统如何为某些查询选择快速廉价的模型，而将更昂贵的模型留给“深度思考”或“规划”。这引出了一个想法，即可以为较小的查询使用本地模型，同时将更难的查询发送给在云端进行处理的更大模型。

## 生活流

另一个本地化的原因是获取用户的个人上下文。当我们看到 Google 历史上在回答用户查询方面表现出色时，这开始变得很有意义，因为它对用户足够了解，可以排除不相关的结果。

合理推断，亚马逊使用来自数百万 Alexa 扬声器的信息训练大语言模型，并区分家庭中每个说话者的身份。但本地大语言模型可以监听和阅读你所有的语音和内容，以便完全理解你的地理位置，以及你具体感兴趣的内容。

虽然“生活流应用”可能带来的奥威尔式后果在2010年代确实让我们感到担忧，但我们仍然用我们持续的状态报告来填充它们。代理式 CLI 使用设置 markdown 文件向大语言模型提供关于项目的提示，因此随着时间的推移分析用户肯定会更有效。苏格拉底据说说过“未经审视的生活不值得过”，虽然我怀疑他会认可人工智能，但适量的记录肯定能为大语言模型提供一个丰富的（如果是个人化的）图谱，以便开始工作。

## 结论

在大型提供商着手提高效率之前，或者在投资者不再追逐“人工通用智能”之前，市场可能需要一次小幅“调整”（即崩盘）。或许大型公司将一起进入另一个炒作领域，以延续AI势头并保持股价高企。但很有可能，工程将转向改进现有投资。

如果你将大语言模型用于软件开发，你将能坐在前排旁观即将到来的任何变化。