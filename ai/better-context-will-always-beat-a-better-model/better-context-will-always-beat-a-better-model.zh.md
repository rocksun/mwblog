AI领导者在过去几年里一直痴迷于基准测试，争论 [GPT-4](https://thenewstack.io/we-used-gpt4-during-a-hackathon-heres-what-we-learned/)、Claude 还是 [Gemini](https://thenewstack.io/google-launches-gemini-3-pro/) 谁能独占鳌头。企业AI的这种讨论一直由一个单一指标主导：模型性能。但这种对原始“智能”的执着，忽略了成功部署中最关键的因素。

随着这些模型能力趋同，战场正在转移。下一代企业应用的差异化因素将不再是模型本身；而是上下文。在一个所有企业都能访问相同前沿模型的环境中，模型的“智能”不再是可持续的护城河。相反，赢家将是那些能够最有效地将其智能植根于自身专有现实的组织。

“[上下文](https://thenewstack.io/why-context-aware-ai-is-quickly-replacing-code-only-tools/)是新的源代码，” [TestMu AI](https://www.testmu.ai/)（前身为LambdaTest）的工程总监 Srinivasan Sekar 说，这是一家AI原生软件测试平台。他阐述道，尽管业界专注于模型规模，但真正的挑战在于数据交付。

“我们发现，模型的智能程度取决于我们为它构建的环境，”他解释说。“如果上下文混乱，即使是最先进的模型也会失败。”

因此，将企业数据输入这些模型被证明比最初想象的要危险和复杂得多。这不仅仅是简单地导入文档；而是要防止AI被 [噪声“噎住”](https://thenewstack.io/why-agentic-ai-needs-a-context-based-approach/)。

这需要我们将AI从一个“无所不知”的“神谕”转变为一个需要高保真信息环境才能产生商业价值的推理引擎。

我与一些著名的工程领导者进行了交谈，他们分享了关于AI的未来如何存在于其周围架构的精确性中的观点。本质上，模型充当处理器，而架构则作为决定速度、准确性和企业级可靠性的燃料。

## “白编码”的兴起与治理鸿沟

这一转型的风险很高，因为AI的作用已经发生了根本性变化。我们已经超越了简单的自动补全，进入了一个 [Iterate.ai](http://iterate.ai) 联合创始人 Brian Sathianathan 称之为“白编码”的范式。在这种环境中，工具不仅仅是完成一行代码；它们从一个单一的提示生成整个架构、多文件编辑和复杂逻辑。曾经需要数天人工努力的任务现在在20分钟内完成。

然而，这种前所未有的速度造成了一个可怕的治理鸿沟。当人类编写代码时，他们逐行管理。当AI在一次会话中生成5,000行代码时，那种细粒度监管就消失了。

Sathianathan 警告说，如果开发人员从一开始就没有适当的上下文和安全防护措施，他们就有可能以机器的速度产生技术债务。如果没有有意的上下文，模型可能会引入已知漏洞的框架，或者创建根本不安全的逻辑流。这些风险可能在为时已晚之前都不会被发现。

为了解决这个问题，工程团队必须从回溯性代码审查转向“预见性上下文治理”。这包括将安全标准直接嵌入到AI“看到”的环境中，确保生成的逻辑保持在安全、预定义的边界内。

## “越多越好”的谬误

大多数开发人员的自然本能是通过向AI提供更多信息来解决不准确性。如果AI理解整个代码库，逻辑认为，它就不会犯错。 [Scaledown](https://scaledown.ai/) 联合创始人兼首席执行官 Neal Patel 警告说，这是一个危险的谬误。他对上下文工程的研究表明，在企业工作负载中，发送给模型的令牌中大约30%到60%没有增加任何价值。

“人们认为更多令牌意味着更高的准确性，但实际上，情况往往相反，” Patel 说。“当模型被松散相关或不相关的上下文超载时，其注意力机制就会被稀释。”

这不仅仅是一个理论上的担忧；它有经验研究支持。Patel 引用了（斯坦福/伯克利）的“[迷失在中间](https://aclanthology.org/2024.tacl-1.9.pdf)”研究，该研究表明，当相关细节被埋藏在长提示的中间时，模型准确性会下降。此外，[Mila/McGill 的研究](https://aclanthology.org/2025.acl-long.1458.pdf) 发现，添加不相关的文本导致11.5%先前正确的AI答案变得错误。

这产生了 Patel 称之为“上下文腐烂”的现象。当一个系统为用户服务数月或数年时，它会积累历史和元数据。相同的用例变得指数级地更重、更慢、更昂贵。

“目标不是塞满窗口；而是提取信号，” Patel 指出。通过隔离查询真正需要的内容而获得的更智能、高保真的上下文，始终优于更大、更嘈杂的上下文。

## 用结构对抗“上下文污染”

这就是工程现实的关键所在：如何构建一个只给AI它需要的，不多不少的系统？ Sekar 将根本问题识别为“不透明系统”。当工程师将整个代码库或模式倾倒到上下文中时，AI被迫在数据海洋中搜索有用的信息，在此过程中经常忽略安全限制。

为了克服这一点，团队应采用结构化检索方法。同样在 TestMu AI 担任工程总监并与 Sekar 合作的 Sai Krishna V，描述了一种在数据到达AI之前“扁平化”复杂数据结构的方法。TestMu AI 将数据规范化为单层，而不是输入增加模型认知负荷的深度嵌套对象。

实施这一点需要“管理AI记忆”的心态。通过使用智能检索只获取当前问题所需的特定注释或逻辑，工程师可以为AI的推理过程创建一个干净的环境。这确保了模型专注于手头的任务，而不会被遥远的、不相关的数据结构所“污染”。

## 上下文缓存与“笔记本”

难题的最后一部分是操作效率。如果一个AI代理必须为每个查询重新阅读和重新分析相同的项目上下文，系统将变得极其昂贵和缓慢。Scaledown 的 Patel 指出，这种低效率也带来了人力成本；每个冗余令牌都会增加延迟，导致搜索被放弃和产品流程变慢。为了解决这个瓶颈，Sekar 提倡一种名为上下文缓存的技术。

Sekar 用一个实际的类比来描述这一点。把代理想象成一个带着笔记本的学生。当代理第一次解决一个复杂的架构问题时，它不应该只输出代码；它应该缓存它对那个问题的“理解”，本质上是记下笔记。下次收到类似请求时，代理会检索缓存的上下文，而不是从头开始推导解决方案。

因此，尽管 Patel 强调减少令牌浪费以保持响应性的必要性，但 Sekar 的方法提供了企业如何实际“管理”其系统“记忆”的技术蓝图。这种转变确保了AI不仅仅是重复计算，而是随着时间的推移建立一个持久、高效的知识库。

## 人类与AI的认知

上下文不仅仅是提高效率的架构机制；它还是工作流中一个活跃的层，帮助人们更审慎地与AI协作。[Magi](https://www.magihq.com/) 创始人兼首席执行官 Bhavana Thudi（Magi 是一家面向AI原生营销团队的上下文感知操作系统）将此描述为在人机循环中设计“暂停时刻”。这些暂停创造了反思、重新考虑和学习的空间，作为工作流程的一部分，形成了一个共享的推理循环，使人类和机器在任务中都变得更好。

当AI系统围绕上下文和工作流中的审慎暂停进行设计时，团队会有意地构建一个周到的工作环境，认知能力会在人机系统中显现。Thudi 指出，这些暂停时刻不仅是认知的，而且是累积的，允许工作将记忆向前推进，而不是每次交互都重置。这就是未来的工作方式。

这对那些构建AI系统的人来说意味着：进步不会来自将人类从循环中移除，而是来自设计能够随着时间推移保留意图、记忆和判断的系统。以上下文为核心构建的系统能够实现更好的工作，并使其价值复合增长。

## 过滤作为竞争优势

随着企业从试验聊天机器人转向部署自主代理，焦点必须从模型转移到数据管道。构建最可靠系统的公司不一定拥有最复杂的AI模型。它们是那些努力重新设计其数据基础，以说机器能理解的语言的公司。

正如 Krishna 总结的那样，在一个充满无限噪声的时代，过滤能力是最终的竞争优势。这种过滤不是发生在模型层面；而是发生在架构层面，特别是在如何构建数据、检索上下文和验证结果方面。明年AI开发的信息很明确：模型提供推理，但工程师必须提供上下文。