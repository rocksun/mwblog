
<!--
title: AI安全建设：夯实基础架构，远胜工具叠加
cover: https://cdn.thenewstack.io/media/2025/06/d599835a-eyeball1a.jpg
summary: AI安全市场存在供应商信息与实际需求脱节的问题。传统安全方案侧重DLP，忽略了AI应用运行时保护和基础设施安全。AI工作负载需要真正的隔离、性能和动态资源分配。企业应关注实际风险和基础设施，而非盲目追随安全趋势。
-->

AI安全市场存在供应商信息与实际需求脱节的问题。传统安全方案侧重DLP，忽略了AI应用运行时保护和基础设施安全。AI工作负载需要真正的隔离、性能和动态资源分配。企业应关注实际风险和基础设施，而非盲目追随安全趋势。

> 译自：[AI Security Needs Better Infrastructure, Not More Tools](https://thenewstack.io/ai-security-needs-better-infrastructure-not-more-tools/)
> 
> 作者：Kaylin Trychon

AI 安全市场正经历它的“皇帝的新装”时刻。根据 Latio 的 [2025 年第二季度 AI 安全市场报告](https://pulse.latio.tech/p/2025-latio-ai-security-report)，93% 的安全专业人士认识到 [AI 在增强网络安全方面的潜力](https://thenewstack.io/navigating-the-turbulent-waters-of-ai-security/)，但高达 77% 的组织感到没有准备好防御 AI 驱动的威胁。更令人震惊的是，当被问及他们当前的 AI 安全优先级时，从业者揭示了供应商信息与实际需求之间令人不安的脱节。

通过与急于[部署 AI 工作负载](https://thenewstack.io/developers-are-embracing-ai-to-streamline-threat-detection-and-stay-ahead/)的企业客户的对话，我亲眼目睹了这种脱节。这些模式与我们在容器采用期间看到的情况类似——[兴奋压倒安全](https://thenewstack.io/ai-is-changing-cybersecurity-fast-and-most-analysts-arent-ready/)，只有在事件发生后才会吸取痛苦的教训。

## 伟大的 AI 安全误导

大多数 AI 安全供应商都在解决昨天的问题，而明天的威胁已经开始显现。该报告显示，大多数 AI 安全解决方案最初都侧重于员工数据泄露防护 (DLP)，本质上是构建精密的监控工具来观察员工在 ChatGPT 中粘贴的内容。其他公司则专注于 AI “防火墙”，过滤 LLM 的输入和输出，这些防火墙很有用，但范围有限。与此同时，真正的风险正在迅速转向可以访问敏感数据并代表用户采取行动的 AI 应用程序。

这种误导并非偶然。DLP 式解决方案更容易构建，并且能利用 CISO 熟悉的恐惧模式。但随着 [Microsoft](https://news.microsoft.com/?utm_content=inline+mention) Copilot 和 [Google](https://cloud.google.com/?utm_content=inline+mention) 的 Gemini 随着内置保护措施的成熟，这些监控解决方案正变得商品化。

根本问题不仅仅在于 AI 安全工具，而在于底层基础设施是否可以支持大规模的安全 AI 工作负载。许多组织正在容器基础上构建 AI 应用程序，而这些容器基础从未设计用于敏感 AI 工作负载所需的隔离要求。

## 基础设施与 AI 安全风险的交汇

该报告最重要的见解集中在“应用程序运行时保护”上——当 AI 系统超越简单的聊天机器人而成为自主代理时出现的[安全挑战](https://thenewstack.io/ai-agents-are-a-security-ticking-time-bomb/)。考虑风险演变：基本的 ChatGPT 会话风险最小，但 AI 代理带来了完全不同的威胁环境。例如，AI 编码助手可以在开发过程中执行任意代码，或者代理可以查询内部数据库并触发金融交易。这些场景需要基础设施级别的安全控制。

这里，基础设施决策变得至关重要。处理敏感数据的 AI 工作负载需要与企业用于其最关键应用程序相同的内核级隔离。许多基于容器的 AI 部署所依赖的共享内核模型创建了攻击途径，而这些途径根本无法仅通过应用程序层保护来充分保护。

根据 [Microsoft 的 2024 年容器安全概述](https://techcommunity.microsoft.com/blog/microsoftdefendercloudblog/new-innovations-in-container-security-with-unified-visibility-investigations-and/4298593)，基于容器的工作负载面临着日益增长的安全威胁，安全团队通常无法跟踪哪些容器正在运行或随时存在漏洞。对于处理敏感数据的 AI 工作负载，这些可见性差距会变成关键漏洞。

## AI 工作负载实际需要的基础设施要求

AI 应用程序需要传统容器部署难以提供的三个基础设施特征：

*   **真正的隔离：** 处理专有数据的 AI 模型需要内核级分离，而不仅仅是命名空间隔离。当 AI 系统可以访问客户记录或财务数据时，共享内核漏洞会变成业务关键型风险。
*   **性能不受影响：** AI 推理工作负载对延迟非常敏感。许多容器架构中固有的网络和 I/O 开销会使实时 AI 应用程序无法使用。
*   **动态资源分配：** 与传统应用程序不同，AI 工作负载具有高度可变的资源需求。可以根据实际需求动态分配 GPU 和内存资源的基础设施消除了困扰标准容器部署的大规模过度配置浪费。

## 构建、购买还是等待的决策

投资格局讲述了一个发人深省的故事。虽然各组织在 2025 年在 [AI 基础设施上的支出超过 3200 亿美元](https://www.cnbc.com/2025/02/08/tech-megacaps-to-spend-more-than-300-billion-in-2025-to-win-in-ai.html)，但 AI 安全市场仅占 250 亿美元——其中一小部分专注于保护这些大规模的基础设施投资。这种差距突出了基础设施支出与安全准备之间的差距。

组织是否应该投资专门的 AI 安全工具的答案取决于您的 AI 采用速度和基础设施成熟度。如果您正在构建处理敏感数据的 AI 应用程序，则专用工具提供的专业化是现有企业无法比拟的。但架构决策比供应商选择更重要。从一开始就使用虚拟机监控程序级隔离和安全多租户构建 AI 基础设施的组织比那些试图将安全性改造到现有容器部署中的组织具有显着优势。

传统的安全供应商正在迅速增加 AI 功能，但他们正在追赶从头开始为这种威胁环境设计的解决方案。最有效的方法是将经过验证的安全原则（如基于虚拟机监控程序的隔离）与针对 AI 工作负载特性优化的现代实现相结合。

## 展望未来

根据报告的调查结果，出现了三个趋势：

首先，随着平台构建原生保护，以 DLP 为中心的 AI 安全性将迅速整合。其次，运行时应用程序保护将推动市场增长的大部分，基础设施安全将成为关键的差异化因素。第三，“设计时安全”与“事后安全”将区分市场的领导者和追随者。

## 组织现在应该做什么

AI 安全市场不断增长的痛苦[反映了过去的技术采用周期](https://thenewstack.io/future-proofing-ai-repeating-mistakes-or-learning-from-the-past/)。能够蓬勃发展的组织是那些专注于其实际风险状况和基础设施基础，而不是追逐安全趋势的组织。

首先要问以下问题：AI 实际部署在您组织的什么位置？它可以访问哪些数据？您当前的基础设施是否可以为这些工作负载提供足够的隔离？答案应该驱动您的 AI 安全策略，而不是供应商营销。

应用经过时间考验的基础设施原则，同时适应 AI 的独特要求。那些构建安全、高效基础的组织将胜过那些将安全性附加到不充分的基础设施上的组织。

皇帝可能没有穿衣服，但认识到这一现实是构建真正有效的 AI 安全的第一步。