
<!--
title: 恶意模型与漏洞：当AI成为攻击者时
cover: https://cdn.thenewstack.io/media/2025/01/1324b217-evil.jpg
-->

AI正在重塑黑客和恶意软件开发的四种方式，以及我们如何保持警惕以应对这些方式。

> 译自 [Evil Models and Exploits: When AI Becomes the Attacker](https://thenewstack.io/evil-models-and-exploits-when-ai-becomes-the-attacker/)，作者 Luke Hinds。

人工智能正在以惊人的速度重塑各个行业，网络安全领域也不例外。从编码助手到渗透测试工具，我们正在见证人工智能驱动机制的兴起，这些机制能够提高生产力和解决问题的能力。

然而，能够增强开发工作流程的相同工具也可能使恶意行为者受益。以下是AI正在重塑黑客和恶意软件开发的四种方式，以及我们如何保持警惕以应对这些方式。

## 1. 智能体增强型黑客攻击

利用AI以新颖且具有潜在破坏性的方式进行智能体增强型黑客攻击的概念，正迅速从假设变为不可避免。

任何使用过AI驱动的编码助手，例如[GitHub Copilot](https://thenewstack.io/github-copilot-a-powerful-controversial-autocomplete-for-developers/), [Cursor](https://thenewstack.io/using-cursor-ai-as-part-of-your-development-workflow/)或类似工具的人，都熟悉输入-反馈循环。此过程涉及AI系统访问shell，执行命令，捕获输出并使用该反馈来改进后续指令。在开发中，此迭代过程使编码助手能够通过直接理解执行环境来创建更有效和更准确的代码。

同样的概念可以被武器化。攻击者可以将输入-反馈循环集成到他们的工具包中，使用AI来协调流行的渗透测试工具，例如OWASP ZAP、Nmap或Nikto2。这些工具的输出可以直接输入AI模型，允许系统根据发现结果创建定制的漏洞利用代码。

如果初始漏洞利用失败，系统的反馈循环使其能够迭代——调整、改进和重试，直到成功。此过程大大减少了识别和利用漏洞所需的时间和专业知识，标志着网络安全威胁格局的根本转变。

## 2. 模型上下文协议

随着模型上下文协议 (MCP) 等技术的出现，出现了更结构化的威胁。MCP最初由Anthropic公司推出，允许大型语言模型 (LLM) 通过[JavaScript](https://thenewstack.io/5-technical-javascript-trends-you-need-to-know-about-in-2025/) API与主机交互。这使得LLM能够通过控制本地资源和服务来执行复杂的运算。

虽然开发人员正在将MCP用于合法用途，例如自动化和集成，但其更黑暗的含义是显而易见的。启用MCP的系统可以轻松地协调一系列恶意活动。可以将其视为能够执行从侦察到漏洞利用等一切操作的AI驱动的操作员。

目前，这些功能可能会出现在白帽研究中，预示着攻击者如何使用此类工具。但恶意行为者效仿只是时间问题，这将在网络攻击中引入更高水平的复杂性和自主性。

## 3. 恶意模型

AI模型的激增既是福也是祸。像Hugging Face这样的平台托管超过一百万个模型，范围从最先进的神经网络到设计不良或恶意修改的版本。在这种丰富性中，一个日益增长的担忧是：模型来源。

想象一下，一个由看似信誉良好的维护者微调的广泛使用的模型，结果却成为国家行为者的工具。训练数据集或架构中的细微修改可能会嵌入偏差、漏洞或后门。然后，这些“恶意模型”可以作为可信资源分发，之后再被武器化。

这种风险强调了需要强大的机制来验证AI模型的来源和完整性。像Sigstore这样的倡议，它使用SLSA（软件工件的供应链级别）等工具来验证软件来源，必须将其工作扩展到包含AI模型和数据集。如果没有这样的保障措施，社区将容易受到大规模操纵。

## 4. 隐私风险和PII信息泄露

AI模型的训练依赖于海量数据，其中大部分是从互联网上抓取或由用户上传的。这些数据通常包括敏感的个人身份信息 (PII)、秘密和令牌。结果？模型会在其输出中无意中泄露这些敏感信息的片段。

考虑一下用户求助于AI进行治疗或个人指导的情况。如果这些互动中嵌入的PII包含在随后的训练周期中，则可能会作为模型输出的一部分重新出现。随着采用的增长，敏感数据泄露的风险也在增加。
这个问题可能会引发一场急需的隐私运动，用户将要求对其数据如何被使用有更大的透明度。“用户即产品”这句古老的格言在人工智能时代可能会获得新的意义，从而导致更严格的法规和技术保障。

## 降低风险：行动号召

随着网络安全环境的发展，开发者、企业和开源社区必须适应。人工智能带来的威胁，包括增强的黑客能力和隐私侵犯，是令人畏惧的，但并非不可克服。以下是三个需要关注的关键领域：

1. **标准化模型来源：**开源社区必须优先考虑[AI供应链](https://thenewstack.io/how-supply-chain-attackers-maximize-their-blast-radius/)中的透明度和验证。像Sigstore和SLSA这样的工具应该成为验证模型及其训练数据集的标准实践。
2. **构建防御性AI系统：**正如攻击者使用AI来增强其能力一样，防御者也必须这样做。这包括利用人工智能进行实时威胁检测、漏洞分析和异常检测，以领先于不断发展的威胁。
3. **隐私优先的AI实践：**保护用户数据应该是AI开发的基石。本地代理可以为编码助手提供基于隐私的保护，并且代表了朝着正确方向迈出的一步。更广泛地采用注重隐私的技术将至关重要。

## 结论

人工智能改变网络安全的潜力巨大，但风险也同样巨大。从代理增强黑客攻击到隐私侵犯，该行业正面临着需要积极解决方案的挑战。对可验证的AI模型、隐私保护和AI增强防御的需求从未如此迫切。

在Stacklok，我们致力于应对这些挑战。我们最近将[CodeGate](http://codegate.ai)，一个用于编码助手和代理的本地隐私保护系统，开源，这是我们致力于使AI既安全又值得信赖的使命的一部分。未来的道路充满不确定性，但通过警惕和合作，我们可以塑造一个AI增强安全而不是破坏安全的未来。
