# Developers: The Last Line of Defense Against AI Risks
![Featued image for: Developers: The Last Line of Defense Against AI Risks](https://cdn.thenewstack.io/media/2025/01/df5e4b89-janilson-furtado-ahagyhoyqb0-unsplash-1024x683.jpg)
[janilson furtado](https://unsplash.com/@janilson_oficial?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)on
[Unsplash](https://unsplash.com/photos/red-and-white-wooden-signage-AhAGyHoYqB0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash).
It’s no secret that AI/ML and [large language model](https://thenewstack.io/llm/) (LLM) technologies are reshaping the software development landscape. As the use of large language models becomes more pervasive, the [task of incorporating AI and ML into software products](https://thenewstack.io/innovating-software-development-with-ai-and-ml-pros-and-cons/) increasingly falls to developers. This innovative trend also introduces [significant security risks](https://thenewstack.io/more-ai-more-problems-for-software-developers-in-2025/), especially when developers lack the resources to ensure secure development practices at all times.

Security oversights can lead to errors no one intended, such as inserting malicious code into AI/ML models. No matter how small, a vulnerability can allow cybercriminals to distribute compromised open source software (OSS) models, which can then be used to breach corporate networks and cause extensive revenue and reputational damage.

Developers are also [increasingly using generative AI for code creation](https://thenewstack.io/should-developers-curb-their-enthusiasm-for-generative-ai/), which poses risks, as developers working at pace may not be able to vet the generated code’s security thoroughly. To tackle these threats, the code must undergo stringent security checks from the beginning, from the binary level, to prevent potential breaches in the software supply chain.

The challenges posed by these security vulnerabilities are ongoing and are expected to escalate as cybercriminals find new ways to exploit AI/ML technologies. Thus,[ developers must integrate security measures](https://thenewstack.io/a-guide-to-generative-ai-for-devops-team-managers/) into their workflows from the very start and adopt a proactive stance to protect their organization’s software supply chain.

Employees must also shoulder some of the responsibility in addition to the developer team’s duties. Promoting ongoing education and awareness of security best practices among developers is vital, and passing them on to wider teams will better safeguard the company. If everyone understands the latest security protocols, everyone stands to benefit as AI and ML technologies evolve.

**Blurring the Lines Between DevOps and DevSecOps**
Considering security early in the software development lifecycle has not traditionally been a standard practice amongst developers. Of course, this oversight is a goldmine for cybercriminals who exploit ML models to inject harmful malware into software. The lack of security training for developers makes the issue worse, particularly when AI-generated code, trained on potentially insecure open source data, is not adequately screened for vulnerabilities.

Unfortunately, once AI/ML models integrate such code, the potential for undetected exploits only increases. Therefore, developers must also function as security champions, and DevOps and Security can no longer be considered separate functions.

Investment in regular security training and providing resources to help developers anticipate threats are crucial. Enhancing collaboration between development and security teams will ensure that security measures are seamlessly integrated, creating a solid defense against threats. Embedding security at every stage of development is essential when deploying secure AI/ML solutions.

**Prioritizing Early Security Measures: The ‘Shift Left’ Approach**
As AI continues to be implemented at scale by different teams, the need for advanced security in ML models is key. Enter the “Shift Left” approach, which advocates for integrating security measures early in the software lifecycle to get ahead and prevent as many future vulnerabilities as possible and ensure comprehensive security throughout the development process. This strategy is critical in AI/ML development, before they’re even deployed, to ensure the security and compliance of code and models, which often come from external sources and sometimes cannot be trusted.

As AI and ML become integral to software development, robust security policies, practices, and education become imperative. Developers must combine their coding expertise with a strong understanding of security to address critical vulnerabilities early in the development process. By ensuring early and continuous security measures throughout the software lifecycle, the “Shift Left” approach ensures companies can maintain trust with customers and employees, mitigate risks, and protect against sophisticated cyberthreats.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)