# 使用 BGE-M3 生成学习型稀疏嵌入

![BGE-M3 生成学习型稀疏嵌入的特色图片](https://cdn.thenewstack.io/media/2024/06/52d3e45b-embedded-1024x576.jpg)

有时，开发人员在选择 LLM 检索方法时需要做出选择。他们可以使用传统的[稀疏嵌入或密集嵌入](https://zilliz.com/learn/sparse-and-dense-embeddings)。稀疏嵌入非常适合关键字匹配过程。我们通常在[自然语言处理](https://zilliz.com/learn/A-Beginner-Guide-to-Natural-Language-Processing?utm_source=vendor&utm_medium=referral&utm_campaign=2024-06-11_blog_bge-m3_tns) (NLP) 中找到稀疏嵌入，这些高维嵌入通常包含零值。这些嵌入中的维度表示一种（或多种）语言中的标记。它使用非零值来显示每个标记与特定文档的相关性。

另一方面，密集嵌入的维度较低，但它们不包含任何零值。顾名思义，密集嵌入充满了信息。这使得密集嵌入非常适合语义搜索任务，使匹配“含义”的精神而不是精确的字符串变得更容易。

BGE-M3 是一种[机器学习模型](https://thenewstack.io/creating-machine-learning-models-takes-too-much-time/)，用于创建一种称为“学习型稀疏嵌入”的先进嵌入类型。这些学习型嵌入的优点是它们结合了稀疏嵌入的精确性和密集嵌入的语义丰富性。该模型使用稀疏嵌入中的标记来学习哪些其他标记可能相关或关联，即使它们没有在原始搜索字符串中明确使用。最终，这将产生一个包含丰富相关信息的嵌入。

## 了解 BERT

来自 Transformer 的双向编码器表示（或[BERT](https://zilliz.com/learn/Sentence-Transformers-for-Long-Form-Text?utm_source=vendor&utm_medium=referral&utm_campaign=2024-06-11_blog_bge-m3_tns)）不仅仅是表面上的东西。它是使 BGE-M3 和[SPLADE](https://zilliz.com/learn/comparing-splade-sparse-vectors-with-bm25?utm_source=vendor&utm_medium=referral&utm_campaign=2024-06-11_blog_bge-m3_tns) 等高级机器学习模型成为可能的底层架构。

BERT 处理文本的方式不同于传统模型。它不是仅仅按顺序读取文本字符串，而是同时检查所有内容，并将所有组件之间的关系考虑在内。BERT 使用双管齐下的方法来做到这一点。这些是模型实现的单独的预训练任务，但它们的输出协同工作以丰富输入的含义。

- 掩码语言建模 (MLM)：首先，BERT 随机隐藏输入标记的一部分。然后，它使用模型来找出哪些选项对隐藏部分有意义。为此，它需要理解不仅单词顺序之间的关系，而且该顺序如何影响含义。
- 下一句预测 (NSP)：虽然 MLM 主要在句子级别起作用，但 NSP 进一步放大。此任务确保句子和段落逻辑地流动，因此它学习预测在这些更广泛的上下文中哪些内容有意义。

当 BERT 模型分析查询时，编码器的每一层都独立于其他层进行分析。这允许每一层生成独特的结果，不受其他编码器的影响。这样输出的是一个更丰富、更健壮的数据集。

了解 BERT 的功能非常重要，因为 BGE-M3 基于 BERT。以下示例演示了 BERT 的工作原理。

## BERT 的实际应用

让我们以一个基本查询为例，看看 BERT 如何从中创建嵌入：

**Milvus 是一个为可扩展相似性搜索而构建的向量数据库。**

第一步是将查询字符串中的单词转换为标记。

```
[CLS] Milvus 是 一个 为 可扩展 相似性 搜索 而 构建 的 向量 数据库 [SEP]
```

您会注意到，模型在标记的开头添加了 [CLS]，在结尾添加了 [SEP]。这些组件只是分别指示句子级别上句子开头和结尾的标记。

接下来，需要将标记转换为嵌入。

此过程的第一部分是嵌入。在这里，嵌入矩阵将每个标记转换为向量。接下来，BERT 添加位置嵌入，因为单词的顺序很重要，而此嵌入保持这些相对位置不变。最后，段嵌入只是跟踪句子之间的断点。

我们可以看到此时嵌入输出是单色的，以表示稀疏嵌入。为了实现更高的密度，这些嵌入会经过多个编码器。就像上面识别的独立工作的预训练任务一样，这些编码器也这样做。嵌入在通过编码器时会不断进行修改。序列中的标记为细化每个编码器生成的表示提供了重要的上下文。
一旦此过程完成，最终输出将比预编码器输出更密集的嵌入。当使用单个标记进行进一步处理或导致单个密集表示的任务时，尤其如此。

## BGE-M3 进入聊天
BERT 为我们提供了密集嵌入，但这里的目标是生成学习的稀疏嵌入。所以现在我们终于可以接触到 BGE-M3 模型了。

BGE-M3 本质上是一个高级机器学习模型，它通过专注于通过多功能性、多语言性和多粒度来增强文本表示，从而将 BERT 推向了更远。所有这些都是说，它不仅仅是通过生成学习的稀疏嵌入来创建密集嵌入，这些嵌入提供了两全其美的优势：词义和精确的词语选择。

## BGE-M3 的实际应用
让我们从与理解 BERT 相同的查询开始。运行查询会生成与上面看到的相同的上下文嵌入序列。我们可以将此输出称为 (Q)。

BGE-M3 模型深入研究这些嵌入，并试图在更细粒度的层面上理解每个标记的重要性。这方面有几个方面。

**标记重要性估计**: BGE-M3 不会将 [CLS] 标记表示 `Q[0]` 视为唯一可能的表示。它还会评估序列中每个标记 `Q[i]` 的上下文嵌入。**线性变换**: 该模型还会获取 BERT 输出，并使用线性层为每个标记创建重要性权重。我们可以将 BGE-M3 生成的权重集称为 `W_{lex}`。**激活函数**: 然后，BGE-M3 对 `W_{lex}` 和 `Q[i]` 的乘积应用线性整流单元 (ReLU) 激活函数，以计算每个标记的项权重 `w_{t}`。使用 ReLU 确保项权重为非负数，有助于嵌入的稀疏性。**学习的稀疏嵌入**: 最终的输出结果是一个稀疏嵌入，其中每个标记都有一个加权值，表示它对原始输入字符串的重要性。
## BGE-M3 在现实世界中的应用
将 BGE-M3 模型应用于现实世界的用例可以帮助证明这种机器学习模型的价值。这些是组织可以从该模型理解大量文本数据中的语言细微差别能力中获益的领域。

### 客户支持自动化 - 聊天机器人和虚拟助手
您可以使用 BGE-M3 为聊天机器人和虚拟助手提供动力，从而显着增强客户支持服务。这些聊天机器人可以处理各种客户查询，提供即时响应并理解复杂的问题和上下文信息。它们还可以从交互中学习，随着时间的推移不断改进。

优势：

* **全天候可用性**: 为客户提供全天候支持。
* **成本效益**: 减少对大型客户支持团队的需求。
* **改善客户体验**: 快速准确的响应提高了客户满意度。
* **可扩展性**: 可以同时处理大量查询，确保在高峰时段提供一致的服务。
### 内容生成和管理，用于营销和媒体
您可以利用 BGE-M3 为博客、社交媒体、广告等生成高质量内容。它可以根据所需的语气、风格和上下文创建文章、[社交媒体帖子](https://thenewstack.io/the-fediverse-points-to-our-social-media-future-post-musk/)，甚至完整的报告。您还可以使用此模型来总结长篇文档、创建摘要和生成产品描述。

优势：

* **效率**: 快速生成大量内容。
* **一致性**: 在不同的内容片段中保持一致的语气和风格。
* **降低成本**: 降低对大型内容创作团队的需求。
* **创造力**: 有助于集思广益和[生成创意内容](https://thenewstack.io/op-ed-the-rise-of-ai-content-generators-is-an-affront-to-creativity/)想法。
### 医疗数据分析 - 临床文档和分析
医疗保健领域的开发人员可以使用 BGE-M3 分析临床文档和患者记录，提取相关信息并帮助生成全面的医疗报告。它还可以帮助从大量医疗数据中识别趋势和见解，从而支持更好的患者护理和研究。

优势：

* **节省时间**: 减少医疗保健专业人员在文档方面花费的时间。
* **准确性**: 提高医疗记录和报告的准确性。
* **洞察力生成**: 识别可以为更好的临床决策提供信息的模式和趋势。
* **合规性**: 有助于确保文档符合监管标准。
## 结论
BGE-M3 模型提供了高度的通用性和先进的自然语言处理能力，这些能力在各个行业和部门都有应用，可以显着提高运营效率和服务质量。

[
YOUTUBE.COM/THENEWSTACK
科技发展迅速，不要错过任何一集。订阅我们的 YouTube 频道，观看所有播客、访谈、演示等。

[https://youtube.com/thenewstack?sub_confirmation=1](https://youtube.com/thenewstack?sub_confirmation=1)