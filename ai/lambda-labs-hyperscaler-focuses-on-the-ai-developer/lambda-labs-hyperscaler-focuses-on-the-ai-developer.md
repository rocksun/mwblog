
<!--
title: Lambda HyperScaler 专注于 AI 开发者
cover: https://cdn.thenewstack.io/media/2025/03/36e838c5-aicloudfordevelopers.jpg
-->

Lambda 是一个由机器学习工程师构建的 AI 云超大规模计算平台，但它已经演变成一个面向所有 AI 开发者的平台。

> 译自：[Lambda HyperScaler Focuses on the AI Developer](https://thenewstack.io/lambda-labs-hyperscaler-focuses-on-the-ai-developer/)
> 
> 作者：Loraine Lawson

人工智能 (AI) 超大规模厂商 [Lambda](https://lambdalabs.com/) 知道一个小秘密：并非所有 AI 开发者都是软件开发者，[Robert Brooks IV](https://www.linkedin.com/in/boborado/) 表示，他是 Lambda 创始团队成员兼收入副总裁。

“这些人不一定是软件工程师。他们是数学专家；那是他们的技能所在，”Brooks 告诉 The New Stack。“如果你允许这些人不必专注于运行云的 DevOps 方面，而只专注于数学，也就是构建机器学习模型，这就是我们在该领域建立一个大品牌的原因。”

## Lambda 专注于 AI 开发者

Lambda 以 Alonzo Church 的 Lambda 演算命名，历史上专注于[机器学习工程师](https://thenewstack.io/5-new-kubeflow-1-3-features-that-machine-learning-engineers-will-love/)——主要是在 AI 市场的训练和微调方面。这意味着开发者要么从头开始构建，要么采用开源模型并根据自己的需求进行微调。

现在，它将自己定位为所有 AI 开发者（从硬核深度学习研究人员到将机器学习 API 插入应用程序[前端](https://thenewstack.io/introduction-to-frontend-development)的 Web 开发者）的超大规模厂商。

“实际上，在过去两年中，我们看到推理在我们的平台上呈爆炸式增长，因为 NVIDIA 芯片的最大优点是它具有训练和推理的双重用途，”Brooks 说。“我们看到与软件工程师相关的用户增长呈爆炸式增长，[他们]在他们的代码中使用 AI。”

在 AI 中，推理是指[经过训练的机器学习模型](https://thenewstack.io/machine-learning-for-real-time-data-analysis-training-models-in-production/)使用其知识对新的、未见过的数据进行预测或决策的过程。它本质上是创建 AI 输出的东西。

## 为什么要有 AI 开发者云？

Lambda 决定专注于 AI 开发者超级云的原因非常简单：他们是[想要构建](https://thenewstack.io/the-machine-learning-building-blocks-developers-require-to-do-mlops/)支持他们自己应用程序的基础设施的机器学习工程师。

该平台拥有 25,000 个 [GPU](https://thenewstack.io/revolutionizing-storage-the-role-of-gpus-in-modern-infrastructure/)，这使其成为“世界上为数不多的能够使用我们的基础设施来正确地为这些应用程序提供服务的公司之一”，Brooks 说。

“很少有公司拥有像我们一样多的 [NVIDIA](https://thenewstack.io/a-developers-guide-to-nim-nvidias-ai-application-platform/) GPU。这就是我们走上这条特定道路的原因，”他补充说。“我们部署基础设施的原因是[因为]我们最终希望能够使用它，因为我们实际上是机器学习工程师。”

> “因为我们拥有所有这些基础设施，所以我们允许人们与我们一起成长，而不是受到人为的限制——对这个词没有双关语。”
>
> — Robert Brooks IV, Lambda 创始团队成员兼收入副总裁

他们的创始人 [Stephen Balaban](https://www.linkedin.com/in/sbalaban/) 是一位机器学习工程师，当 AI 开始能够区分狗和猫时，他参加了一场计算机视觉竞赛。他构建了一个在 iPhone 上本地运行的计算机视觉应用程序 DreamScope，该应用程序使用粗糙的、定制的[神经网络](https://thenewstack.io/who-needs-neural-networks-the-generative-prowess-of-state-transition-models/)来完成今天生成式 AI 所做的事情，Brooks 说。

“最终，通过构建这些应用程序，我们越来越依赖 NVIDIA GPU 来构建 AI 并将其投入生产，”他说。“因此，我们在 2015 年、2016 年遇到了这个问题，并在 2017 年左右完全转向为其他 AI 团队构建此基础设施，因为我们非常熟悉这个问题。”

据他们所知，Lambda 发布了世界上成本最低的推理 API 之一，Brooks 声称。该公司有目的地做出了这个决定。

“我们是垂直整合的，因此我们有很多竞争对手只是建立在其他人的 GPU 云之上，但因为我们实际上拥有基础设施，所以我们可以更好地控制成本，”他说。“我们拥有数据中心。我们拥有 GPU。我们拥有一切。我们编写所有软件。我们实际上将自己定位为最便宜的推理 API。它没有速率限制。”

公司通常允许开发者每小时、每天或每月使用高达 X 百万个令牌，但他表示，然后开发者必须与销售人员交谈。
因为我们拥有所有这些基础设施，所以我们允许人们与我们一同成长，而不是受到人为的限制——这里用这个词并非双关语，”Brooks补充道。

Brooks说，许多人担心GPU的保质期，因为英伟达开始每两年推出新的GPU，而历史上这个周期是每三到四年。Lambda 不用担心这个问题，因为它现在为客户部署 GPU，一旦合同结束，Lambda 会将这些 GPU 用于他们自己的 AI 训练和服务。

“这形成了一个非常好的反馈循环，因为我们是机器学习工程师，运营着一个云平台，这使我们能够更快地成长，并随着时间的推移产生更多的 AI 研究和更多的机器学习软件，”他说。

## AI 开发者的工具

这家初创公司帮助机器学习工程师和 AI 开发者的一种方式是，它采用了一种即插即用的方式来入门。

Brooks说：“当机器学习工程师或 AI 开发者进入我们的云平台并启动 GPU 时，他们需要依赖的所有工具、框架、库——本质上，他们的应用商店——都已经预先构建好并在其中，因此他们实际上可以专注于他们的工作。”

Lambda 推出了其 [推理 API](https://docs.lambdalabs.com/public-cloud/lambda-inference-api/)，该 API 于 12 月全面上市。它允许开发者在他们的应用程序中使用 AI 语言模型，而无需管理复杂的基础设施。

“我们现在有专门的服务来帮助客户在他们的应用程序中实际提供模型。这是一个很好的、逐渐上升的趋势，朝着更加以生产为中心的 AI，或者我们称之为推理的方向发展，”Brooks说。

它还推出了 [Lambda Chat](https://lambda.chat/)，它提供了对各种 [大型语言模型](https://thenewstack.io/why-large-language-models-wont-replace-human-coders/) 的访问，其中许多是开源的。Brooks说，Lambda Chat 是他们首次涉足 [使用开源模型为消费者提供服务](https://thenewstack.io/google-serves-up-cloud-gpus-with-a-side-of-open-source-llms/)。消费者可以免费使用，但 Lambda 会向开发者收取 API 调用费用。

上个月，该公司 [筹集了 4.8 亿美元的 D 轮融资](https://lambdalabs.com/blog/lambda-raises-480m-to-expand-ai-cloud-platform?)，以扩展其 AI 云平台。该计划是将这笔钱用于为 AI 开发者构建更多的软件工具，并部署更多的 GPU 以满足客户的需求。

该公司还表示，它将继续开发 Lambda Chat，该平台托管了 [DeepSeek-R1](https://thenewstack.io/deep-dive-into-deepseek-r1-how-it-works-and-what-it-can-do/) 和其他开源模型。

## DeepSeek 进入 Chat

TNS 询问 Brooks，DeepSeek R1 如何改变了超大规模企业中的 AI 现状。DeepSeek R1 是一个大型的 6710 亿参数的混合专家 (MoE) 模型，这意味着它旨在通过策略性地激活大型模型中不同的“专家”子网络来处理复杂的任务。它因能够以更少的 GPU 进行 AI 而成为头条新闻。

Brooks 补充说，DeepSeek 的推出导致了需求的增长。

“我们经历了来自这方面的极端需求，不仅与机器学习和工程师在我们的云上启动实例有关，而且还与人们使用我们的推理 API 插入到他们的应用程序中有关，”他说。“因此，这在开始将更多的功能扩展到更多的应用程序中方面产生了一种民主化的影响。”

但最终，Deep Seek 本身没有足够的 GPU 来运行生产，他说。这就是为什么你可以提出的问题数量有限制。

他说，Lambda 为许多大型企业提供这些 4,000 到 8,000 个 GPU 集群的私有云构建。企业有兴趣运行 DeepSeek，但使用更多的 GPU。

## 竞争对手

其他提供 GPU 即服务的公司包括：

- [Nebius](https://nebius.com/)，它提供机器学习托管服务；
- [Beam](https://www.beam.cloud/)，它拥有面向开发者的 AI 基础设施；
- [Cerebrium](https://www.cerebrium.ai/)，它为机器学习工程师销售无服务器服务；
- [SaturnCloud](https://saturncloud.io/)，它面向 [数据科学家](https://roadmap.sh/ai-data-scientist)；以及 
- [Modal](https://thenewstack.io/serverless-for-ai-devs-modals-python-and-rust-based-platform/)。

