# Tecton Tackles Next Big GenAI Challenge: Personalization
![Featued image for: Tecton Tackles Next Big GenAI Challenge: Personalization](https://cdn.thenewstack.io/media/2024/10/1ecae9cc-tecton-1024x683.jpg)
Just having a well-tuned model is not enough to enjoy success with generative AI. You also need to connect it firmly to the business practices.

“Everybody has access to the same models now. Everybody can go use OpenAI or Anthropic and so everyone is at the same lowest common denominator with one part of the AI application,” explained [Mike Del Balso](https://www.linkedin.com/in/michaeldelbalso/), CEO and co-founder of [Tecton](https://www.tecton.ai/), in a conversation with The New Stack. “The competitive advantage really comes from what data you have collected and what data you can continue to collect and use.”

## Make It to Production
Tecton’s mission is to [provide a platform](https://thenewstack.io/tecton-helps-data-scientists-own-features-and-the-model-lifecycle/) to do the work of the thousands of engineers that scale out companies like Uber and Netflix enjoy, so smaller businesses can get similar results, allowing them to test new AI applications for their data and then productize the results really easily.

For Tecton, every company has the possibility of becoming a Pinterest, or Uber, or Instagram; they all simple repackage the data generated by users and feed it back to them in novel and useful ways.

The company quotes Gartner, which found that only [53% of AI projects ever make it to production](https://www.forbes.com/councils/forbestechcouncil/2023/04/10/why-most-machine-learning-applications-fail-to-deploy/#), suggesting that organizations of having difficulty productizing their experiments with[ large language models](https://thenewstack.io/how-to-increase-plasticity-in-llms-and-ai-applications/) (LLMs) within their own dynamic business environments. The [LLMs need specific company and customer](https://thenewstack.io/new-ai-dev-platform-allows-you-to-customize-open-source-llms/) information in order to personalize the information meaningfully for the user.

Prior to ramping up, Tecton Del Balso and Tecton CTO [Kevin Stumpf](https://www.linkedin.com/in/kevinstumpf/) were on the Uber team that built out one of the first machine learning platforms for Uber, called [Michelangelo](https://www.uber.com/blog/michelangelo-machine-learning-platform/), which [focused on making ML processes accessible](https://www.tecton.ai/resources/how-michelangelo-ml-enabled-uber-to-scale-up-its-ml-models-mike-del-balso-tecton) for business development teams.

“If you get the tooling right, it enables the company to scale its application of AI really quickly,” Del Balso said, noting Michelangelo allowed Uber to go from no AI to thousands of models in production in two years.

Amassing data is an important process in the ML model build, but so is connecting the models to business processes.

## Tecton 1.0
Now, the company has [expanded its platform](https://www.tecton.ai/blog/expanding-tecton-to-activate-data-for-genai/) to help organizations personalize their generative AI apps, to better meet the needs of its users.

One significant new feature is Embeddings. This service transforms unstructured text into numerical vectors that capture semantic meaning, so it can be used by [a generative AI app](https://thenewstack.io/agents-shift-genai-from-order-takers-to-collaborators/). It could be used, for instance, for capturing [sentiment](https://thenewstack.io/machine-learning-for-twitter-sentiment-analysis/) and key aspects of user reviews. Tecton manages the entire embedding process, providing an software development kit (SDK) for users to identify which tables to embed.

“So now we can process both structured and unstructured data as embeddings, and the embeddings can then be used for both traditional machine learning models and generative AI models, and served in real time,” Del Balso said.

![Tecton personalization chart.](https://cdn.thenewstack.io/media/2024/10/b7991ec3-tecton-image2-1024x389.webp)
Source: Tecton.

Another new feature is the Feature Retrieval API. This integration provides a way for LLMs to access real-time and streaming data that could be able to user behavior, transactions, and operational metrics. These can be used to customize the user experience with context about the individual (or, as Del Balso calls it, “hyper-personalization”)

For a customer service application, an LLM-driven application could also incorporate data from a customer’s recent purchases. A travel site could recommend new destinations based on what is known about the customer’s previous journeys. A foodie site could recommend a restaurant that just opened that specializes in the user’s favorite cuisines.

In almost every company, “there’s data locked away in certain ways, and the people that they have working on the AI team are not also the people who are going to build a whole set of data infrastructure to connect these two things. You really do need a really high-quality bridge between those systems to make both your data and your AI application work together as an AI product,” Del Balso said. “And we’ve built that bridge.”

Other new features of the just-released Tecton 1.0 that aid in personalization include:

**Dynamic, version-controlled prompts**provide a way for an application inject personal data into the prompt itself. Tecton also offers version control, change tracking, and easy rollback of prompts when necessary.**Natural language interface for engineers,**which leverages LLMs to allow developers and engineers to state the features or data formats they want to build through natural language.
“The developers love it,” Del Balso said. “It makes some things faster for them, but it also just enables things that have never really been possible for them before.”

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)