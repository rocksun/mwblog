<!--
title: 大模型降本增效：令牌高效数据预处理指南
cover: https://cdn.thenewstack.io/media/2025/12/6eace39e-token.jpg
summary: RAG和AI系统因低效数据序列化浪费40-70%令牌，导致成本增高、性能下降。解决方案是消除结构冗余、优化精度和分层扁平化，以提高令牌效率和经济可持续性。
-->

RAG和AI系统因低效数据序列化浪费40-70%令牌，导致成本增高、性能下降。解决方案是消除结构冗余、优化精度和分层扁平化，以提高令牌效率和经济可持续性。

> 译自：[A Guide to Token-Efficient Data Prep for LLM Workloads](https://thenewstack.io/a-guide-to-token-efficient-data-prep-for-llm-workloads/)
> 
> 作者：Minav Suresh Patel

随着组织将[检索增强生成 (RAG)](https://thenewstack.io/how-to-scale-rag-and-build-more-accurate-llms/) 架构和代理驱动的 AI 系统投入生产，一个关键的性能问题正在浮现：糟糕的数据[序列化](https://thenewstack.io/working-with-json-data-in-python/)通过不必要的格式开销消耗了 40% 到 70% 的可用令牌。这导致 API 成本膨胀、有效上下文窗口减少以及模型性能下降。

在数据量有限的试点阶段，这个问题通常不被注意，但在规模化时会变得非常严重。一个低效序列化的记录可能会浪费数百个[令牌](https://thenewstack.io/what-is-an-llm-token-beginner-friendly-guide-for-developers/)。如果将其乘以数百万次查询，成本影响将变得巨大，往往代表着经济上可行的 AI 部署与不可持续的基础设施成本之间的差异。

## **理解规模化时的令牌浪费**

[大型语言模型](https://thenewstack.io/7-guiding-principles-for-working-with-llms/) (LLM) 应用程序中的令牌消耗通常分为几类，但序列化开销是最大的优化机会之一。[理解令牌化](https://thenewstack.io/what-is-an-llm-token-beginner-friendly-guide-for-developers/)对于有效的 AI 实现至关重要，它直接影响模型性能和成本。

考虑一个需要来自多个数据源的上下文的标准企业查询：

*   历史记录（20-50 条）
*   实体元数据
*   行为模式
*   实时信号

使用 JSON 序列化，这个上下文通常会消耗 3,000 到 4,000 个令牌。在 8,192 个令牌的上下文窗口中，这留给实际分析的空间非常有限。对于需要更深层上下文或多轮对话的应用程序来说，这[成为一个关键限制](https://thenewstack.io/wrangling-data-is-becoming-critical-in-an-ai-driven-world/)。

开销通常分布如下：

[![](https://cdn.thenewstack.io/media/2025/12/663a0632-image-1.png)](https://cdn.thenewstack.io/media/2025/12/663a0632-image-1.png)

最后一个类别，结构化格式，代表纯粹的低效率。在数千条记录中重复的字段名和 JSON 语法消耗了令牌，却没有传达模型所需的信息。

## **3 大核心优化策略**

有效的令牌优化需要从三个维度采取系统化方法：

### **1. 消除结构冗余**

JSON 的冗长使其易于人类阅读，但令牌效率低下。支持模式的格式可以消除重复的结构：

### **2. 优化数值精度**

LLM 在分析任务中很少需要毫秒级的精度。精度感知的格式化可以将数值令牌消耗减少 30% 到 40%：

[![](https://cdn.thenewstack.io/media/2025/12/716cd909-image-3.png)](https://cdn.thenewstack.io/media/2025/12/716cd909-image-3.png)

**实施方法：** 通过测试确定精度要求。大多数业务应用程序在以下方面表现良好：

*   货币：两位小数
*   时间戳：分钟级精度
*   坐标：两到三位小数
*   百分比：一到两位小数

通过 A/B 测试验证降低的精度不会影响您的特定用例的模型准确性。

### **3. 应用分层扁平化**

嵌套的 JSON 结构会产生显著的开销。扁平化层次结构以仅包含基本字段：

[![](https://cdn.thenewstack.io/media/2025/12/7e9abca6-image-4.png)](https://cdn.thenewstack.io/media/2025/12/7e9abca6-image-4.png)

这种 69% 的减少来自于提取与任务相关的字段并消除不必要的嵌套。

**实施方法：** 分析您的查询中模型实际需要哪些字段。删除：

*   冗余标识符（保留一个主键）
*   内部系统字段
*   可以扁平化的高度嵌套结构
*   很少影响模型输出的字段

## **构建预处理管道**

有效的优化需要在数据检索和 LLM 推理之间建立一个系统化的预处理层。随着组织[规模化 RAG 系统](https://thenewstack.io/a-blueprint-for-implementing-rag-at-scale/)，高效数据准备的需求变得至关重要，尤其是在处理无法整体传递给 LLM 的大规模文档语料库时。

**关键组件：**

*   **模式检测：** 自动识别数据类型和结构。
*   **压缩规则：** 根据数据类型应用格式转换。
*   **去重：** 删除记录中重复的结构。
*   **令牌计数：** 监控和强制执行令牌预算。
*   **验证：** 确保压缩数据保持语义完整性。

**配置驱动方法：** 不同的用例需要不同的压缩级别。高精度分析可能需要更完整的上下文，而常规查询则受益于积极的压缩。在您的管道中构建灵活性，以便根据查询类型进行调整。

## **预期性能影响**

实施这些策略的组织通常会看到：

**令牌效率：**

*   上下文大小减少 60% 到 70%。
*   有效上下文容量增加两到三倍。
*   每查询令牌成本按比例降低。

**性能指标：**

*   保持或提高准确性（通过 A/B 测试验证）。
*   减少查询延迟（需要处理的数据更少）。
*   消除上下文窗口耗尽。

**成本影响：**

*   大规模运行时 API 成本显著降低。
*   相同基础设施成本下容量增加两到三倍。

随着[AI 支出持续挑战企业预算](https://thenewstack.io/what-does-ai-cost-no-one-knows/)，成本影响变得尤为重要。令牌优化直接解决了生产 LLM 部署中的一个关键成本驱动因素。

## **重要注意事项**

*   **格式选择很重要。** 对于表格数据，CSV 比 JSON 性能高出 40% 到 50%。当您可以控制序列化的两端时，自定义紧凑格式可以实现更高的效率。
*   **精度需要验证。** 不要假设安全的精度水平；进行测试。许多应用程序可以容忍比最初预期更多的精度降低。
*   **上下文很重要。** [代理工作流需要与 RAG 管道不同的优化](https://thenewstack.io/rag-and-model-optimization-a-practical-guide-to-ai/)。对话历史记录需要另一种方法。为不同的用例维护多个压缩配置文件。随着[高级检索增强生成 (RAG) 技术](https://thenewstack.io/advanced-retrieval-augmented-generation-rag-techniques/)的发展，数据准备策略也必须相应调整。
*   **持续监控。** 将令牌效率作为与准确性和延迟并列的一等指标进行跟踪。效率下降表明数据漂移或序列化问题。

## **商业案例**

令牌浪费的经济影响在规模化时迅速累积：

*   每查询浪费 1,000 个令牌
*   × 每日 1,000 万次查询
*   × 每 1,000 个令牌 0.002 美元
*   = 每日浪费 20,000 美元（每年 730 万美元）

令牌优化不仅仅是成本降低；它更是能力提升。更好的序列化能够实现更有效的上下文，从而以更低的成本驱动更好的模型性能。这是使生产 AI 在经济上可持续的优化。

## **开始行动**

首先，对您当前的令牌使用情况进行检测。大多数组织发现现有序列化方法中存在 40% 到 60% 的浪费。测量整个数据管道的令牌消耗，识别影响最大的优化机会，并逐步实施更改，在每个步骤进行验证。

LLM 优化中最容易实现的部分不在模型本身——它在于为模型提供数据的数据准备层。