
<!--
title: ä½¿ç”¨LangChainã€Chainlitå’ŒLiteral AIæ„å»ºä¸€ä¸ªå¯è§‚æµ‹çš„arXiv RAGèŠå¤©æœºå™¨äºº
cover: https://miro.medium.com/v2/resize:fit:1024/1*GUoz9w5z8FLzXHmQUarXyg.png
-->

ä½¿ç”¨ RAGã€LangChainã€Chainlit åä½œåº”ç”¨ç¨‹åºå’Œ Literal AI å¯è§‚æµ‹æ€§æ„å»ºè¯­ä¹‰è®ºæ–‡å¼•æ“çš„æ•™ç¨‹ã€‚

> è¯‘è‡ª [Building an Observable arXiv RAG Chatbot with LangChain, Chainlit, and Literal AI](https://towardsdatascience.com/building-an-observable-arxiv-rag-chatbot-with-langchain-chainlit-and-literal-ai-9c345fcd1cd8)ï¼Œä½œè€… Tahreem Rasulã€‚

åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æ„å»ºè¯­ä¹‰ç ”ç©¶è®ºæ–‡å¼•æ“ã€‚æˆ‘å°†åˆ©ç”¨
[LangChain](https://www.langchain.com) ä½œä¸ºæ„å»ºè¯­ä¹‰å¼•æ“çš„ä¸»è¦æ¡†æ¶ï¼Œä»¥åŠ [OpenAI](https://openai.com) çš„è¯­è¨€æ¨¡å‹å’Œ [Chroma DB](https://www.trychroma.com) çš„å‘é‡æ•°æ®åº“ã€‚å¯¹äºæ„å»ºåä½œåµŒå…¥å¼ Web åº”ç”¨ç¨‹åºï¼Œæˆ‘å°†ä½¿ç”¨ [Chainlit](https://docs.chainlit.io/get-started/overview) çš„åä½œåŠŸèƒ½ï¼Œå¹¶æ•´åˆ [Literal AI](https://literalai.com) çš„å¯è§‚æµ‹æ€§åŠŸèƒ½ã€‚æ­¤å·¥å…·å¯ä»¥é€šè¿‡æ›´è½»æ¾åœ°æŸ¥æ‰¾ç›¸å…³è®ºæ–‡æ¥ä¿ƒè¿›å­¦æœ¯ç ”ç©¶ã€‚ç”¨æˆ·è¿˜èƒ½å¤Ÿé€šè¿‡è¯¢é—®æœ‰å…³æ¨èè®ºæ–‡çš„é—®é¢˜æ¥ç›´æ¥ä¸å†…å®¹è¿›è¡Œäº¤äº’ã€‚æœ€åï¼Œæˆ‘ä»¬å°†åœ¨åº”ç”¨ç¨‹åºä¸­é›†æˆå¯è§‚æµ‹æ€§åŠŸèƒ½ï¼Œä»¥è·Ÿè¸ªå’Œè°ƒè¯•å¯¹ LLM çš„è°ƒç”¨ã€‚

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*GUoz9w5z8FLzXHmQUarXyg.png)

*CopilotåµŒå…¥å¼è¯­ä¹‰ç ”ç©¶è®ºæ–‡åº”ç”¨ç¨‹åºçš„åº”ç”¨ç¨‹åºæ¶æ„ã€‚ä½œè€…æä¾›çš„æ’å›¾ã€‚*

ä»¥ä¸‹æ˜¯æœ¬æ•™ç¨‹ä¸­æˆ‘ä»¬å°†æ¶µç›–çš„æ‰€æœ‰å†…å®¹çš„æ¦‚è¿°ï¼š

- ä½¿ç”¨ OpenAIã€LangChain å’Œ Chroma DB å¼€å‘ RAG ç®¡é“ï¼Œä»¥å¤„ç†å’Œæ£€ç´¢ arXiv API ä¸­æœ€ç›¸å…³çš„ PDF æ–‡æ¡£ã€‚
- å¼€å‘ä¸€ä¸ªå¸¦æœ‰åä½œåŠŸèƒ½çš„ Chainlit åº”ç”¨ç¨‹åºï¼Œç”¨äºåœ¨çº¿è®ºæ–‡æ£€ç´¢ã€‚
- ä½¿ç”¨ Literal AI å¢å¼ºåº”ç”¨ç¨‹åºçš„ LLM å¯è§‚æµ‹æ€§åŠŸèƒ½ã€‚

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*jtr_Sbj0Dt7nIBV2JAesuA.gif)

*CopilotåµŒå…¥å¼è¯­ä¹‰ç ”ç©¶è®ºæ–‡åº”ç”¨ç¨‹åºå®æˆ˜ã€‚ä½œè€…æä¾›ã€‚*

å¯ä»¥åœ¨[æ­¤ GitHub ä»“åº“](https://github.com/tahreemrasul/semantic_research_engine) ä¸­æ‰¾åˆ°æœ¬æ•™ç¨‹çš„ä»£ç ã€‚

## ç¯å¢ƒè®¾ç½®

åˆ›å»ºä¸€ä¸ªæ–°çš„ conda ç¯å¢ƒï¼š

```
conda create -n semantic_research_engine python=3.10
```

æ¿€æ´»ç¯å¢ƒï¼š

```
conda activate semantic_research_engine
```

é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤åœ¨å·²æ¿€æ´»çš„ç¯å¢ƒä¸­å®‰è£…æ‰€æœ‰å¿…éœ€çš„ä¾èµ–é¡¹ï¼š

```
pip install -r requirements.txt
```

## RAG ç®¡é“åˆ›å»º

æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) æ˜¯ä¸€ç§æµè¡Œçš„æŠ€æœ¯ï¼Œå®ƒå…è®¸æ‚¨ä½¿ç”¨è‡ªå·±çš„æ•°æ®æ„å»ºè‡ªå®šä¹‰ä¼šè¯å¼ AI åº”ç”¨ç¨‹åºã€‚RAG çš„åŸç†éå¸¸ç®€å•ï¼šæˆ‘ä»¬å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºå‘é‡åµŒå…¥ï¼Œå¹¶å°†è¿™äº›åµŒå…¥æ’å…¥åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚ç„¶åå°†æ­¤æ•°æ®åº“é“¾æ¥åˆ°å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)ã€‚æˆ‘ä»¬æ­£åœ¨é™åˆ¶æˆ‘ä»¬çš„ LLM ä»æˆ‘ä»¬è‡ªå·±çš„æ•°æ®åº“ä¸­è·å–ä¿¡æ¯ï¼Œè€Œä¸æ˜¯ä¾èµ–äºå…ˆéªŒçŸ¥è¯†æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢ã€‚åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä¸ºæˆ‘ä»¬çš„è¯­ä¹‰ç ”ç©¶è®ºæ–‡å¼•æ“æ‰§è¡Œæ­¤æ“ä½œã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåä¸º rag_test.py çš„æµ‹è¯•è„šæœ¬ï¼Œä»¥äº†è§£å’Œæ„å»º RAG ç®¡é“çš„ç»„ä»¶ã€‚åœ¨æ„å»ºæˆ‘ä»¬çš„åä½œé›†æˆ Chainlit åº”ç”¨ç¨‹åºæ—¶ï¼Œå°†é‡å¤ä½¿ç”¨è¿™äº›ç»„ä»¶ã€‚

### æ­¥éª¤ 1

é€šè¿‡æ³¨å†Œä¸€ä¸ªå¸æˆ·æ¥ä¿æŠ¤ OpenAI API å¯†é’¥ã€‚å®Œæˆåï¼Œåœ¨é¡¹ç›®ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ª .env æ–‡ä»¶ï¼Œå¹¶æŒ‰å¦‚ä¸‹æ–¹å¼æ·»åŠ æ‚¨çš„ OpenAI API å¯†é’¥ï¼š

```
OPENAI_API_KEY="your_openai_api_key"
```

æ­¤ .env å°†å®¹çº³æˆ‘ä»¬é¡¹ç›®çš„æ‰€æœ‰ API å¯†é’¥ã€‚

### æ­¥éª¤ 2ï¼šå¯¼å…¥

åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ•°æ®åº“æ¥å­˜å‚¨ç»™å®šç”¨æˆ·æŸ¥è¯¢çš„ç ”ç©¶è®ºæ–‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä» arXiv API ä¸­æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„è®ºæ–‡åˆ—è¡¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ LangChain ä¸­çš„ ArxivLoader() åŒ…ï¼Œå› ä¸ºå®ƒæŠ½è±¡äº† API äº¤äº’ï¼Œå¹¶æ£€ç´¢è®ºæ–‡ä»¥ä¾›è¿›ä¸€æ­¥å¤„ç†ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™äº›è®ºæ–‡åˆ†æˆæ›´å°çš„å—ï¼Œä»¥ç¡®ä¿ä»¥åè¿›è¡Œé«˜æ•ˆå¤„ç†å’Œç›¸å…³ä¿¡æ¯æ£€ç´¢ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LangChain ä¸­çš„ RecursiveTextSplitter()ï¼Œå› ä¸ºå®ƒç¡®ä¿åœ¨æ‹†åˆ†æ–‡æ¡£æ—¶ä¿ç•™ä¿¡æ¯çš„è¯­ä¹‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [HuggingFace](https://python.langchain.com/docs/integrations/platforms/huggingface/#embedding-models) ä¸­çš„ sentence-transformers åµŒå…¥ä¸ºè¿™äº›å—åˆ›å»ºåµŒå…¥ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›æ‹†åˆ†æ–‡æ¡£åµŒå…¥å¯¼å…¥ Chroma DB æ•°æ®åº“ä¸­ä»¥ä¾›è¿›ä¸€æ­¥æŸ¥è¯¢ã€‚

```python
# rag_test.py 
from langchain_community.document_loaders import ArxivLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

query = "lightweight transformer for language tasks"
arxiv_docs = ArxivLoader(query=query, load_max_docs=3).load()
pdf_data = []
for doc in arxiv_docs:
    text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=100)
    texts = text_splitter.create_documents([doc.page_content])
    pdf_data.append(texts)

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-l6-v2")
db = Chroma.from_documents(pdf_data[0], embeddings)
```

## æ­¥éª¤ 3ï¼šæ£€ç´¢å’Œç”Ÿæˆ

åœ¨ä¸ºç‰¹å®šä¸»é¢˜åˆ›å»ºæ•°æ®åº“ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ­¤æ•°æ®åº“ä½œä¸ºæ£€ç´¢å™¨ï¼Œæ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚LangChain ä¸ºæ£€ç´¢æä¾›äº†å‡ ä¸ªä¸åŒçš„é“¾ï¼Œæœ€ç®€å•çš„å°±æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„ RetrievalQA é“¾ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ from_chain_type() æ–¹æ³•å¯¹å…¶è¿›è¡Œè®¾ç½®ï¼ŒæŒ‡å®šæ¨¡å‹å’Œæ£€ç´¢å™¨ã€‚å¯¹äºå°†æ–‡æ¡£é›†æˆåˆ° LLM ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ stuff é“¾ç±»å‹ï¼Œå› ä¸ºå®ƒä¼šå°†æ‰€æœ‰æ–‡æ¡£å¡«å……åˆ°ä¸€ä¸ªæç¤ºä¸­ã€‚

```python
# rag_test.py
from langchain.chains import RetrievalQA
from langchain_openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)
qa = RetrievalQA.from_chain_type(llm=llm, 
                                 chain_type="stuff", 
                                 retriever=db.as_retriever())

question = "how many and which benchmark datasets and tasks were 
            compared for light weight transformer?"
result = qa({"query": question})
```

Now that we have covered the online retrieval from the arXiv API and the ingestion and retrieval steps of the RAG pipeline, we are ready to develop a web application for the semantic search engine.

## Understanding Literal AI

[Literal AI](https://literalai.com) is an observability, evaluation, and analytics platform for building production-grade LLM applications. Some of the key features that Literal AI provides include:

- **Observability**: Ability to monitor LLM applications, including conversations, intermediate steps, prompts, etc.
- **Datasets**: Allows for the creation of datasets mixing production data and hand-written examples.
- **Online Evaluation**: Ability to evaluate threads and their performance in production using different evaluators.
- **Prompt Playground**: Allows for iteration, versioning, and prompt deployment.

We will use the observability and prompt iteration features to evaluate and debug the calls made using our semantic search paper application.

## Using Literal AI's Prompt Playground

When creating conversational AI applications, developers need to iterate over multiple versions of prompts to get the one that produces the best results. Prompt engineering plays a crucial role in most LLM tasks, as small modifications can significantly change the response of the language model. Literal AI's Prompt Playground can be used to simplify this process. After selecting the model provider, you can enter the initial prompt template, add any additional information, and iterate to optimize the prompt to find the most suitable one. In the next few steps, we will use this playground to find the best prompt for our application.

### Step 1

Create an API key by navigating to the [Literal AI Dashboard](https://cloud.getliteral.ai/). Sign up for an account, navigate to the **Projects** page, and create a new project. Each project has its unique API key. You will find your API key in the **Settings** tab, under the **API Keys** section. Add it to your `.env` file:

```
LITERAL_API_KEY="your_literal_api_key"
```

### Step 2

In the left sidebar, click on **Prompts**, then navigate to **New Prompt**. This should open a new prompt creation session.

Once in the playground, in the left sidebar, add a new **System** message in the **Template** section. Anything in curly braces will be added to the **Variables** and treated as input in the prompt:

```
You are a helpful assistant. Answer the user's {{question}} using the provided {{context}}. Do not use any prior knowledge.

Answer:
```

In the right sidebar, you can provide your OpenAI API key. Select parameters such as **Model**, **Temperature**, and **Max Completion Length** to use with the prompt.

For the prompt version you are satisfied with, click **Save**. You will be prompted to enter a name for the prompt and an optional description. We can add this version to our code. In a new script called `search_engine.py`, add the following code:

```python
#search_engine.py
from literalai import LiteralClient
from dotenv import load_dotenv
load_dotenv()
client = LiteralClient()
# This will fetch the champion version, you can also pass a specific version
prompt = client.api.get_prompt(name="test_prompt")
prompt = prompt.to_langchain_chat_prompt_template()
prompt.input_variables = ["context", "question"]
```

Literal AI allows you to save different runs of a prompt and has versioning capabilities. You can also see how each version differs from the previous one. By default, the champion version will be fetched. If you want to change a version to be the champion, you can select it in the playground and click **Promote**.

After adding the above code, we will be able to see the generations for a specific prompt in the Literal AI dashboard (more on this later).

## Understanding Chainlit's Co-pilot

[Chainlit](https://github.com/Chainlit/chainlit) is an open-source Python package designed to build production-oriented conversational AI applications. It provides decorators for multiple events (chat start, user message, session resume, session stop, etc.). You can check out my article below for a more comprehensive explanation:
## ä½¿ç”¨ Chainlit å’Œ LangChain æ„å»ºèŠå¤©æœºå™¨äººåº”ç”¨ç¨‹åº

### åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Chainlitï¼ˆä¸€ä¸ªæ¡†æ¶ï¼‰ä¸ºæˆ‘ä»¬çš„è‡ªå®šä¹‰èŠå¤©æœºå™¨äºº Scoopsie å¼€å‘ä¸€ä¸ªåº”ç”¨ç¨‹åºç•Œé¢â€¦

[medium.com](https://medium.com/)

å…·ä½“æ¥è¯´ï¼Œåœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç»ä½¿ç”¨ Chainlit ä¸ºæˆ‘ä»¬çš„ RAG åº”ç”¨ç¨‹åºæ„å»ºä¸€ä¸ª [è½¯ä»¶å‰¯é©¾é©¶](https://docs.chainlit.io/deployment/copilot)ã€‚Chainlit å‰¯é©¾é©¶åœ¨åº”ç”¨ç¨‹åºä¸­æä¾›ä¸Šä¸‹æ–‡æŒ‡å¯¼å’Œè‡ªåŠ¨ç”¨æˆ·æ“ä½œã€‚

## æ„å»ºå‰¯é©¾é©¶

åœ¨åº”ç”¨ç¨‹åºç½‘ç«™ä¸­åµŒå…¥å‰¯é©¾é©¶å‡ºäºå¤šç§åŸå› å¾ˆæœ‰ç”¨ã€‚æˆ‘ä»¬å°†ä¸ºæˆ‘ä»¬çš„è¯­ä¹‰ç ”ç©¶è®ºæ–‡å¼•æ“æ„å»ºä¸€ä¸ªç®€å•çš„ Web ç•Œé¢ï¼Œå¹¶åœ¨å…¶ä¸­é›†æˆä¸€ä¸ªå‰¯é©¾é©¶ã€‚æ­¤å‰¯é©¾é©¶å°†å…·æœ‰å‡ ä¸ªä¸åŒçš„åŠŸèƒ½ï¼Œä½†ä»¥ä¸‹æ˜¯å…¶ä¸­æœ€çªå‡ºçš„åŠŸèƒ½ï¼š

- å®ƒå°†åµŒå…¥åˆ°æˆ‘ä»¬ç½‘ç«™çš„ HTML æ–‡ä»¶ä¸­ã€‚
- å‰¯é©¾é©¶å°†èƒ½å¤Ÿä»£è¡¨ç”¨æˆ·æ‰§è¡Œæ“ä½œã€‚å‡è®¾ç”¨æˆ·è¦æ±‚æä¾›ç‰¹å®šä¸»é¢˜çš„åœ¨çº¿ç ”ç©¶è®ºæ–‡ã€‚è¿™äº›è®ºæ–‡å¯ä»¥åœ¨æ¨¡æ€ä¸­æ˜¾ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥é…ç½®æˆ‘ä»¬çš„å‰¯é©¾é©¶è‡ªåŠ¨æ‰§è¡Œæ­¤æ“ä½œï¼Œè€Œæ— éœ€ç”¨æˆ·è¾“å…¥ã€‚

åœ¨æ¥ä¸‹æ¥çš„å‡ ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨ Chainlit ä¸ºæˆ‘ä»¬çš„è¯­ä¹‰ç ”ç©¶å¼•æ“åˆ›å»ºè½¯ä»¶å‰¯é©¾é©¶ã€‚

## æ­¥éª¤ 1

ç¬¬ä¸€æ­¥æ¶‰åŠä¸ºæˆ‘ä»¬çš„ chainlit åº”ç”¨ç¨‹åºç¼–å†™é€»è¾‘ã€‚æˆ‘ä»¬å°†ä¸ºæˆ‘ä»¬çš„ç”¨ä¾‹ä½¿ç”¨ä¸¤ä¸ª chainlit è£…é¥°å™¨å‡½æ•°ï¼š@cl.on_chat_start å’Œ @cl.on_messageã€‚æˆ‘ä»¬å°†ä»åœ¨çº¿æœç´¢å’Œ RAG ç®¡é“å‘è¿™äº›å‡½æ•°æ·»åŠ é€»è¾‘ã€‚éœ€è¦è®°ä½ä»¥ä¸‹å‡ ç‚¹ï¼š

- @cl.on_chat_start åŒ…å«åœ¨æ–°ç”¨æˆ·ä¼šè¯å¼€å§‹æ—¶éœ€è¦æ‰§è¡Œçš„æ‰€æœ‰ä»£ç ã€‚
- @cl.on_message åŒ…å«åœ¨ç”¨æˆ·å‘é€æ–°æ¶ˆæ¯æ—¶éœ€è¦æ‰§è¡Œçš„æ‰€æœ‰ä»£ç ã€‚

æˆ‘ä»¬å°†ä»æ¥æ”¶ç ”ç©¶ä¸»é¢˜åˆ°åˆ›å»ºæ•°æ®åº“å’Œåœ¨ @cl.on_chat_start è£…é¥°å™¨ä¸­å¯¼å…¥æ–‡æ¡£çš„æ•´ä¸ªè¿‡ç¨‹å°è£…èµ·æ¥ã€‚åœ¨ search_engine.py è„šæœ¬ä¸­ï¼Œå¯¼å…¥æ‰€æœ‰å¿…éœ€çš„æ¨¡å—å’Œåº“ï¼š

```python
# search_engine.py
import chainlit as cl
from langchain_community.document_loaders import ArxivLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv()
```

ç°åœ¨è®©æˆ‘ä»¬æ·»åŠ  @cl.on_chat_start è£…é¥°å™¨çš„ä»£ç ã€‚æˆ‘ä»¬å°†ä½¿æ­¤å‡½æ•°å¼‚æ­¥ï¼Œä»¥ç¡®ä¿å¤šä¸ªä»»åŠ¡å¯ä»¥åŒæ—¶è¿è¡Œã€‚

```python
# search_engine.py
# ç»­
@cl.on_chat_start
async def retrieve_docs():
    # æŸ¥è¯¢éƒ¨åˆ†
    arxiv_query = None
    # ç­‰å¾…ç”¨æˆ·å‘é€ä¸»é¢˜
    while arxiv_query is None:
        arxiv_query = await cl.AskUserMessage(
            content="è¯·è¾“å…¥ä¸€ä¸ªä¸»é¢˜ä»¥å¼€å§‹ï¼", timeout=15).send()
    query = arxiv_query['output']
    # ARXIV æ–‡æ¡£éƒ¨åˆ†
    arxiv_docs = ArxivLoader(query=arxiv_query, load_max_docs=3).load()
    # å‡†å¤‡ arXiv ç»“æœä»¥ä¾›æ˜¾ç¤º
    arxiv_papers = [f"Published: {doc.metadata['Published']} \n "
                    f"Title: {doc.metadata['Title']} \n "
                    f"Authors: {doc.metadata['Authors']} \n "
                    f"Summary: {doc.metadata['Summary'][:50]}... \n---\n"
                    for doc in arxiv_docs]
    await cl.Message(content=f"{arxiv_papers}").send()
    await cl.Message(content=f"Downloading and chunking articles for {query} "
                    f"This operation can take a while!").send()
    # æ•°æ®åº“éƒ¨åˆ†
    pdf_data = []
    for doc in arxiv_docs:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.create_documents([doc.page_content])
        pdf_data.append(texts)
    llm = ChatOpenAI(model='gpt-3.5-turbo',
                    temperature=0)
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-l6-v2")
    db = Chroma.from_documents(pdf_data[0], embeddings)
    # é“¾éƒ¨åˆ†
    chain = RetrievalQA.from_chain_type(llm=llm,
                                        chain_type="stuff",
                                        retriever=db.as_retriever(),
                                        chain_type_kwargs={
                                            "verbose": True,
                                            "prompt": prompt
                                        }
                                        )
    # è®©ç”¨æˆ·çŸ¥é“ç®¡é“å·²å‡†å¤‡å°±ç»ª
    await cl.Message(content=f"Database creation for `{query}` complete. "
                    f"You can now ask questions!").send()
    cl.user_session.set("chain", chain)
    cl.user_session.set("db", db)
```

è®©æˆ‘ä»¬æµè§ˆä¸€ä¸‹æˆ‘ä»¬åœ¨æ­¤å‡½æ•°ä¸­åŒ…è£…çš„ä»£ç ï¼š

**æç¤ºç”¨æˆ·æŸ¥è¯¢ï¼š**æˆ‘ä»¬é¦–å…ˆè®©ç”¨æˆ·å‘é€ç ”ç©¶ä¸»é¢˜ã€‚æ­¤å‡½æ•°åœ¨ç”¨æˆ·æäº¤ä¸»é¢˜ä¹‹å‰ä¸ä¼šç»§ç»­æ‰§è¡Œã€‚

**åœ¨çº¿æœç´¢ï¼š**æˆ‘ä»¬ä½¿ç”¨ LangChain çš„ arXiv æœç´¢åŒ…è£…å™¨æ£€ç´¢ç›¸å…³è®ºæ–‡ï¼Œå¹¶ä»¥å¯è¯»æ ¼å¼æ˜¾ç¤ºæ¯ä¸ªæ¡ç›®ä¸­çš„ç›¸å…³å­—æ®µã€‚

**å¯¼å…¥ï¼š**æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ–‡ç« åˆ†æˆå—å¹¶åˆ›å»ºåµŒå…¥ä»¥è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†ã€‚åˆ†å—ç¡®ä¿é«˜æ•ˆå¤„ç†å¤§å‹è®ºæ–‡ã€‚ä¹‹åï¼Œä»å¤„ç†è¿‡çš„æ–‡æ¡£å—å’ŒåµŒå…¥ä¸­åˆ›å»º Chroma æ•°æ®åº“ã€‚

**æ£€ç´¢ï¼š**æœ€åï¼Œæˆ‘ä»¬è®¾ç½®ä¸€ä¸ª RetrievalQA é“¾ï¼Œå°† LLM å’Œæ–°åˆ›å»ºçš„æ•°æ®åº“é›†æˆä½œä¸ºä¸€ä¸ªæ£€ç´¢å™¨ã€‚æˆ‘ä»¬è¿˜æä¾›äº†æˆ‘ä»¬åœ¨ Literal AI æ¸¸ä¹åœºä¸­åˆ›å»ºçš„æç¤ºã€‚

**å­˜å‚¨å˜é‡ï¼š**æˆ‘ä»¬ä½¿ç”¨ cl.user_session.set åŠŸèƒ½å­˜å‚¨ chain å’Œ db ä¸­çš„å˜é‡ï¼Œä»¥ä¾¿ä»¥åé‡å¤ä½¿ç”¨ã€‚
**ç”¨æˆ·æ¶ˆæ¯ï¼š**æˆ‘ä»¬åœ¨æ•´ä¸ªå‡½æ•°ä¸­ä½¿ç”¨ Chainlit çš„ `cl.Message` åŠŸèƒ½ä¸ç”¨æˆ·äº¤äº’ã€‚

ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„ `@cl.on_message` å‡½æ•°ï¼Œå¹¶æ·»åŠ  RAG ç®¡é“çš„ç”Ÿæˆéƒ¨åˆ†ã€‚ç”¨æˆ·åº”è¯¥èƒ½å¤Ÿä»æ‘„å–çš„è®ºæ–‡ä¸­æé—®ï¼Œåº”ç”¨ç¨‹åºåº”è¯¥æä¾›ç›¸å…³ç­”æ¡ˆã€‚

```python
@cl.on_message
async def retrieve_docs(message: cl.Message):
    question = message.content
    chain = cl.user_session.get("chain")
    db = cl.user_session.get("db")
    # ä¸ºæ¯æ¬¡è°ƒç”¨åˆ›å»ºä¸€ä¸ªæ–°çš„å›è°ƒå¤„ç†ç¨‹åºå®ä¾‹
    cb = client.langchain_callback()
    variables = {"context": db.as_retriever(search_kwargs={"k": 1}),
                 "query": question}
    database_results = await chain.acall(variables,
                                        callbacks=[cb])
    results = [f"Question: {question} "
               f"\n Answer: {database_results['result']}"]
    await cl.Message(results).send()
```

ä»¥ä¸‹æ˜¯ä¸Šè¿°å‡½æ•°ä¸­ä»£ç çš„ç»†åˆ†ï¼š

**é“¾å’Œæ•°æ®åº“æ£€ç´¢ï¼š**æˆ‘ä»¬é¦–å…ˆä»ç”¨æˆ·ä¼šè¯ä¸­æ£€ç´¢å…ˆå‰å­˜å‚¨çš„é“¾å’Œæ•°æ®åº“ã€‚

**LangChain å›è°ƒé›†æˆï¼š**ä¸ºäº†ç¡®ä¿æˆ‘ä»¬å¯ä»¥è·Ÿè¸ªæç¤ºä»¥åŠä½¿ç”¨ç‰¹å®šæç¤ºç‰ˆæœ¬çš„æ‰€æœ‰ç”Ÿæˆï¼Œæˆ‘ä»¬éœ€è¦åœ¨è°ƒç”¨é“¾æ—¶ä» Literal AI æ·»åŠ  LangChain å›è°ƒå¤„ç†ç¨‹åºã€‚æˆ‘ä»¬ä½¿ç”¨ `LiteralClient` å®ä¾‹çš„ `langchain_callback()` æ–¹æ³•åˆ›å»ºå›è°ƒå¤„ç†ç¨‹åºã€‚æ­¤å›è°ƒå°†è‡ªåŠ¨å°†æ‰€æœ‰ LangChain äº¤äº’è®°å½•åˆ° Literal AIã€‚

**ç”Ÿæˆï¼š**æˆ‘ä»¬å®šä¹‰å˜é‡ï¼šæ•°æ®åº“ä½œä¸ºæ£€ç´¢çš„ä¸Šä¸‹æ–‡ï¼Œç”¨æˆ·çš„ç–‘é—®ä½œä¸ºæŸ¥è¯¢ï¼Œè¿˜æŒ‡å®šæ£€ç´¢æœ€é«˜ç»“æœï¼ˆ`k: 1`ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æä¾›çš„å˜é‡å’Œå›è°ƒè°ƒç”¨é“¾ã€‚

## æ­¥éª¤ 2

ç¬¬äºŒæ­¥æ¶‰åŠå°†å‰¯é©¾é©¶åµŒå…¥åˆ°æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºç½‘ç«™ä¸­ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘ç«™è¿›è¡Œæ¼”ç¤ºã€‚åˆ›å»ºä¸€ä¸ª `index.html` æ–‡ä»¶ï¼Œå¹¶å‘å…¶ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š

```html
<!DOCTYPE html>
<html>
<head>
<title>è¯­ä¹‰æœç´¢å¼•æ“</title>
</head>
<body>
<!-- ... -->
<script src="http://localhost:8000/copilot/index.js"></script>
<script>
window.mountChainlitWidget({
    chainlitServer: "http://localhost:8000",
});
</script>
</body>
</html>
```

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æŒ‡å‘æ‰˜ç®¡æˆ‘ä»¬åº”ç”¨ç¨‹åºçš„ Chainlit æœåŠ¡å™¨çš„ä½ç½®ï¼Œå°†å‰¯é©¾é©¶åµŒå…¥åˆ°æˆ‘ä»¬çš„ç½‘ç«™ä¸­ã€‚ `window.mountChainlitWidget` åœ¨æ‚¨ç½‘ç«™çš„å³ä¸‹è§’æ·»åŠ äº†ä¸€ä¸ªæµ®åŠ¨æŒ‰é’®ã€‚å•å‡»å®ƒå°†æ‰“å¼€å‰¯é©¾é©¶ã€‚

ä¸ºäº†ç¡®ä¿æˆ‘ä»¬çš„å‰¯é©¾é©¶æ­£å¸¸å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆè¿è¡Œæˆ‘ä»¬çš„ Chainlit åº”ç”¨ç¨‹åºã€‚å¯¼èˆªåˆ°æ‚¨çš„é¡¹ç›®ç›®å½•å¹¶è¿è¡Œï¼š

```
chainlit run search_engine.py -w
```

ä¸Šé¢çš„ä»£ç åœ¨ `localhost:8000` ä¸Šè¿è¡Œåº”ç”¨ç¨‹åºã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦æ‰˜ç®¡æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºç½‘ç«™ã€‚æ‰“å¼€ `index.html` æ–‡ä»¶ï¼Œæ‚¨ä¼šå‘ç°è¯¥è„šæœ¬ä¸èµ·ä½œç”¨ã€‚ç›¸åï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª HTTPS æµ‹è¯•æœåŠ¡å™¨ã€‚æ‚¨å¯ä»¥é€šè¿‡ä¸åŒçš„æ–¹å¼æ¥å®ç°ï¼Œä½†ä¸€ç§ç›´æ¥çš„æ–¹æ³•æ˜¯ä½¿ç”¨ `npx`ã€‚

`npx` åŒ…å«åœ¨ `npmï¼ˆNode åŒ…ç®¡ç†å™¨ï¼‰` ä¸­ï¼Œå®ƒéš Node.js ä¸€èµ·æä¾›ã€‚è¦è·å– `npx`ï¼Œæ‚¨åªéœ€ [åœ¨æ‚¨çš„ç³»ç»Ÿä¸Šå®‰è£… Node.js](https://nodejs.org/)ã€‚å¯¼èˆªåˆ°æ‚¨çš„ç›®å½•å¹¶è¿è¡Œï¼š

```
npx http-server
```

è¿è¡Œä¸Šé¢çš„å‘½ä»¤å°†åœ¨ `https://localhost:8080` ä¸Šå¯åŠ¨ä¸€ä¸ªæœåŠ¡å™¨ã€‚å¯¼èˆªåˆ°è¯¥åœ°å€ï¼Œæ‚¨å°†èƒ½å¤Ÿçœ‹åˆ°ä¸€ä¸ªå¸¦æœ‰åµŒå…¥å¼å‰¯é©¾é©¶çš„ç®€å• Web ç•Œé¢ã€‚

ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨ `@cl.on_chat_start` åŒ…è£…å‡½æ•°æ¥æ¬¢è¿ç”¨æˆ·ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°† `show_readme_as_default` è®¾ç½®ä¸º `false` åœ¨æˆ‘ä»¬çš„ Chainlit é…ç½®ä¸­ï¼Œä»¥é¿å…é—ªçƒã€‚æ‚¨å¯ä»¥åœ¨é¡¹ç›®ç›®å½•ä¸­çš„ `.chainlit/config.toml` ä¸­æ‰¾åˆ°æ‚¨çš„é…ç½®æ–‡ä»¶ã€‚

## æ­¥éª¤ 3

è¦ä»…åœ¨å‰¯é©¾é©¶ä¸­æ‰§è¡Œä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š

```python
@cl.on_message
async def retrieve_docs(message: cl.Message):
    if cl.context.session.client_type == "copilot":
        # ä»…åœ¨å‰¯é©¾é©¶ä¸­æ‰§è¡Œçš„ä»£ç 
```

æ­¤ä»£ç å—ä¸­çš„ä»»ä½•ä»£ç ä»…åœ¨æ‚¨ä»å‰¯é©¾é©¶ä¸­ä¸æ‚¨çš„åº”ç”¨ç¨‹åºäº¤äº’æ—¶æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨åœ¨æ‰˜ç®¡åœ¨ `https://localhost:8000` çš„ Chainlit åº”ç”¨ç¨‹åºç•Œé¢å¤„è¿è¡ŒæŸ¥è¯¢ï¼Œåˆ™ä¸Šè¿° if ä»£ç å—ä¸­çš„ä»£ç å°†ä¸ä¼šæ‰§è¡Œï¼Œå› ä¸ºå®ƒæœŸæœ›å®¢æˆ·ç«¯ç±»å‹ä¸ºå‰¯é©¾é©¶ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠŸèƒ½ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒæ¥åŒºåˆ†ç›´æ¥åœ¨ Chainlit åº”ç”¨ç¨‹åºä¸­æ‰§è¡Œçš„æ“ä½œå’Œé€šè¿‡å‰¯é©¾é©¶ç•Œé¢å¯åŠ¨çš„æ“ä½œã€‚é€šè¿‡è¿™æ ·åšï¼Œæ‚¨å¯ä»¥æ ¹æ®è¯·æ±‚çš„ä¸Šä¸‹æ–‡å®šåˆ¶åº”ç”¨ç¨‹åºçš„è¡Œä¸ºï¼Œä»è€Œå®ç°æ›´åŠ¨æ€ã€æ›´å“åº”çš„ç”¨æˆ·ä½“éªŒã€‚

## æ­¥éª¤ 4

å‰¯é©¾é©¶å¯ä»¥åœ¨æ‚¨çš„ç½‘ç«™ä¸Šè°ƒç”¨å‡½æ•°ã€‚è¿™å¯¹äºä»£è¡¨ç”¨æˆ·æ‰§è¡Œæ“ä½œï¼ˆä¾‹å¦‚æ‰“å¼€æ¨¡æ€ã€åˆ›å»ºæ–°æ–‡æ¡£ç­‰ï¼‰éå¸¸æœ‰ç”¨ã€‚æˆ‘ä»¬å°†ä¿®æ”¹æˆ‘ä»¬çš„ Chainlit è£…é¥°å™¨å‡½æ•°ï¼Œä»¥åŒ…æ‹¬ä¸¤ä¸ªæ–°çš„å‰¯é©¾é©¶å‡½æ•°ã€‚æˆ‘ä»¬éœ€è¦åœ¨
### Corrected Markdown

**index.html File**

In the `index.html` file, we need to handle how the frontend should respond when a Copilot function in the Chainlit backend application is invoked. The specific response will vary depending on the application. For our semantic research paper engine, we will generate a pop-up notification in the frontend whenever relevant papers or database answers need to be displayed based on the user's query.

We will create two Copilot functions in our application:

- `showArxivResults`: This function will be responsible for displaying online results fetched by the `arxivAPI` based on the user's query.
- `showDatabaseResults`: This function will be responsible for displaying results fetched from our ingested database based on the user's question.

First, let's set up the backend logic in the `search_engine.py` script and modify the `@cl.on_chat_start` function:

```python
@cl.on_chat_start
async def retrieve_docs():
    if cl.context.session.client_type == "copilot":
        # Same code as before
        # Trigger pop-up for arXiv results
        fn_arxiv = cl.CopilotFunction(name="showArxivResults",
                                      args={"results": "\n".join(arxiv_papers)})
        await fn_arxiv.acall()
        # Same code as before
```

In the above code, we define a Copilot function named `showArxivResults` and call it asynchronously. This function is intended to display a formatted list of arXiv papers directly in the Copilot interface. The function signature is straightforward: we specify the name of the function and the arguments it will send back. We will use this information in the `index.html` file to create the pop-up.

Next, we need to modify our `@cl.on_message` function with a second Copilot function that will be executed when the user asks a question based on the ingested papers:

```python
@cl.on_message
async def retrieve_docs(message: cl.Message):
    if cl.context.session.client_type == "copilot":
        # Same code as before
        # Trigger pop-up for database results
        fn_db = cl.CopilotFunction(name="showDatabaseResults",
                                   args={"results": "\n".join(results)})
        await fn_db.acall()
        # Same code as before
```

In the above code, we define a second Copilot function named `showDatabaseResults` to be called asynchronously. This function is responsible for displaying results retrieved from the database in the Copilot interface. The function signature specifies the name of the function and the arguments it will send back.

## Step 5

We will now edit the `index.html` file to include the following changes:

- Add the two Copilot functions.
- Specify what should happen on our website when either of the two Copilot functions is triggered. We will create a pop-up to display the query results from the application's backend.
- Add simple styling for the pop-up.

First, we need to add event listeners for the Copilot functions. In the `<script>` tag of the `index.html` file, add the following code:

```html
<script>
// Previous code
window.addEventListener("chainlit-call-fn", (e) => {
  const { name, args, callback } = e.detail;
  if (name === "showArxivResults") {
    document.getElementById("arxiv-result-text").innerHTML =
      args.results.replace(/\n/g, "<br>");
    document.getElementById("popup").style.display = "flex";
    if (callback) callback();
  } else if (name === "showDatabaseResults") {
    document.getElementById("database-results-text").innerHTML =
      args.results.replace(/\n/g, "<br>");
    document.getElementById("popup").style.display = "flex";
    if (callback) callback();
  }
});
</script>
```

Here's a breakdown of the above code:

- We include functions to show (showPopup()) and hide (hidePopup()) the pop-up modal.
- We register an event listener for the `chainlit-call-fn` event, which is triggered when a Copilot function (either `showArxivResults` or `showDatabaseResults`) is invoked.
- Upon detecting the event, the listener checks the name of the Copilot function that was invoked. Based on the function name, it updates the content in the relevant section of the pop-up using the results provided by the function. It replaces newlines (\n) with HTML line breaks (<br>) for proper formatting of text for HTML display.
- After updating the content, the pop-up modal is displayed (display: "flex"), allowing the user to view the results. The modal can be hidden using the close button, which calls the hidePopup() function.

Next, we need to define the pop-up modal that we specified above. We can do this by adding the following code to the `<body>` tag of the `index.html` script:

```html
<div id="popup" class="popup">
  <span class="close-btn" onclick="hidePopup()">Ã—</span>
  <div class="arxiv-results-wrapper">
    <h1>Arxiv Results</h1>
    <p id="arxiv-result-text">Online results will be displayed here.</p>
  </div>
  <div class="database-results-wrapper">
    <h1>Database Results</h1>
    <p id="database-results-text">Database results will be displayed here.</p>
  </div>
</div>
```

We also add some styling for the pop-up. Edit the `<head>` tag of the `index.html` file:

```html
<style>
* {
  box-sizing: border-box;
}
body {
  font-family: sans-serif;
}
.close-btn {
  position: absolute;
  top: 10px;
  right: 20px;
  font-size: 24px;
  cursor: pointer;
}
.popup {
  display: none;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  width: 50%;
  max-width: 500px;
  padding: 20px;
  background-color: #fff;
  border: 1px solid #ccc;
  border-radius: 5px;
  z-index: 999;
}
.arxiv-results-wrapper,
.database-results-wrapper {
  margin-top: 20px;
}
</style>
```
### å¯åŠ¨åº”ç”¨ç¨‹åº

ç°åœ¨ï¼Œæˆ‘ä»¬å·²å°† Copilot é€»è¾‘æ·»åŠ åˆ° Chainlit åº”ç”¨ç¨‹åºä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åŒæ—¶è¿è¡Œåº”ç”¨ç¨‹åºå’Œç½‘ç«™ã€‚ä¸ºäº†è®© Copilot æ­£å¸¸å·¥ä½œï¼Œæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºå¿…é¡»å·²ç»è¿è¡Œã€‚åœ¨é¡¹ç›®ç›®å½•ä¸­æ‰“å¼€ä¸€ä¸ªç»ˆç«¯ï¼Œå¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥å¯åŠ¨ Chainlit æœåŠ¡å™¨ï¼š

```
chainlit run search.py -h
```

åœ¨æ–°çš„ç»ˆç«¯ä¸­ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç½‘ç«™ï¼š

```
npx http-server
```

# ä½¿ç”¨ Literal AI å®ç° LLM å¯è§‚æµ‹æ€§

å°†å¯è§‚æµ‹æ€§åŠŸèƒ½é›†æˆåˆ°ç”Ÿäº§çº§åº”ç”¨ç¨‹åºä¸­ï¼ˆä¾‹å¦‚æˆ‘ä»¬çš„ Copilot è¿è¡Œçš„è¯­ä¹‰ç ”ç©¶å¼•æ“ï¼‰é€šå¸¸æ˜¯å¿…éœ€çš„ï¼Œä»¥ç¡®ä¿åº”ç”¨ç¨‹åºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å¯é æ€§ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Literal AI æ¡†æ¶æ¥å®ç°æ­¤ç›®çš„ã€‚

å¯¹äºä»»ä½• Chainlit åº”ç”¨ç¨‹åºï¼ŒLiteral AI éƒ½ä¼šè‡ªåŠ¨å¼€å§‹ç›‘è§†åº”ç”¨ç¨‹åºå¹¶å°†æ•°æ®å‘é€åˆ° Literal AI å¹³å°ã€‚æˆ‘ä»¬åœ¨ `search_engine.py` è„šæœ¬ä¸­åˆ›å»ºæç¤ºæ—¶å·²ç»å¯åŠ¨äº† Literal AI å®¢æˆ·ç«¯ã€‚ç°åœ¨ï¼Œæ¯æ¬¡ç”¨æˆ·ä¸æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºäº¤äº’æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šåœ¨ Literal AI ä»ªè¡¨æ¿ä¸­çœ‹åˆ°æ—¥å¿—ã€‚

# ä»ªè¡¨æ¿

å¯¼èˆªåˆ° [Literal AI ä»ªè¡¨æ¿](https://cloud.getliteral.ai/projects/)ï¼Œä»å·¦ä¾§é¢æ¿ä¸­é€‰æ‹©é¡¹ç›®ï¼Œç„¶åå•å‡» **å¯è§‚æµ‹æ€§**ã€‚æ‚¨å°†çœ‹åˆ°ä»¥ä¸‹åŠŸèƒ½çš„æ—¥å¿—ã€‚

## çº¿ç¨‹

çº¿ç¨‹è¡¨ç¤ºåŠ©æ‰‹å’Œç”¨æˆ·ä¹‹é—´çš„ä¼šè¯ã€‚æ‚¨åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°ç”¨æˆ·åœ¨åº”ç”¨ç¨‹åºä¸­è¿›è¡Œçš„æ‰€æœ‰å¯¹è¯ã€‚

å±•å¼€ç‰¹å®šå¯¹è¯å°†æä¾›å…³é”®è¯¦ç»†ä¿¡æ¯ï¼Œä¾‹å¦‚æ¯ä¸ªæ­¥éª¤èŠ±è´¹çš„æ—¶é—´ã€ç”¨æˆ·æ¶ˆæ¯çš„è¯¦ç»†ä¿¡æ¯ä»¥åŠè¯¦ç»†è¯´æ˜æ‰€æœ‰æ­¥éª¤çš„åŸºäºæ ‘çš„è§†å›¾ã€‚æ‚¨è¿˜å¯ä»¥å°†å¯¹è¯æ·»åŠ åˆ°æ•°æ®é›†ã€‚

## è¿è¡Œ

è¿è¡Œæ˜¯ä»£ç†æˆ–é“¾é‡‡å–çš„ä¸€ç³»åˆ—æ­¥éª¤ã€‚è¿™æä¾›äº†æ¯æ¬¡æ‰§è¡Œé“¾æˆ–ä»£ç†æ—¶é‡‡å–çš„æ‰€æœ‰æ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯ã€‚ä½¿ç”¨æ­¤é€‰é¡¹å¡ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—æ¯ä¸ªç”¨æˆ·æŸ¥è¯¢çš„è¾“å…¥å’Œè¾“å‡ºã€‚

æ‚¨å¯ä»¥å±•å¼€è¿è¡Œï¼Œè¿™å°†æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚åŒæ ·ï¼Œæ‚¨å¯ä»¥å°†æ­¤ä¿¡æ¯æ·»åŠ åˆ°æ•°æ®é›†ã€‚

## ç”Ÿæˆ

ç”ŸæˆåŒ…å«å‘é€åˆ° LLM çš„è¾“å…¥åŠå…¶å®Œæˆã€‚è¿™æä¾›äº†å…³é”®è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨äºå®Œæˆçš„æ¨¡å‹ã€ä»¤ç‰Œè®¡æ•°ä»¥åŠè¯·æ±‚å®Œæˆçš„ç”¨æˆ·ï¼ˆå¦‚æœæ‚¨é…ç½®äº†å¤šä¸ªç”¨æˆ·ä¼šè¯ï¼‰ã€‚

# åœ¨ Literal AI ä¸­è¯„ä¼°æç¤º

è‡ªæˆ‘ä»¬æ·»åŠ  LangChain é›†æˆä»¥æ¥ï¼Œæˆ‘ä»¬å¯ä»¥é’ˆå¯¹åº”ç”¨ç¨‹åºä»£ç ä¸­åˆ›å»ºå’Œä½¿ç”¨çš„æ¯ä¸ªæç¤ºè·Ÿè¸ªç”Ÿæˆå’Œçº¿ç¨‹ã€‚å› æ­¤ï¼Œæ¯æ¬¡ä¸ºç”¨æˆ·æŸ¥è¯¢è°ƒç”¨é“¾æ—¶ï¼Œéƒ½ä¼šåœ¨ Literal AI ä»ªè¡¨æ¿ä¸­é’ˆå¯¹å®ƒæ·»åŠ æ—¥å¿—ã€‚è¿™æœ‰åŠ©äºäº†è§£å“ªäº›æç¤ºå¯¼è‡´äº†ç‰¹å®šç”Ÿæˆï¼Œå¹¶æ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„æ€§èƒ½ã€‚

# ç»“è®º

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangChainã€OpenAI å’Œ ChromaDB ä½¿ç”¨ RAG åŠŸèƒ½åˆ›å»ºè¯­ä¹‰ç ”ç©¶è®ºæ–‡å¼•æ“ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜å±•ç¤ºäº†å¦‚ä½•ä¸ºè¯¥å¼•æ“å¼€å‘ Web åº”ç”¨ç¨‹åºï¼Œå¹¶é›†æˆäº† Literal AI çš„ Copilot å’Œå¯è§‚æµ‹æ€§åŠŸèƒ½ã€‚åœ¨ç°å®ä¸–ç•Œçš„è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºä¸­ï¼Œé€šå¸¸éœ€è¦çº³å…¥è¯„ä¼°å’Œå¯è§‚æµ‹æ€§ä»¥ç¡®ä¿æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒCopilot å¯¹äºä¸åŒçš„è½¯ä»¶åº”ç”¨ç¨‹åºæ¥è¯´å¯èƒ½æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„åŠŸèƒ½ï¼Œæœ¬æ•™ç¨‹å¯ä»¥ä½œä¸ºä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œäº†è§£å¦‚ä½•ä¸ºæ‚¨çš„åº”ç”¨ç¨‹åºè®¾ç½®å®ƒã€‚

æ‚¨å¯ä»¥åœ¨æˆ‘çš„ [GitHub](https://github.com/tahreemrasul/semantic_research_engine) ä¸Šæ‰¾åˆ°æœ¬æ•™ç¨‹çš„ä»£ç ã€‚å¦‚æœæ‚¨è§‰å¾—æœ¬æ•™ç¨‹æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘é€šè¿‡ç»™äºˆå®ƒäº”åæ¬¡é¼“æŒæ¥è¡¨ç¤ºæ”¯æŒã€‚æ‚¨å¯ä»¥ [å…³æ³¨](/@tahreemrasul)ï¼Œå› ä¸ºæˆ‘åˆ†äº«äº†äººå·¥æ™ºèƒ½é¢†åŸŸçš„å®é™…æ¼”ç¤ºã€è§£é‡Šå’Œå¾ˆé…·çš„å‰¯é¡¹ç›®ã€‚æ¥ [X](https://twitter.com/tahreemrasul1) æ‰“ä¸ªæ‹›å‘¼ï¼æˆ‘åœ¨é‚£é‡Œåˆ†äº«æŒ‡å—ã€ä»£ç ç‰‡æ®µå’Œå…¶ä»–æœ‰ç”¨çš„å†…å®¹ã€‚ğŸ‘‹