<!--
title: 使用MCP进行AI集成的6大理由
cover: https://cdn.thenewstack.io/media/2025/04/339aeb36-connect.jpg
summary: 还在手动集成 LLM？快用 MCP！它像 AI 的 USB-C，标准化 AI 访问 API 和内部系统的方式。通过 MCP，LLM 能实时连接你的堆栈，灵活切换 AI 供应商，并无缝融入现有治理模型，为 AI 原生开发体验赋能。Konnect 已支持 MCP！
-->

还在手动集成 LLM？快用 MCP！它像 AI 的 USB-C，标准化 AI 访问 API 和内部系统的方式。通过 MCP，LLM 能实时连接你的堆栈，灵活切换 AI 供应商，并无缝融入现有治理模型，为 AI 原生开发体验赋能。Konnect 已支持 MCP！

> 译自：[6 Reasons You’ll Want To Use MCP for AI Integration](https://thenewstack.io/six-reasons-youll-want-to-use-mcp-for-ai-integration/)
> 
> 作者：Ross Kukulinski

在过去的一年中，大型语言模型 (LLM) 已经从研究新事物转变为业务关键型工具。但是，将这些模型集成到生产工作流程中，尤其是在将它们与 API 和内部系统一起使用时，仍然非常困难。大多数开发人员仍然在手动连接脆弱的函数调用，将粘合代码拼接在一起或依赖实验性插件。缺少的是一种一致的、标准化的方式来将 LLM 连接到驱动现代应用程序的工具、[数据和基础设施](https://thenewstack.io/the-architects-guide-to-the-modern-data-stack/)。

这就是 [模型上下文协议 (MCP)](https://thenewstack.io/model-context-protocol-bridges-llms-to-the-apps-they-need/) 的用武之地。

MCP 是一种轻量级的开放协议，旨在标准化应用程序向 AI 代理和 LLM 提供上下文的方式。可以将其视为 AI 的 USB-C：一种将智能代理插入其需要理解和交互的服务和系统的通用方法。无论您是连接到本地数据源还是远程 API，MCP 都能以一种干净、安全和灵活的方式让 AI 与现实世界集成，而无需每次都编写定制的连接器。

![模型上下文协议通用架构图](https://cdn.thenewstack.io/media/2025/04/16c72e2d-mcp-architecture.png)

*MCP 通用架构。（来源：Kong，基于 [模型上下文协议图](https://modelcontextprotocol.io/introduction)。）*

让我们探讨一下为什么 [MCP 正在迅速成为](https://thenewstack.io/mcp-the-missing-link-between-ai-agents-and-apis/) 在 AI、API 和基础设施交叉领域工作的团队的必备工具的六个原因。

## 1. 如果 LLM 无法看到您的堆栈，它们就毫无用处

LLM 在推理、总结和生成方面非常强大，但它们无法对它们不知道存在的系统采取行动。除非有人明确地连接这些点，通常是通过脆弱的一次性代码或过时的 API 定义，否则大多数企业基础设施对 AI 代理仍然是不可见的。

MCP 引入了一种标准的客户端-服务器模型，其中 AI 代理（或其开发人员环境）查询称为 MCP 服务器的结构化接口。这些服务器通过可发现的、经过授权的接口公开 API、服务甚至控制平面数据。这意味着您的代理最终可以看到您的内部工具、API 和服务，并在上下文中对它们进行推理。

## 2. 它为非结构化生态系统带来结构

当 AI 代理可以以编程方式与服务交互时，它们的工作效果最佳，但通常，这些服务没有文档记录、不一致或缺乏智能消费所需的元数据。虽然像 OpenAPI 这样的标准为描述 API 提供了坚实的基础，但它们通常是静态的，并且没有提供 AI 代理开箱即用所需的完整语义或运行时上下文。

MCP 通过定义服务应如何向 AI 描述自己来构建此基础 - 不仅具有结构，还具有意图。它使开发人员能够以模型可以理解、查询和操作的方式指定 API 操作的目的、参数和结果。

这是从文档到交互的关键飞跃。

## 3. 它支持实时代理工作流程

在 MCP 之前，大多数 LLM 代理集成都需要轮询、脚本或中间 API 才能触发简单的操作。这对于一次性任务来说可能还可以，但是当您需要可扩展的多代理工作流程时，尤其是在实时环境中，它会崩溃。

使用 MCP，代理维护与其依赖的服务的活动客户端-服务器连接。这意味着 IDE 或终端中的 AI 助手可以立即查询遥测数据、查找策略配置或调用微服务，而无需重新编译或手动重新配置集成。

这种实时连接为真正自主的代理打开了大门，这些代理可以监视、响应和适应实时基础设施。

## 4. MCP 为您提供灵活性，而无需锁定

MCP 最容易被忽视的优点之一是它在设计上与 AI 供应商无关。因此，它不会将您锁定到特定的 LLM 提供商、AI 工具链或云供应商。实际上，MCP 正在被多个 LLM 客户端采用，并且与跨生态系统的工具兼容，包括 IDE、脚本环境和像 AutoGen 这样的代理框架。

由于其模块化架构，您可以混合和匹配 AI 系统，并随着时间的推移更换供应商，而不会破坏您的集成。这在当今快速变化的 AI 格局中尤其有价值，在这种格局中，互操作性正变得比单独的性能更重要。

## 5. 它使 AI 成为您治理模型的一部分

将基础设施暴露给 LLM 提出了一个关键问题：如何保护它？
借助 MCP，数据访问不再是插入工具的副作用，而是协议的一部分。每个 MCP 服务器都可以精确定义其公开的内容以及使用的凭据。这意味着 AI 代理继承了您已在系统中使用的相同权限模型。

因此，您可以像对待人类用户一样严格地授予 AI 访问权限，甚至可以将其限制于特定服务、团队或部署环境。这使得构建安全、可审计的工作流程成为可能，从而避免 AI 成为合规性负担。

## 6. 它让您为 AI 原生开发者体验做好准备

软件开发的未来是 AI 原生的。IDE 正在变成副驾驶。终端正在被自然语言取代。开发人员已经在要求 LLM 搭建代码、调试日志和启动服务。

MCP 为这些环境提供了它们所缺少的环节：对它们应该管理的 инфраструктура 进行实时、结构化的访问。无论是发现 API、检查使用模式还是触发部署，MCP 都使 инфраструктура 可供 AI 访问，并因此可供使用它的人类访问。

## 最后的想法

[MCP 不仅仅是堆栈中的另一层](https://thenewstack.io/building-your-first-model-context-protocol-server/)，它是 AI 和现代软件系统之间的新型连接组织。通过提供一致、开放和安全的方式将服务公开给智能代理，MCP 有助于释放 LLM 在生产中的全部潜力。
虽然团队可以构建自己的 MCP 服务器来公开 API、可观测性数据或策略配置，但一些平台已经提供了开箱即用的 MCP 支持，这使得入门和利用比以往任何时候都更容易。

Kong 已在我们的 API 生命周期管理平台 [Konnect 中添加了对 MCP 的支持](https://konghq.com/blog/product-releases/mcp-server)，使组织能够将其整个 API 系统记录公开给 AI 代理，从服务发现到流量分析和策略配置。

AI 原生时代即将到来。而 MCP 可能正是保持一切和每个人连接的标准。