向任何 GenAI 提问，你都可能收到不准确的回复或幻觉。AI 幻觉会以许多代价高昂的方式给企业带来重大风险。根据 Vectara 最近的一项[研究](https://github.com/vectara/hallucination-leaderboard)，AI 幻觉发生的几率在 0.7% 到 29.9% 之间，具体取决于所使用的大型语言模型。

幻觉的影响可能会扰乱运营，削弱效率和信任，并导致代价高昂的挫折。如果公众发现企业依赖虚假信息，例如银行聊天机器人提供不正确的贷款条款、AI 机器人顾问推荐不适当的投资或呼叫中心散布不实信息，客户和利益相关者的信任会迅速瓦解。

## AI 幻觉的实际风险

最近的一个例子是，当我们与一家领先的机电设备制造商合作时。他们的客户支持团队在一个高流失率的环境中运作，经验水平各异的代表每天处理大量关于设备故障排除的电话。主要问题出现在 AI 助手表现出“过度热情”的行为时，它为知识库中未包含的产品提供了故障排除建议。这导致 AI 从类似产品中推断解决方案或生成幻觉响应。鉴于制造商设备的细微差别，这尤其成问题，因为看似相同的产品可能具有显着的功能差异。

为了解决幻觉问题，我们实施了一种侧重于精确性和验证的综合方法。该团队在 AI 代理中开发了额外的检查和防护措施，以确保在知识库搜索期间更准确地分析产品变体。这些保障措施旨在保证用于故障排除的知识库段与所讨论的特定设备高度精确地匹配。此外，该解决方案在故障排除工作流程中加入了额外的验证步骤，在 AI 通过用户界面向呼叫中心代表提出建议之前创建了多个检查点。

[增强的防护措施和验证流程的实施成功地](https://thenewstack.io/zero-trust-adoption-4-steps-to-implementation-success-2/)解决了幻觉问题，同时在客户支持运营方面实现了可衡量的改进。改进后的 AI 助手现在提供更准确、针对特定设备的故障排除指导，从而显着降低了误诊率。额外的精度控制确保建议基于精确的产品匹配，而不是可能产生误导的推断。因此，客户体验到了更快的解决问题时间、更高的支持质量以及对 AI 辅助故障排除过程的更高信心，最终为代表和客户创造了更有效的[支持环境](https://thenewstack.io/elyras-jupyter-ai-pipelines-now-support-custom-components/)。

## 如何使用 Agentic AI 来防止 AI 幻觉

AI 代理正在成为缓解 AI 幻觉风险的常用方法。它们能够感知环境，推理可能的行动，采取行动以实现目标，并从反馈中学习以不断提高其性能。

正如我们在呼叫中心示例中看到的那样，[Agentic AI 可以评估](https://thenewstack.io/ai-agentic-evaluation-tools-help-devs-fight-hallucinations/)响应是否与已知事实或逻辑推理相符，然后再最终确定答案，这是一种由 AI 代理执行的自我评估技术。这种迭代推理有助于在内部识别和纠正错误。Agentic AI 还从经过验证的外部资源中检索实时信息，而不是依赖于预先训练的数据。这确保了响应的及时性和准确性。更进一步，AI 代理可以交叉引用多个来源或跨不同数据集运行查询，以确认其答案的一致性和可靠性，从而降低信息被捏造的可能性。

[Agentic AI 模型还可以采用思维链提示](https://thenewstack.io/how-to-define-an-ai-agent-persona-by-tweaking-llm-prompts/)，这会将复杂的问题分解为逻辑步骤或思维链推理；这种技术在推理密集型任务中尤其有效。

最后，利用[检索增强生成](https://fpt.ai/blogs/retrieval-augmented-generation/) (RAG) 解决方案已被证明是有帮助的。Agentic [RAG 将检索增强生成的优势](https://thenewstack.io/advanced-retrieval-augmented-generation-rag-techniques/)与自主代理推理相结合，使 AI 能够管理复杂的任务，个性化响应，并优先考虑相关信息——所有这些都基于可信的知识来源。

## 数据质量与数据数量的神话

虽然 AI 需要大量数据，但认为更多数据会导致幻觉是一种错误的假设。数据量与 AI 幻觉之间的关系是微妙的，很大程度上取决于数据的质量和类型。虽然企业通常认为“更多数据 = 更好的 AI”，但关键因素是验证（消除不准确性）、来源控制（优先考虑人工生成的数据）和多样性（避免领域过度代表性）。当满足这些条件时，扩展数据会提高可靠性；如果没有这些条件，幻觉会恶化。

## 预防 AI 幻觉的关键策略

虽然幻觉无法完全消除，但可以采取一些策略来大大减轻其发生和影响。

毫不奇怪，确保 AI [模型在多样化、准确和全面的数据集上进行训练](https://thenewstack.io/nvidia-shaves-up-to-30-off-large-language-model-training-times/)可以最大限度地降低幻觉的风险。定期更新和清理训练数据使模型能够适应新信息并避免产生过时或不正确的输出。制定清晰、具体和详细的提示可以引导 AI 给出更准确的响应并减少歧义，否则可能导致不准确或误导性的结果。

整合人工监督，尤其是在高风险或受监管的环境中，使专家能够在 AI 生成的输出被使用或发布之前对其进行验证和纠正，从而作为防止错误和不准确的最后一道防线。明确定义 AI 系统的预期用途和边界，确保它仅应用于为其设计和训练的任务，从而最大限度地降低在面对不熟悉的情况下出现幻觉的风险。