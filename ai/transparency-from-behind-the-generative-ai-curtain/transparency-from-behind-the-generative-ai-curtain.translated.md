# 生成式 AI 幕后的透明度
![生成式 AI 幕后的透明度特色图片](https://cdn.thenewstack.io/media/2024/07/4bd1b47b-curtain-1024x576.jpg)

AI 基础模型已经成为超级模型：在这个 ChatGPT 后的世界，像 [LLaMA](https://llama.meta.com/), [Gemini](https://gemini.google.com/) 和 [Claude](https://claude.ai/) 这样的名字，现在对科技评论家来说，拥有与人类名人一样的轰动效应和即时认可度。

然而，随着 AI 在全球重要性日益提升，我们也越来越需要了解产生我们从那些提示框中输入的结果的技术。[斯坦福大学、麻省理工学院和普林斯顿大学的研究人员最近的两项研究](https://crfm.stanford.edu/fmti/May-2024/index.html) 深入研究了支撑世界上一些最先进且广受欢迎的生成式 AI 工具的 AI 基础模型，这些工具每天被数百万（甚至数十亿）人使用。透明度的需求至关重要。

然而，[2024 年基础模型透明度指数](https://crfm.stanford.edu/fmti/May-2024/index.html) 的结果表明，随着时间的推移，随着公司在开发其旗舰基础模型上投入越来越多的资金，他们对用于训练这些模型的大量数据以及这些数据的来源变得越来越不透明。

## 什么是基础模型？
“基础模型”一词是用来描述支撑生成式 AI 的大型深度学习神经网络的统称。[基础模型](https://thenewstack.io/vision-foundation-models-when-does-size-matter/) 在大量数据上进行训练，以执行各种各样的任务，从生成文本、图像和编程代码到流畅地用自然语言响应书面提示和问题。不过，可以说，它们最大的力量在于支撑新的 AI 应用：与从头开始构建自己的模型相比，基础模型允许工程团队更快、更经济地开发新的生成式 AI 应用。

## 为什么 AI 透明度很重要
由于相对较少的几个基础模型支撑着如此多的面向人类的生成式 AI 工具，因此透明度的需求至关重要。例如，当我们使用 [ChatGPT](https://openai.com/chatgpt/) 生成文本，使用 [Stable Diffusion](https://stability.ai/) 创建图像，以及使用 [Tabnine](https://www.tabnine.com/) 生成代码时，我们需要了解它们的基础 ML 模型是如何开发和部署的。用户希望知道，我们可以信任我们已经快速熟悉并喜爱，甚至严重依赖的 AI 工具。我们需要知道它们是公平、可解释和安全的。

这种在 AI 的狂野西部时期建立信任和透明度的紧迫感，源于我们在社交媒体早期不受监管的日子里吸取的教训。现在我们已经了解得更多，我们可以防止类似的危机，因为我们开始采用 AI。但是，我们如何衡量仍在发展中的技术中的实际功能透明度？

## 衡量 AI 透明度
2023 年 10 月，来自斯坦福大学、麻省理工学院和普林斯顿大学的研究人员合作，通过评估当时排名前 10 的基础模型的旗舰模型，树立了一面重要的 AI 透明度旗帜。由此产生的白皮书，名为“[基础模型透明度指数](https://crfm.stanford.edu/fmti/May-2024/index.html)”（FMTI），展示了当时排名前 10 的主要基础模型的透明度指数。Meta 的模型得分最高，满足了 100 个透明度协议因素中的 54 个，而亚马逊得分最低（100 个中的 12 个）。所有模型提供商的平均得分 37 分——并非及格分数——揭示了一个行业正在努力[向公众开放](https://thenewstack.io/transparency-and-community-an-open-source-vision-for-ai/)。

[同一批研究人员在 2024 年 5 月发布了一份后续报告，其中有一些变化](https://crfm.stanford.edu/fmti/May-2024/company-reports/index.html)。最初的报告依赖于公开数据；对于为期六个月的后续研究，模型开发人员自己提交了透明度报告，披露了他们针对 FMTI 的 100 个指标中的每一个指标的实践。这次，有 14 个组织提交了透明度报告。参与的开发人员还超出了研究人员定义的最初 100 个透明度协议因素；总体而言，组织在各自的报告中平均提供了 17 个新的透明度信息指标。
## 透明度得分上升
FMTI 2024 在 2023 年的变化方面揭示了什么，这告诉我们关于透明度新现状的什么？
这次，来自 BigCode/Hugging Face/ServiceNow 的 StarCoder 基础模型的透明度得分达到 100 分中的 85 分，创历史新高。所有 14 个模型开发者的平均得分攀升至 100 分中的 58 分，比 2023 年 10 月的 FMTI 平均得分提高了 21 分。

总体而言，2024 年的 FMTI 文件显示出明显的改进：最高透明度得分提高了 31 分，最低得分提高了 21 分。在第一份报告和后续报告中都出现的八位开发者都提高了他们的得分；亚马逊的整体涨幅最大，从 2023 年的 12 分跃升至 2024 年的 41 分。更令人欣慰的是，一位开发者满足了研究人员制定的 100 个透明度指标中的 96 个，多位开发者成功满足了 89 个指标。

这些趋势总体上相当积极，但数据也揭示了一些不那么乐观的成果。虽然透明度的整体现状有了显著改善，但一些领域仍然顽固地难以触及。根据报告，“有关数据（版权、许可和 PII）、公司护栏的有效性（缓解评估）以及基础模型的下游影响（人们如何使用模型以及有多少人在特定地区使用模型）的信息仍然非常不透明。”

换句话说，在一些关键领域，模型开发者仍在掩盖他们的做法，尤其是在数据来源、隐私和缓解方面。

## 我们作为社区下一步该怎么做？
可能最重要的收获是：围绕数据访问的透明度从 2023 年 10 月的 20% 下降到 2024 年 5 月的 7%。在新报告中，研究人员将其归因于“公司在披露用于构建基础模型的数据方面面临的重大法律风险”。特别是，“如果数据包含受版权保护的、私人的或非法的內容，这些公司可能会面临责任。”不幸的是，2024 年参与的模型开发者在“模型缓解”方面的得分也很低，这意味着他们没有充分披露其解决版权侵犯或隐私侵犯问题的策略。

科技公司必须优先考虑人工智能的透明度，因为在该行业之外，大多数人根本不了解人工智能是什么或它是如何运作的。公平地说，就像这些技术是新的，围绕透明度的标准和期望也是新的。

对于技术来说，这些都是未知领域，在某种程度上，我们是在行走中开辟道路。这些 FMTI 报告和协议的最大价值在于为模型开发者提供明确且可操作的步骤，以提高模型透明度。

2023 年和 2024 年 FMTI 报告之间的信息差异揭示了基础模型提供商内部持续存在的——也是系统性的——不透明领域。在人工智能方面，基础模型开发者的组织本身需要赢得最终用户、政府实体以及——不夸张地说——全人类的信任和信心。

作为社区，我们的工作是推动可访问性。无论我们这些软件开发人员如何——甚至是否——使用人工智能和机器学习技术，我们都有责任确保这些新兴技术基于道德标准运作，并共同努力减少任何潜在的危害。

作为人工智能的消费者，企业可以通过在采用特定人工智能工具之前提出明智的问题来推动这种可访问性。这份报告揭示了为什么必须审查实际的许可协议和服务条款，不仅是提供人工智能代码助手工具的供应商，还有工具背后使用的模型的许可协议和服务条款。

我们的结论是：在透明度成为基础模型开发者运营的默认方式之前，保证与任何人工智能平台交互所用数据的绝对隐私的唯一方法是，这些数据无法离开您的边界。

[YOUTUBE.COM/THENEWSTACK 科技发展迅速，不要错过任何一集。订阅我们的 YouTube 频道，观看我们所有的播客、访谈、演示等。](https://youtube.com/thenewstack?sub_confirmation=1)