# Tiny Agents in Pythonï¼šä¸€ä¸ªç”± MCP é©±åŠ¨çš„ Agentï¼Œä»£ç é‡çº¦ä¸º 70 è¡Œ

[åœ¨ GitHub ä¸Šæ›´æ–°](https://github.com/huggingface/blog/blob/main/python-tiny-agents.md)

çµæ„Ÿæ¥è‡ª [JS ä¸­çš„ Tiny Agents](https://huggingface.co/blog/tiny-agents)ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªæƒ³æ³•ç§»æ¤åˆ°äº† Python ğŸï¼Œå¹¶æ‰©å±•äº† `huggingface_hub` [å®¢æˆ·ç«¯ SDKï¼Œä½¿å…¶å¯ä»¥å……å½“ MCP å®¢æˆ·ç«¯ï¼Œä»è€Œå¯ä»¥ä» MCP æœåŠ¡å™¨æ‹‰å–å·¥å…·ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´å°†å…¶ä¼ é€’ç»™ LLMã€‚](https://github.com/huggingface/huggingface_hub/)

MCP ([æ¨¡å‹ä¸Šä¸‹æ–‡åè®®](https://modelcontextprotocol.io/)) æ˜¯ä¸€ä¸ªå¼€æ”¾åè®®ï¼Œç”¨äºæ ‡å‡†åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ä¸å¤–éƒ¨å·¥å…·å’Œ API çš„äº¤äº’æ–¹å¼ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œå®ƒæ¶ˆé™¤äº†ä¸ºæ¯ä¸ªå·¥å…·ç¼–å†™è‡ªå®šä¹‰é›†æˆçš„éœ€è¦ï¼Œä»è€Œå¯ä»¥æ›´è½»æ¾åœ°å°†æ–°åŠŸèƒ½æ’å…¥åˆ° LLM ä¸­ã€‚

åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•å¼€å§‹ä½¿ç”¨ Python ä¸­çš„ Tiny Agentï¼Œè¯¥ Agent è¿æ¥åˆ° MCP æœåŠ¡å™¨ä»¥è§£é”å¼ºå¤§çš„å·¥å…·åŠŸèƒ½ã€‚æ‚¨å°†çœ‹åˆ°å¯åŠ¨è‡ªå·±çš„ Agent å¹¶å¼€å§‹æ„å»ºæ˜¯å¤šä¹ˆå®¹æ˜“ï¼

å‰§é€ï¼šAgent æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ„å»ºåœ¨ MCP å®¢æˆ·ç«¯ä¹‹ä¸Šçš„ while å¾ªç¯ï¼

## å¦‚ä½•è¿è¡Œæ¼”ç¤º

æœ¬èŠ‚å°†å¼•å¯¼æ‚¨äº†è§£å¦‚ä½•ä½¿ç”¨ç°æœ‰çš„ Tiny Agentsã€‚æˆ‘ä»¬å°†ä»‹ç»è®¾ç½®å’Œè¿è¡Œ Agent çš„å‘½ä»¤ã€‚

é¦–å…ˆï¼Œæ‚¨éœ€è¦å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ `huggingface_hub`ï¼Œå¹¶ä½¿ç”¨ `mcp` extra æ¥è·å–æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ã€‚

```
pip install "huggingface_hub[mcp]>=0.32.0"
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ CLI è¿è¡Œä¸€ä¸ª Agentï¼

æœ€é…·çš„éƒ¨åˆ†æ˜¯ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä» Hugging Face Hub [tiny-agents](https://huggingface.co/datasets/tiny-agents/tiny-agents) æ•°æ®é›†ä¸­åŠ è½½ Agentï¼Œæˆ–è€…æŒ‡å®šæ‚¨è‡ªå·±çš„æœ¬åœ° Agent é…ç½®çš„è·¯å¾„ï¼

```
> tiny-agents run --help
Usage: tiny-agents run [OPTIONS] [PATH] COMMAND [ARGS]...
Run the Agent in the CLI
â•­â”€ Arguments â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ path [PATH] Path to a local folder containing an agent.json file or a built-in agent stored in the 'tiny-agents/tiny-agents' Hugging Face dataset â”‚
â”‚ (https://huggingface.co/datasets/tiny-agents/tiny-agents) â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€ Options â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ --help Show this message and exit. â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

å¦‚æœæ‚¨æ²¡æœ‰æä¾›ç‰¹å®š Agent é…ç½®çš„è·¯å¾„ï¼Œæˆ‘ä»¬çš„ Tiny Agent é»˜è®¤ä¼šè¿æ¥åˆ°ä»¥ä¸‹ä¸¤ä¸ª MCP æœåŠ¡å™¨ï¼š

- â€œcanonicalâ€[æ–‡ä»¶ç³»ç»ŸæœåŠ¡å™¨](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem)ï¼Œå®ƒå¯ä»¥è®¿é—®æ‚¨çš„æ¡Œé¢ï¼Œä»¥åŠ
- [Playwright MCP](https://github.com/microsoft/playwright-mcp) æœåŠ¡å™¨ï¼Œå®ƒçŸ¥é“å¦‚ä½•ä¸ºæ‚¨ä½¿ç”¨æ²™ç›’ Chromium æµè§ˆå™¨ã€‚

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†ä¸€ä¸ªé…ç½®ä¸ºé€šè¿‡ Nebius æ¨ç†æä¾›ç¨‹åºä½¿ç”¨ [Qwen/Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct) æ¨¡å‹çš„ Web æµè§ˆ Agentï¼Œå®ƒé…å¤‡äº†ä¸€ä¸ª playwright MCP æœåŠ¡å™¨ï¼Œè¿™ä½¿å…¶å¯ä»¥ä½¿ç”¨ Web æµè§ˆå™¨ï¼Agent é…ç½®æ˜¯é€šè¿‡æŒ‡å®š [å…¶åœ¨ tiny-agents/tiny-agents](https://huggingface.co/datasets/tiny-agents/tiny-agents/tree/main/celinah/web-browser) Hugging Face æ•°æ®é›†ä¸­çš„è·¯å¾„æ¥åŠ è½½çš„ã€‚

å½“æ‚¨è¿è¡Œ Agent æ—¶ï¼Œæ‚¨ä¼šçœ‹åˆ°å®ƒåŠ è½½ï¼Œå¹¶åˆ—å‡ºå®ƒä»å…¶è¿æ¥çš„ MCP æœåŠ¡å™¨ä¸­å‘ç°çš„å·¥å…·ã€‚ç„¶åï¼Œå®ƒå°±å¯ä»¥æ¥å—æ‚¨çš„æç¤ºäº†ï¼

æ­¤æ¼”ç¤ºä¸­ä½¿ç”¨çš„æç¤ºï¼š

```
do a Web Search for HF inference providers on Brave Search and open the first result and then give me the list of the inference providers supported on Hugging Face
```

æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ Gradio Spaces ä½œä¸º MCP æœåŠ¡å™¨ï¼ä»¥ä¸‹ç¤ºä¾‹é€šè¿‡ Nebius æ¨ç†æä¾›ç¨‹åºä½¿ç”¨ [Qwen/Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct) æ¨¡å‹ï¼Œå¹¶è¿æ¥åˆ° `FLUX.1 [schnell]` å›¾åƒç”Ÿæˆ HF Space ä½œä¸º MCP æœåŠ¡å™¨ã€‚è¯¥ Agent æ˜¯ä» Hugging Face Hub ä¸Šçš„ [tiny-agents/tiny-agents](https://huggingface.co/datasets/tiny-agents/tiny-agents/tree/main/julien-c/flux-schnell-generator) æ•°æ®é›†ä¸­çš„é…ç½®åŠ è½½çš„ã€‚

æ­¤æ¼”ç¤ºä¸­ä½¿ç”¨çš„æç¤ºï¼š

```
Generate a 1024x1024 image of a tiny astronaut hatching from an egg on the surface of the moon.
```

ç°åœ¨æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•è¿è¡Œç°æœ‰çš„ Tiny Agentsï¼Œä»¥ä¸‹å„èŠ‚å°†æ›´æ·±å…¥åœ°æ¢è®¨å®ƒä»¬çš„å·¥ä½œåŸç†ä»¥åŠå¦‚ä½•æ„å»ºè‡ªå·±çš„ Agentã€‚

### Agent é…ç½®
æ¯ä¸ªä»£ç†çš„è¡Œä¸ºï¼ˆå…¶é»˜è®¤æ¨¡å‹ã€æ¨ç†æä¾›è€…ã€è¦è¿æ¥çš„ MCP æœåŠ¡å™¨ä»¥åŠå…¶åˆå§‹ç³»ç»Ÿæç¤ºï¼‰ç”±ä¸€ä¸ª `agent.json` æ–‡ä»¶å®šä¹‰ã€‚æ‚¨è¿˜å¯ä»¥åœ¨åŒä¸€ç›®å½•ä¸­æä¾›è‡ªå®šä¹‰çš„ `PROMPT.md`ï¼Œä»¥è·å¾—æ›´è¯¦ç»†çš„ç³»ç»Ÿæç¤ºã€‚è¿™æ˜¯ä¸€ä¸ªä¾‹å­ï¼š

`agent.json`
`model` å’Œ `provider` å­—æ®µæŒ‡å®šäº†ä»£ç†ä½¿ç”¨çš„ LLM å’Œæ¨ç†æä¾›è€…ã€‚`servers` æ•°ç»„å®šä¹‰äº†ä»£ç†å°†è¿æ¥çš„ MCP æœåŠ¡å™¨ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œé…ç½®äº†ä¸€ä¸ªâ€œstdioâ€MCP æœåŠ¡å™¨ã€‚è¿™ç§ç±»å‹çš„æœåŠ¡å™¨ä½œä¸ºæœ¬åœ°è¿›ç¨‹è¿è¡Œã€‚ä»£ç†ä½¿ç”¨æŒ‡å®šçš„ `command` å’Œ `args` å¯åŠ¨å®ƒï¼Œç„¶åé€šè¿‡ stdin/stdout ä¸å…¶é€šä¿¡ï¼Œä»¥å‘ç°å’Œæ‰§è¡Œå¯ç”¨çš„å·¥å…·ã€‚

```json
{
"model": "Qwen/Qwen2.5-72B-Instruct",
"provider": "nebius",
"servers": [
{
"type": "stdio",
"config": {
"command": "npx",
"args": ["@playwright/mcp@latest"]
}
}
]
}
```

`PROMPT.md`

```
You are an agent - please keep going until the userâ€™s query is completely resolved [...]
```

æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co)æ‰¾åˆ°å…³äº Hugging Face æ¨ç†æä¾›è€…çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚

[
](#llms-can-use-tools)

## LLMs å¯ä»¥ä½¿ç”¨å·¥å…·

ç°ä»£ LLM æ˜¯ä¸ºå‡½æ•°è°ƒç”¨ï¼ˆæˆ–å·¥å…·ä½¿ç”¨ï¼‰è€Œæ„å»ºçš„ï¼Œè¿™ä½¿ç”¨æˆ·èƒ½å¤Ÿè½»æ¾æ„å»ºé’ˆå¯¹ç‰¹å®šç”¨ä¾‹å’Œå®é™…ä»»åŠ¡é‡èº«å®šåˆ¶çš„åº”ç”¨ç¨‹åºã€‚

å‡½æ•°ç”±å…¶æ¨¡å¼å®šä¹‰ï¼Œè¯¥æ¨¡å¼å‘ŠçŸ¥ LLM å®ƒåšä»€ä¹ˆä»¥åŠå®ƒæœŸæœ›ä»€ä¹ˆè¾“å…¥å‚æ•°ã€‚LLM å†³å®šä½•æ—¶ä½¿ç”¨å·¥å…·ï¼Œç„¶åä»£ç†åè°ƒè¿è¡Œè¯¥å·¥å…·å¹¶å°†ç»“æœåé¦ˆå›æ¥ã€‚

```json
tools = [
{
"type": "function",
"function": {
"name": "get_weather",
"description": "Get current temperature for a given location.",
"parameters": {
"type": "object",
"properties": {
"location": {
"type": "string",
"description": "City and country e.g. Paris, France"
}
},
"required": ["location"],
},
}
}
]
```

`InferenceClient` å®ç°äº†ä¸ [OpenAI Chat Completions API](https://platform.openai.com/docs/guides/function-calling?api-mode=chat) ç›¸åŒçš„å·¥å…·è°ƒç”¨æ¥å£ï¼Œè¿™æ˜¯æ¨ç†æä¾›è€…å’Œç¤¾åŒºçš„æ—¢å®šæ ‡å‡†ã€‚

[
](#building-our-python-mcp-client)

## æ„å»ºæˆ‘ä»¬çš„ Python MCP å®¢æˆ·ç«¯

`MCPClient` æ˜¯æˆ‘ä»¬å·¥å…·ä½¿ç”¨åŠŸèƒ½çš„æ ¸å¿ƒã€‚å®ƒç°åœ¨æ˜¯ `huggingface_hub` çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä½¿ç”¨ `AsyncInferenceClient` ä¸ LLM é€šä¿¡ã€‚

å®Œæ•´çš„ `MCPClient` ä»£ç åœ¨[è¿™é‡Œ](https://huggingface.co)ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨å®é™…ä»£ç æ¥å­¦ä¹  ğŸ¤“

`MCPClient` çš„ä¸»è¦èŒè´£ï¼š

- ç®¡ç†ä¸ä¸€ä¸ªæˆ–å¤šä¸ª MCP æœåŠ¡å™¨çš„å¼‚æ­¥è¿æ¥ã€‚
- ä»è¿™äº›æœåŠ¡å™¨å‘ç°å·¥å…·ã€‚
- ä¸º LLM æ ¼å¼åŒ–è¿™äº›å·¥å…·ã€‚
- é€šè¿‡æ­£ç¡®çš„ MCP æœåŠ¡å™¨æ‰§è¡Œå·¥å…·è°ƒç”¨ã€‚

ä»¥ä¸‹æ˜¯å®ƒå¦‚ä½•è¿æ¥åˆ° MCP æœåŠ¡å™¨çš„ç®€è¦ä»‹ç»ï¼ˆ`add_mcp_server` æ–¹æ³•ï¼‰ï¼š

```python
# Lines 111-219 of `MCPClient.add_mcp_server`
# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L111:L219
class MCPClient:
...
async def add_mcp_server(self, type: ServerType, **params: Any):
# 'type' can be "stdio", "sse", or "http"
# 'params' are specific to the server type, e.g.:
# for "stdio": {"command": "my_tool_server_cmd", "args": ["--port", "1234"]}
# for "http": {"url": "http://my.tool.server/mcp"}
# 1. Establish connection based on type (stdio, sse, http)
# (Uses mcp.client.stdio_client, sse_client, or streamablehttp_client)
read, write = await self.exit_stack.enter_async_context(...)
# 2. Create an MCP ClientSession
session = await self.exit_stack.enter_async_context(
ClientSession(read_stream=read, write_stream=write, ...)
)
await session.initialize()
# 3. List tools from the server
response = await session.list_tools()
for tool in response.tools:
# Store session for this tool
self.sessions[tool.name] = session
# Add tool to the list of available tools and Format for LLM
self.available_tools.append({
"type": "function",
"function": {
"name": tool.name,
"description": tool.description,
"parameters": tool.input_schema,
},
})
```

å®ƒæ”¯æŒç”¨äºæœ¬åœ°å·¥å…·çš„ `stdio` æœåŠ¡å™¨ï¼ˆä¾‹å¦‚è®¿é—®æ‚¨çš„æ–‡ä»¶ç³»ç»Ÿï¼‰å’Œç”¨äºè¿œç¨‹å·¥å…·çš„ `http` æœåŠ¡å™¨ï¼å®ƒè¿˜ä¸ `sse` å…¼å®¹ï¼Œ`sse` æ˜¯ä»¥å‰çš„è¿œç¨‹å·¥å…·æ ‡å‡†ã€‚

[
](#using-the-tools-streaming-and-processing)

## ä½¿ç”¨å·¥å…·ï¼šæµå¼ä¼ è¾“å’Œå¤„ç†

`MCPClient` çš„ `process_single_turn_with_tools` æ–¹æ³•æ˜¯ LLM äº¤äº’å‘ç”Ÿçš„åœ°æ–¹ã€‚å®ƒé€šè¿‡ `AsyncInferenceClient.chat.completions.create(..., stream=True)` å°†å¯¹è¯å†å²è®°å½•å’Œå¯ç”¨å·¥å…·å‘é€åˆ° LLMã€‚

[
](#1-prepare-tools-and-calling-the-llm)

## 1. å‡†å¤‡å·¥å…·å¹¶è°ƒç”¨ LLM

é¦–å…ˆï¼Œè¯¥æ–¹æ³•ç¡®å®š LLM åº”è¯¥çŸ¥é“çš„å½“å‰å›åˆçš„æ‰€æœ‰å·¥å…·â€”â€”è¿™åŒ…æ‹¬æ¥è‡ª MCP æœåŠ¡å™¨çš„å·¥å…·å’Œä»»ä½•ç”¨äºä»£ç†æ§åˆ¶çš„ç‰¹æ®Šâ€œé€€å‡ºå¾ªç¯â€å·¥å…·ï¼›ç„¶åï¼Œå®ƒå¯¹ LLM è¿›è¡Œæµå¼è°ƒç”¨ï¼š

```python
# Lines 241-251 of `MCPClient.process_single_turn_with_tools`
# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L241:L251
# Prepare tools list based on options
tools = self.available_tools
```
å¦‚æœ `exit_loop_tools` ä¸æ˜¯ `None`ï¼š

```python
tools = [*exit_loop_tools, *self.available_tools]
# Create the streaming request to the LLM
response = await self.client.chat.completions.create(
    messages=messages,
    tools=tools,
    tool_choice="auto",  # LLM decides if it needs a tool
    stream=True,
)
```

å½“ä» LLM æ¥æ”¶åˆ°æ•°æ®å—æ—¶ï¼Œè¯¥æ–¹æ³•ä¼šéå†å®ƒä»¬ã€‚æ¯ä¸ªæ•°æ®å—éƒ½ä¼šç«‹å³äº§ç”Ÿï¼Œç„¶åæˆ‘ä»¬é‡å»ºå®Œæ•´çš„æ–‡æœ¬å“åº”å’Œä»»ä½•å·¥å…·è°ƒç”¨ã€‚

```
# Lines 258-290 of `MCPClient.process_single_turn_with_tools`
# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L258:L290
# Read from stream
async for chunk in response:
    # Yield each chunk to caller
    yield chunk
    # Aggregate LLM's text response and parts of tool calls
    â€¦
```

## 2. æ‰§è¡Œå·¥å…·

ä¸€æ—¦æµå®Œæˆï¼Œå¦‚æœ LLM è¯·æ±‚äº†ä»»ä½•å·¥å…·è°ƒç”¨ï¼ˆç°åœ¨å·²åœ¨ `final_tool_calls` ä¸­å®Œå…¨é‡å»ºï¼‰ï¼Œè¯¥æ–¹æ³•å°†å¤„ç†æ¯ä¸€ä¸ªè°ƒç”¨ï¼š

```
# Lines 293-313 of `MCPClient.process_single_turn_with_tools`
# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/mcp_client.py#L293:L313
for tool_call in final_tool_calls.values():
    function_name = tool_call.function.name
    function_args = json.loads(tool_call.function.arguments or "{}")
    # Prepare a message to store the tool's result
    tool_message = {"role": "tool", "tool_call_id": tool_call.id, "content": "", "name": function_name}
    # a. Is this a special "exit loop" tool?
    if exit_loop_tools and function_name in [t.function.name for t in exit_loop_tools]:
        # If so, yield a message and terminate this turn's processing
        messages.append(ChatCompletionInputMessage.parse_obj_as_instance(tool_message))
        yield ChatCompletionInputMessage.parse_obj_as_instance(tool_message)
        return  # The Agent's main loop will handle this signal
    # b. It's a regular tool: find the MCP session and execute it
    session = self.sessions.get(function_name)  # self.sessions maps tool names to MCP connections
    if session is not None:
        result = await session.call_tool(function_name, function_args)
        tool_message["content"] = format_result(result)  # format_result processes tool output
    else:
        tool_message["content"] = f"Error: No session found for tool: {function_name}"
    tool_message["content"] = error_msg
    # Add tool result to history and yield it
    ...
```

å®ƒé¦–å…ˆæ£€æŸ¥è°ƒç”¨çš„å·¥å…·æ˜¯å¦é€€å‡ºäº†å¾ªç¯ï¼ˆ`exit_loop_tool`ï¼‰ã€‚å¦‚æœä¸æ˜¯ï¼Œå®ƒä¼šæ‰¾åˆ°è´Ÿè´£è¯¥å·¥å…·çš„æ­£ç¡® MCP ä¼šè¯ï¼Œå¹¶è°ƒç”¨ `session.call_tool()`ã€‚ç„¶åï¼Œç»“æœï¼ˆæˆ–é”™è¯¯å“åº”ï¼‰ä¼šè¢«æ ¼å¼åŒ–ï¼Œæ·»åŠ åˆ°å¯¹è¯å†å²è®°å½•ä¸­ï¼Œå¹¶äº§ç”Ÿï¼Œä»¥ä¾¿ Agent çŸ¥é“å·¥å…·çš„è¾“å‡ºã€‚

## æˆ‘ä»¬çš„å°å‹ Python Agentï¼šå®ƒï¼ˆå‡ ä¹ï¼‰åªæ˜¯ä¸€ä¸ªå¾ªç¯ï¼

ç”±äº `MCPClient` å®Œæˆäº†æ‰€æœ‰å·¥å…·äº¤äº’çš„å·¥ä½œï¼Œæˆ‘ä»¬çš„ `Agent` ç±»å˜å¾—éå¸¸ç®€å•ã€‚å®ƒç»§æ‰¿è‡ª `MCPClient` å¹¶æ·»åŠ äº†å¯¹è¯ç®¡ç†é€»è¾‘ã€‚

Agent ç±»å¾ˆå°ï¼Œä¸“æ³¨äºå¯¹è¯å¾ªç¯ï¼Œä»£ç å¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py)æ‰¾åˆ°ã€‚

## 1. åˆå§‹åŒ– Agent

åˆ›å»º Agent æ—¶ï¼Œå®ƒä¼šè·å– Agent é…ç½®ï¼ˆæ¨¡å‹ã€æä¾›ç¨‹åºã€è¦ä½¿ç”¨çš„ MCP æœåŠ¡å™¨ã€ç³»ç»Ÿæç¤ºï¼‰ï¼Œå¹¶ä½¿ç”¨ç³»ç»Ÿæç¤ºåˆå§‹åŒ–å¯¹è¯å†å²è®°å½•ã€‚ç„¶åï¼Œ`load_tools()` æ–¹æ³•ä¼šéå†æœåŠ¡å™¨é…ç½®ï¼ˆåœ¨ agent.json ä¸­å®šä¹‰ï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªé…ç½®è°ƒç”¨ `add_mcp_server`ï¼ˆæ¥è‡ªçˆ¶ç±» `MCPClient`ï¼‰ï¼Œä»è€Œå¡«å…… Agent çš„å·¥å…·ç®±ã€‚

```
# Lines 12-54 of `Agent`
# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L12:L54
class Agent(MCPClient):
    def __init__(
        self,
        *,
        model: str,
        servers: Iterable[Dict],  # Configuration for MCP servers
        provider: Optional[PROVIDER_OR_POLICY_T] = None,
        api_key: Optional[str] = None,
        prompt: Optional[str] = None,  # The system prompt
    ):
        # Initialize the underlying MCPClient with model, provider, etc.
        super().__init__(model=model, provider=provider, api_key=api_key)
        # Store server configurations to be loaded
        self._servers_cfg = list(servers)
        # Start the conversation with a system message
        self.messages: List[Union[Dict, ChatCompletionInputMessage]] = [
            {"role": "system", "content": prompt or DEFAULT_SYSTEM_PROMPT}
        ]

    async def load_tools(self) -> None:
        # Connect to all configured MCP servers and register their tools
        for cfg in self._servers_cfg:
            await self.add_mcp_server(cfg["type"], **cfg["config"])
```

## 2. Agent çš„æ ¸å¿ƒï¼šå¾ªç¯

`Agent.run()` æ–¹æ³•æ˜¯ä¸€ä¸ªå¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºå¤„ç†å•ä¸ªç”¨æˆ·è¾“å…¥ã€‚å®ƒç®¡ç†å¯¹è¯è½®æ¬¡ï¼Œå†³å®š Agent å½“å‰çš„ä»»åŠ¡ä½•æ—¶å®Œæˆã€‚

```
# Lines 56-99 of `Agent.run()`
# https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/inference/_mcp/agent.py#L56:L99
async def run(self, user_input: str, *, abort_event: Optional[asyncio.Event] = None, ...) -> AsyncGenerator[...]:
    ...
```

```python
while True: # Main loop for processing the user_input
...
# Delegate to MCPClient to interact with LLM and tools for one step.
# This streams back LLM text, tool call info, and tool results.
async for item in self.process_single_turn_with_tools(
self.messages,
...
):
    yield item
...
# Exit Conditions
# 1. Was an "exit" tool called?
if last.get("role") == "tool" and last.get("name") in {t.function.name for t in EXIT_LOOP_TOOLS}:
    return
# 2. Max turns reached or LLM gave a final text answer?
if last.get("role") != "tool" and num_turns > MAX_NUM_TURNS:
    return
if last.get("role") != "tool" and next_turn_should_call_tools:
    return
next_turn_should_call_tools = (last_message.get("role") != "tool")
```

åœ¨ `run()` å¾ªç¯ä¸­ï¼š

- é¦–å…ˆï¼Œå®ƒå°†ç”¨æˆ·æç¤ºæ·»åŠ åˆ°å¯¹è¯ä¸­ã€‚
- ç„¶åï¼Œå®ƒè°ƒç”¨ `MCPClient.process_single_turn_with_tools(...)` æ¥è·å– LLM çš„å“åº”å¹¶å¤„ç†æ¨ç†çš„æ¯ä¸ªæ­¥éª¤çš„ä»»ä½•å·¥å…·æ‰§è¡Œã€‚
- æ¯ä¸ªé¡¹ç›®éƒ½ä¼šç«‹å³äº§ç”Ÿï¼Œä»è€Œå¯ä»¥å®æ—¶æµå¼ä¼ è¾“åˆ°è°ƒç”¨è€…ã€‚
- åœ¨æ¯ä¸ªæ­¥éª¤ä¹‹åï¼Œå®ƒä¼šæ£€æŸ¥é€€å‡ºæ¡ä»¶ï¼šæ˜¯å¦ä½¿ç”¨äº†ç‰¹æ®Šçš„â€œé€€å‡ºå¾ªç¯â€å·¥å…·ï¼Œæ˜¯å¦è¾¾åˆ°äº†æœ€å¤§è½®æ•°é™åˆ¶ï¼Œæˆ–è€… LLM æ˜¯å¦æä¾›äº†å¯¹äºå½“å‰è¯·æ±‚æ¥è¯´ä¼¼ä¹æ˜¯æœ€ç»ˆçš„æ–‡æœ¬å“åº”ã€‚

[
](#next-steps)

## ä¸‹ä¸€æ­¥

æœ‰å¾ˆå¤šå¾ˆé…·çš„æ–¹æ³•å¯ä»¥æ¢ç´¢å’Œæ‰©å±• MCP Client å’Œ Tiny Agent ğŸ”¥ ä»¥ä¸‹æ˜¯ä¸€äº›å¸®åŠ©ä½ å…¥é—¨çš„æƒ³æ³•ï¼š

- è¡¡é‡ä¸åŒçš„ LLM æ¨¡å‹å’Œæ¨ç†æä¾›å•†å¦‚ä½•å½±å“ Agent çš„æ€§èƒ½ï¼šå·¥å…·è°ƒç”¨æ€§èƒ½å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæ¯ä¸ªæä¾›å•†å¯èƒ½ä¼šå¯¹å…¶è¿›è¡Œä¸åŒçš„ä¼˜åŒ–ã€‚ä½ å¯ä»¥åœ¨[æ­¤å¤„](https://huggingface.co/docs/inference-providers/index#partners)æ‰¾åˆ°æ”¯æŒçš„æä¾›å•†åˆ—è¡¨ã€‚
- ä½¿ç”¨æœ¬åœ° LLM æ¨ç†æœåŠ¡å™¨ï¼ˆä¾‹å¦‚ [llama.cpp](https://github.com/ggerganov/llama.cpp) æˆ– [LM Studio](https://lmstudio.ai/)ï¼‰è¿è¡Œå¾®å‹ Agentã€‚
- å½“ç„¶ï¼Œè¿˜å¯ä»¥åšå‡ºè´¡çŒ®ï¼åœ¨ Hugging Face Hub ä¸Šçš„ [tiny-agents/tiny-agents](https://huggingface.co/datasets/tiny-agents/tiny-agents) æ•°æ®é›†ä¸­åˆ†äº«ä½ ç‹¬ç‰¹çš„å¾®å‹ Agent å¹¶æ‰“å¼€ PRã€‚

æ¬¢è¿æå‡º Pull Request å’Œè´¡çŒ®ï¼åŒæ ·ï¼Œè¿™é‡Œçš„ä¸€åˆ‡éƒ½æ˜¯[å¼€æº](https://github.com/huggingface/huggingface_hub)çš„ï¼ğŸ’â¤ï¸