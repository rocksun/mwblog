<!--
title: 人机回路：一种真正奏效的全新AI控制模型
cover: https://cdn.thenewstack.io/media/2025/07/5e9ee0d9-dawit-2b_coiuzcki-unsplash-scaled.jpg
summary: 文章讨论了AI代理从人-在-环路(HITL)向人-在-回路(HOTL)的转变，强调在扩展AI应用时建立适当的控制模型以确保安全高效运行。HOTL通过最小权限工具、运行时可观测性、可触发的干预、验证管道和事后分析就绪的日志记录来实现安全自主性，并需要跨职能治理。
-->

文章讨论了AI代理从人-在-环路(HITL)向人-在-回路(HOTL)的转变，强调在扩展AI应用时建立适当的控制模型以确保安全高效运行。HOTL通过最小权限工具、运行时可观测性、可触发的干预、验证管道和事后分析就绪的日志记录来实现安全自主性，并需要跨职能治理。

> 译自：[Human-on-the-Loop: The New AI Control Model That Actually Works](https://thenewstack.io/human-on-the-loop-the-new-ai-control-model-that-actually-works/)
> 
> 作者：Steve Wilson

你的 AI 代理正在成长。现在是时候让你的控制模型也跟着成长了。

在过去的两年里，大部分关于 AI 的讨论都集中在风险上，这是理所当然的——越狱、数据泄露和意外行为。自从 2023 年中期首次发布 OWASP LLM Top 10 以来，我一直是“谨慎团队”的队长。

现在的问题不再是“AI 是否有风险？”，而是“我们如何安全地扩展 AI？”

AI 代理已经成熟，应用场景也在不断扩大。AI 推理能力正在快速提高。它们正在编写代码、分类警报、解决问题单、起草报告——而且它们正在生产环境中执行这些操作，而不仅仅是在实验室中。现在紧迫的问题不是这些代理是否危险。

紧迫的问题是你是否已经建立了正确的控制模型，以便它们能够安全高效地运行。

现在是从人-在-环路 (Human-in-the-Loop, HITL) 转向人-在-回路 (Human-on-the-Loop, HOTL) 的时候了。

## 从提示工程到 AI 代理监督的演变

我们已经走过了 AI 的复制粘贴时代。今天，代理正在驱动。

让我们看看过去几年 AI 行为的演变。我们从 ChatGPT 开始。它可以生成想法、起草电子邮件或编写代码片段。但是，人类驱动着整个自动化循环。你复制、粘贴、运行代码并处理执行。AI 是被动的。你负责，因为你做了所有的工作。

然后出现了像 Cursor 这样的工具。Cursor 赋予了 AI 在编码工作流程中更大的权力。它可以读取和写入文件、执行命令以及直接修改你的代码库。然而，在典型用法中，它经常暂停，在采取大多数操作之前寻求人类开发人员的指导或许可。即使人类互动只是简单地重复按 Tab 键，人类仍然从始至终完全参与。这就是实践中的人-在-环路：AI 工作，但人类驱动。

现在我们看到了一种不同的模式出现，尤其是在像 Claude Code 这样的工具中，它们倾向于允许更多自主性的操作模式。

你仍然可以保守地运行 Claude Code，但许多开发人员现在允许它更自主地运行。它不再需要不断检查，而是提出一个计划，获得一次批准，然后在多个步骤中执行——编写、测试、调试和迭代。这些无人监督的工作流程步骤，过去在人类批准检查之间只有几秒钟的时间，现在通常可以延长到 10 分钟或更长时间。

你仍然参与其中。你正在监控。但你不是在微观管理。

这就是人-在-回路——并且它正在迅速成为扩展的唯一可行途径。

而且这不仅仅是一个软件故事。在国防领域，同样的辩论也在进行中，军事领导人正在权衡是否应该允许自主无人机在没有人在环路的情况下采取致命行动。这不是科幻小说。这是国家层面的系统架构。

## 侧边栏：空中的自主性

HITL 与 HOTL 的辩论不仅仅是一个软件问题——它正在世界领先的国防项目中展开。

多个政府正在探索完全自主的战斗机，能够识别威胁并在没有实时人工输入的情况下执行致命武力。

与此同时，对“忠诚僚机”系统进行了大量投资——半自主无人机与人类飞行员并肩飞行，执行委派的任务，同时保持人类牢牢地控制着环路。

这是一个备受争议的设计选择。完全自主性承诺速度和覆盖范围。但是 HOTL 设计提供更好的责任性、协调性和人类判断。

目前，忠诚僚机模式占据优势。它捕捉了自主性的许多好处——而没有切断与人类决策的联系。

这不是一个哲学脚注。这是一个实际的设计决策，它决定了自主性如何在你的系统中工作——以及是否工作。

## 构建安全自主性：HOTL 实施框架

HOTL 不仅仅是关于开发人员的工作流程。它是关于在机器代表我们行动的*任何地方*扩展自主性。

今天最成熟的例子是在软件开发中——但是相同的模式正在跨领域出现：

* 管理日历、文档和外联的生产力代理。
* 解决问题和路由问题单的客户支持机器人。
* 物流、金融和基础设施中的自主系统。

除了企业之外，其影响甚至更为重大。机器何时应该独立行动，何时应该服从人类的问题不仅仅是关于代码。而是关于政策、安全和伦理。

## **安全自主性的实际样子**

人-在-回路并不意味着消除安全措施。这意味着构建不*依赖*于持续中断来保持安全的系统。

如果你希望你的代理在不耗尽其人类处理者的情况下高效且负责任地行事，你需要：

* **最小权限工具**
  + 不要给你的代理 blanket 权限。限制它可以访问的内容以及它可以使用的工具。较小的信任面 = 较小的风险。
* **运行时可观测性 (obserablity)**
  + 实时跟踪你的代理正在做什么。这包括命令、文件编辑、工具使用和外部调用。不仅仅是日志——而是遥测数据。
* **可触发的干预**
  + 为升级而设计。当代理遇到意外情况或高风险操作时，它们应该暂停、通知或路由给人类。
* **验证管道**
  + 来自代理的输出——尤其是那些影响系统或用户的输出——应该像 CI/CD 中的人类代码一样，通过验证管道。
* **事后分析就绪的日志记录**
  + 当出现问题时，你应该能够看到发生了什么以及原因。可追溯性不是可选的——它是基础。

## AI 代理控制系统的跨职能治理

如果你正在领导一项 AI 转型工作——作为首席 AI 官、产品主管或职能领导——这种转变会直接影响你。

你不能仅仅依靠你的工程团队来设置代理行为的边界。这是一个跨职能的治理问题。

* 法律和合规部门需要权衡可接受的自主性。
* 产品和 UX 团队需要定义代理和用户之间交接发生的位置。
* 安全需要为运行时监控和遏制而设计。
* 如果没有跨职能治理，AI 代理就会变成生产力海市蜃楼——或者更糟，变成安全责任。

转向 HOTL 会改变你的运营模式。忽视它不会延迟变化——它只会确保你的组织在变化到来时没有做好准备。

## **你不能袖手旁观**

今天最有效的代理是在结构化自主性下运行的代理。不是被压制。不是完全自由。只是快速、有能力和受监督。

这就是人-在-回路。

这不是一种妥协。而是一个蓝图。

如果你仍然需要在每个步骤都获得人工批准，那么你就是在束缚自己。如果你在没有监督的情况下将完全控制权交给模型，那么你就是在冒险。

但是，如果你使用护栏、可观测性和精心设计的自主性边界来构建你的代理，那么你就可以大规模地释放代理 AI 的真正价值。

所以不要再等待许可了。

重新设计你的系统。定义你的边界。让人类参与到回路中，而不是进行微观管理。