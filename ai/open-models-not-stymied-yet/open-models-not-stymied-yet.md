<!--
title:  开放模型还在康庄大道上
cover: https://cdn.thenewstack.io/media/2023/12/bb82548c-jim-zemlin-1-1024x576.jpg
-->

封闭基础模型的支持者担心开源大型语言模型技术可能落入错误的手中，Linux基金会的Jim Zemlin在AI.dev大会上否认了这种“模糊”的观点。

> 译自 [Open Models: Not Stymied Yet](https://thenewstack.io/open-models-not-stymied-yet/)，作者 Alex Williams 是 The New Stack 的创始人兼出版商。他是一位资深的技术记者，曾在 TechCrunch、SiliconAngle 和现在被称为 ReadWrite 的地方工作。自 1980 年代末以来，Alex 一直是一名记者，起初在...

SAN JOSE, CALIF. — 来自 Linux 基金会执行董事 Jim Zemlin 在该组织在这里举行的首届 [AI.dev 大会上](https://events.linuxfoundation.org/ai-dev-north-america/)的主题演讲中表示，对于封闭基础模型的论点源自一些模糊的担忧，即开源[大型语言模型](https://thenewstack.io/what-is-a-large-language-model/)（LLM）技术在错误的手中可能变得过于危险。

“不幸的是，今天我们开始看到一些趋势，远离[生成式 AI](https://thenewstack.io/ai/) 的开放性，转向更多只能通过 API 访问的封闭基础模型，” Zemlin 在他的主题演讲中说道。“我发现这些论点相当模糊且不令人信服。”

背景是来自欧盟的有关首次人工智能法规的[消息](https://www.reuters.com/technology/eus-ai-act-could-exclude-open-source-models-regulation-2023-12-07/)。开源LLM获得了豁免。这是一个宽慰，因为该法律的原始版本将对基础模型，包括开源模型，进行严格的监管。在遭到反对之后，欧盟删除了严格的限制，采取了更为慎重的态度。

完整文本仍然保密。其他监管机构将密切研究该法律以获取指导，给人们带来了一些希望，即个别国家可能会注意到并认识到为什么开源LLM是比关闭开放创新更安全的方法。

[Mistral](https://docs.mistral.ai/)，这个开源LLM，得知这一消息后迅速行动。由[OpenAI](https://thenewstack.io/beyond-chatgpt-exploring-the-openai-platform/)和Google技术人员组成的Mistral在欧盟的消息传出后立即发布了其开源LLM。更令人振奋的是，该公司还筹集了4.15亿美元。

讽刺的是，生成式人工智能大型语言模型技术的许多发展都源于开源技术。开源软件使基础模型和机器学习模型的构建成为可能。LLM 技术是由 PyTorch、Python 软件包、开放数据集以及学术研究等社区的努力所成就的。

Zemlin 表示，更多的危险来自于关闭开放创新。对开放性的限制往往只会使一小部分现有企业受益。

而恶意行为者将无视禁令。开源有助于更好的隐私和更安全的世界。这与LLM有什么关系呢？一个开源的LLM或开源模型是透明的，因此人们可以发现漏洞、审查参数等。

从专有LLM中无法获得这种透明度。

“你知道，关于大型语言模型有一点奇怪的是我们不太知道它们实际是如何工作的，” Zemlin 说道。“不知道它们实际是如何工作的。很难得到第二个属性，即信任，对吧？我们怎么知道幻觉发生在什么时候？我们怎么知道数据来自哪里？”

## 企业担心 GenAI 安全问题

企业计划投资于生成式人工智能。Zemlin 表示，阻碍它们的是[安全性](https://thenewstack.io/security/)问题。他引用了 Linux 基金会的最新研究。

![](https://cdn.thenewstack.io/media/2023/12/6a2d19a2-subject-1024x610.jpg)

"几乎所有我们交流的人都表示，构建生成式人工智能工具和基础模型的技术应该是开放的，并且应该由他们可以长期信赖的中立组织管理，尤其对于这样一项基础技术而言是至关重要的，" Zemlin 表示。

透明性为企业中的人们提供了他们在业务决策和结果方面所需的信心。

但开源LLM是否真的对人类构成如此威胁呢？对于人类来说，是否定的。对于大科技公司呢？嗯，这取决于你如何定义开源。

