
<!--
title: 推动AI和大数据增长的隐秘悖论
cover: https://cdn.thenewstack.io/media/2025/07/ceed83b6-jevons-paradox.jpg
summary: 文章探讨了杰文斯悖论在人工智能和数据领域的应用，指出效率提高反而导致资源消耗增加。强调了高质量、长期数据保留的重要性，并分析了旧模式下数据处理的局限性。提出新模式需具备高维度、高基数、按需可用性和经济高效的特点，以满足人工智能模型的需求。
-->

文章探讨了杰文斯悖论在人工智能和数据领域的应用，指出效率提高反而导致资源消耗增加。强调了高质量、长期数据保留的重要性，并分析了旧模式下数据处理的局限性。提出新模式需具备高维度、高基数、按需可用性和经济高效的特点，以满足人工智能模型的需求。

> 译自：[The Obscure Paradox Fueling AI and Big Data Growth](https://thenewstack.io/the-obscure-paradox-fueling-ai-and-big-data-growth/)
> 
> 作者：Franz Knupfer

[杰文斯悖论](https://en.wikipedia.org/wiki/Jevons_paradox)指的是这样一种现象：当资源的使用效率提高时，资源的消耗反而会增加。在 19 世纪，经济学家威廉·斯坦利·杰文斯 (William Stanley Jevons) 发现，随着煤炭开采效率的提高，成本降低导致需求增加和使用量增加。现在，这种 19 世纪的悖论正被应用于人工智能使用和生产力的爆炸式增长。微软 CEO [萨蒂亚·纳德拉 (Satya Nadella) 在谈到杰文斯悖论时](https://www.npr.org/sections/planet-money/2025/02/04/g-s1-46018/ai-deepseek-economics-jevons-paradox)表示：“随着人工智能变得更加高效和易于访问，我们将看到它的使用量猛增，将其变成一种我们永远无法满足的商品。”

随着摄取、保留和分析数据的解决方案变得更加高效和易于访问，对数据的需求及其价值将继续增长。通过利用现代数据解决方案，实现经济高效的长期数据保留，企业可以生成大型、高质量的数据集，并提高其人工智能就绪度。

让我们更仔细地看看杰文斯悖论，以及它如何应用于人工智能和数据——然后研究一下成本高昂、短期数据保留的旧模式是如何被新的数据模式所取代的，这种新模式将提高数据质量并实现更有效的人工智能模型。

## 杰文斯悖论如何应用于人工智能和数据

杰文斯悖论背后的基本概念相当简单：更高的效率导致更多的资源消耗，而不是更少，因为更多的人采用高效技术。将其应用于人工智能，效率的提高和成本的降低将导致人工智能工具的消耗增加。

然而，人工智能模型在运行时需要大量的计算，并且通常在大型数据集上训练时表现更好。这通常意味着计算（训练模型）和存储（以易于访问的格式保存大量数据）的巨大成本。

OpenAI 可能是最突出的例子，据估计，2024 年花费了 [40 亿美元](https://www.axios.com/2024/10/03/openai-investors-profit-money-costs) 来维持 ChatGPT 的运行。其中大部分成本是由于计算和 ChatGPT 运行所需的大量数据造成的。虽然 OpenAI 是一个特别昂贵的例子，但许多企业都在面临类似的挑战，即模型的训练和运行成本都很高，从而降低了投资回报率 (ROI)，甚至导致亏损。

提高人工智能的效率（并降低成本）部分取决于提高摄取、存储和分析大量数据的效率。反过来，这种更高的效率将导致更多的数据消耗——无论是训练人工智能还是支持团队用于诸如数据科学中的[可观测性](https://thenewstack.io/observability-2-0-or-just-logs-all-over-again)等用例。换句话说，杰文斯悖论也适用于数据。数据解决方案在人工智能模型的大数据方面必须变得更加高效和具有成本效益，才能满足期望，不仅能产生收入，还能产生实际利润。

不幸的是，许多企业在数据方面都面临着瓶颈。新的数据密集型模式与旧模式发生冲突，在旧模式中，PB 级的数据传统上来说保留和分析成本太高。

## 旧模式：低可用性和丢弃的数据

在旧模式中，企业通常通过两种方式处理数据的巨大增长（例如日志）：

* **快速将数据移动到难以访问的经济高效的存储中。** 非结构化数据可能会被移动到数据湖中，如果以后不进行结构化，数据湖可能会很快变成数据沼泽。查询通常既困难又昂贵。与此同时，安全和遥测数据可能会被移动到冻结存储或存档中，在那里重新激活既耗时又困难。由于这些数据并非设计用于大量查询，因此模型训练既低效又昂贵。
* **对数据进行采样、聚合或丢弃，导致数据集不完整。** 大量数据被完全丢弃，要么通过采样，要么通过聚合（汇总数据然后丢弃底层数据）。这在遥测数据中尤其常见。结果是低保真数据，可能导致不准确或延迟的模型。

旧模式将数据视为一种昂贵的、通常不是很有用的商品，不需要长期保留。这种方法的一个常见例子是可观测性，其中[日志数据](https://thenewstack.io/observability-isnt-enough-its-time-to-federate-log-data)只保留很短的时间，以在问题发生时缓解问题，然后丢弃。

很容易理解为什么企业传统上偏爱这种方法。摄取、存储和分析数据的成本是（而且现在仍然是）一个真实的、通常很大的数字——这是一笔会削减利润的账单。相比之下，数据的价值通常更难量化。例如，一年前的内容分发网络 (CDN) 日志的价值是多少？可能需要保留它以用于安全和合规性，但在其他方面，它的价值是模糊的，被认为是零。但现在，这些数据可以用于训练用于异常检测、用户个性化等模型，从而提高其价值。

## 新模式：人工智能需要更多数据

在训练人工智能模型时，通常更多的数据更好，并且它会导致更好的泛化和更少的偏差。更少的数据可能导致欠拟合和简单的模型，无法做出有用的预测。一些模型（例如那些专注于异常检测的模型）需要有趣的异常值才能变得更有效。这些异常值可能只存在于非常大的数据集中。

为了满足人工智能模型的数据密集型需求——并确保模型变得更有效、更高效和更经济实惠——数据解决方案必须提供完全保真的数据，使企业能够在现在和将来训练模型。这些解决方案必须提供以下内容：

### 高维度

日志和其他类型的数据可以有数百甚至数千个维度。虽然在训练模型时通常会限制维度，但企业通常无法预测他们将来需要哪些维度进行训练。术语“[任意宽、结构化事件](https://www.honeycomb.io/blog/time-to-version-observability-signs-point-to-yes)”在可观测性中变得流行，因为它允许企业对数据进行切片和切块，以用于许多不同的监控和可观测性用例。出于同样的原因，任意宽度的数据对于训练代理人工智能也很重要。以许多不同的方式对数据进行切片和切块的能力使企业在训练方面具有最大的灵活性。所有这些维度都是模型的可能参数。

### 高基数

[高基数](https://hydrolix.io/blog/high-cardinality-data/)列有许多唯一值，当数据量很大时，如果数据解决方案没有配备处理能力，则高基数数据的存储和分析成本可能特别高。但高基数通常是大型、完全保真数据集的属性。基数减少技术也会降低数据的粒度，使其在模型训练中不太准确。数据解决方案必须能够有效地摄取、存储和查询这些数据，同时使用压缩技术尽可能地减小其大小。

### 按需可用性

将大型数据集移动到[冻结或存档存储](https://thenewstack.io/stop-freezing-your-data-to-death)的传统方法会导致成本高昂、耗时的数据重新激活。相反，数据必须按需提供以用于模型训练。对于许多利益相关者访问的数据集（例如，数据科学和站点可靠性团队可能需要实时访问相同的日志数据），可以将模型训练安排在非高峰时段和周末，并且解决方案应该能够根据需要快速扩展计算资源。换句话说，数据应该是“热”的，以实现高可用性，而不是冷或冻结的（导致低可用性）。并且还需要将其提供给用于训练模型的平台，例如 Apache Spark 生态系统。

### 经济高效

摄取、存储和分析高维度、高基数数据集——并保持它们随时可用的传统方法——非常昂贵。这些解决方案通常使用 SSD 和其他昂贵的存储解决方案来实现高可用性和快速查询。它们并非专为许多企业现在摄取的数据量而设计——通常每天数 TB——并且它们早于代理人工智能。现代解决方案最大限度地提高了经济高效、可水平扩展的对象存储的性能，从而可以保留更多数据更长时间。例如，像 Databricks 这样的平台正在大力投资 Apache Iceberg 生态系统，该生态系统使用诸如列式存储、Parquet 压缩和分区等技术来使商品对象存储具有高性能。

## 杰文斯悖论的真实日志记录示例

让我们看一个具体的用例，其中效率和可访问性的提高极大地提高了传统上被丢弃的数据的价值：CDN 日志。

CDN 生成大量日志数据。媒体提供商等全球企业每天可以生成数 TB 的 CDN 日志数据。像[超级碗这样的活动可以在短短几个小时内生成](https://hydrolix.io/blog/hyperscale-logging/)数百 TB 的 CDN 日志数据。传统上，CDN 日志数据不会被保留，因为它太昂贵，因此价值低。为什么企业要监控代表他们处理可交付性、增强的安全性和可靠性的 CDN？

但随着像 Hydrolix 这样的现代解决方案使保留完全保真的 CDN 日志更具成本效益，企业开始保留它们，并且它们的价值变得更加明显。它们可用于检测 DDoS 攻击等问题，并监控 CDN 切换等用例的性能。

这些只是一些短期用例。与此同时，长期保留 CDN 日志的企业可以使用这些日志来更好地了解用户的行为和偏好。在足够长的时间范围内（一年或更长时间），可以深入了解季节性销售、产品发布和其他高收入、高影响事件等周期性事件，这些事件可能会决定盈亏。

现在保留这些日志的企业拥有庞大的 PB 级数据集，这些数据集可用于训练人工智能模型，使它们比没有这些数据的企业具有竞争优势。流媒体、广播和其他垂直行业的企业现在正在分析这些数据以获得更深入的见解。

CDN 日志数据包含有价值的信息，例如 IP 地址和所有 HTTP 请求的记录，包括请求的资产，从而可以创建详细的用户配置文件。人工智能模型可用于更好地了解哪些帐户有流失的风险，或哪些新产品最能吸引单个客户，然后发送个性化电子邮件或投放有针对性的广告。CDN 日志数据可能包含异常的异常值，例如低速攻击和其他恶意行为，这些行为可以在较长时间内检测到。人工智能模型可以更快地检测和响应这些安全异常。

在短短几年内，CDN 日志数据已从“可丢弃”日志数据的高成本来源转变为可以支持更好的性能和安全性并为人工智能模型提供深入训练的有价值的商品。这得益于杰文斯悖论背后的理论。随着像 Hydrolix 这样的现代解决方案使摄取、存储和分析大数据更具成本效益，它正变得越来越有价值的商品，并且消耗量将继续上升。在开发人工智能模型方面，未能有效最大化其数据价值的企业将被抛在后面。

## 人工智能时代的高质量数据

高质量数据对于构建人工智能模型至关重要——但数据质量差是许多企业面临的一个巨大问题。根据 Gartner 报告，Zain Khan 的“[通过专注于基础数据工具和技术做好人工智能准备](https://www.gartner.com/en/documents/5923775)”，“到 2025 年，数据质量差将仍然是阻止高级分析（例如，人工智能）部署的最常被提及的挑战之一。”

对于许多类型的数据，需要时间来生成大型、高质量的数据集。以天气数据为例。一周或一个月的天气数据对于人工智能模型来说根本不会非常有用。在夏季数据上训练的模型在每个其他季节都会不准确。它没有关于气候变化等长期趋势或极端天气事件模式的数据。

现在将这个类比应用于销售或运营数据。夏季销售的数据对于冬季的同等销售可能非常不准确。需求、用户、性能和恶意行为的变化等微妙的长期趋势将无法检测到。

许多商业领袖现在想要代理人工智能和产生收入的人工智能模型——但如果企业没有长期、高质量的数据，它们就已经远远落后于曲线，并且它们还无法创建这些模型。没有高质量数据，根本不可能构建高质量模型。并且由于数据经常被丢弃的旧的、限制性数据模式，许多企业没有他们需要的数据。

正如那句名言所说，种树的最佳时间是 20 年前，其次是现在。由于成本高昂而仍在丢弃数据的企业必须重新思考他们的方法，并采用允许长期保留和随时可用的数据的现代数据解决方案，无论其存在时间长短。他们今天必须种下这些种子。其中一些数据对于人工智能模型具有明显的价值，但数据的价值并不总是显而易见或易于量化——至少目前还没有。但是，企业今天保留的数据将成长为明天的 AI 模型。其中将有一些令人惊讶的幼苗，它们将创造新的收入来源和产品。不要因为丢弃数据而错失良机。