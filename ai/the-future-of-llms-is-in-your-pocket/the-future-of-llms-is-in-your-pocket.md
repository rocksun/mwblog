
<!--
title: 大语言模型的未来就在你的口袋里
cover: https://cdn.thenewstack.io/media/2024/07/3bdf2905-phone-869669_1280.jpg
-->

在模型可以在普通消费级硬件上运行的世界中，创新潜力要大得多。这对开发者来说具有重大意义。

> 译自 [The Future of LLMs Is in Your Pocket](https://thenewstack.io/the-future-of-llms-is-in-your-pocket/)，作者 Javier Redondo。

想象一个世界，其中一个设备充当用户界面，并远程连接到执行实际计算的第二个设备。这在 1960 年代很常见。[电传打字机](https://en.wikipedia.org/wiki/Teleprinter) 用于输入命令和输出结果，可以在办公室环境和高中图书馆等地方找到。但是，代码执行过于资源密集，无法放在每个房间。相反，每个电传打字机都会远程连接到一个大型的、时间共享的计算机，该计算机在许多客户端之间共享。

当前的[生成式 AI 架构](https://thenewstack.io/the-architects-guide-to-the-genai-tech-stack-10-tools/) 处于电传打字机时代：一个应用程序在手机上运行，但它依赖于只能在云端托管的模型。这是一种过时的遗留产物。几十年来，电传打字机和大型机让位于个人电脑。同样，生成式 AI 最终将在消费级硬件上运行——但这种转变将发生得更快。

这种转变对应用程序开发人员具有重大意义。

## 我们是如何走到这一步的

你可能知道，生成式 AI 模型由计算密集型步骤定义，这些步骤将输入（例如，提示）转换为输出（例如，答案）。此类模型由数十亿个参数（也称为权重）指定，这意味着生成输出还需要数十亿个操作，这些操作可以在硬件提供的尽可能多的核心上并行化。GPU 拥有数千个核心，非常适合运行生成式 AI 模型。不幸的是，由于消费级 GPU 的内存有限，它们无法容纳大小为 10GB 的模型。因此，生成式 AI 工作负载已转移到数据中心，数据中心配备了（昂贵的）工业 GPU 网络，这些网络将资源集中在一起。

我们一直在听到模型会“不断改进”，因此它们会继续变得更大。我们持相反观点：虽然模型性能的任何阶跃式变化在出现时都可能是革命性的，但使当前一代模型能够在用户设备上运行具有同样深远的影响——而且今天已经可以实现。

## 为什么这很重要

在讨论本地模型的可行性之前，需要回答一个问题：为什么要费心？简而言之，本地模型改变了生成式 AI 开发人员的一切，而依赖于云模型的应用程序可能会变得过时。

第一个原因是，由于 GPU 的成本，生成式 AI 打破了 SaaS 享受的近乎零边际成本模型。如今，任何捆绑生成式 AI 的产品都需要很高的座位价格才能使产品在经济上可行。这种与底层价值的脱节对许多无法以最佳价格定价以最大化收入的产品来说是至关重要的。在实践中，一些产品受到价格底线的限制（例如，不可能以 50% 的折扣出售 10 倍的销量），而一些功能无法推出，因为加价无法支付推理成本（例如，视频游戏中的 AI 角色）。使用本地模型，价格不再是问题：它们完全免费。

第二个原因是，远程模型的用户体验可能会更好：生成式 AI 使得有用的新功能成为可能，但它们往往以更差的体验为代价。以前不依赖于互联网连接的应用程序（例如，照片编辑器）现在需要它。远程推理会带来额外的摩擦，例如延迟。本地模型消除了对互联网连接的依赖。

第三个原因与模型如何处理用户数据有关。这在两个方面发挥作用。首先，人们对与 AI 系统共享越来越多的私人信息表示严重关切。其次，大多数生成式 AI 采用者被迫使用通用（也称为基础）模型，因为个性化模型的规模化分发过于困难。本地模型保证数据隐私，并为与设备数量一样多的模型变体打开了大门。

## 我们需要 1T 个参数吗？

生成式 AI 模型将在本地运行的想法可能听起来令人惊讶。随着时间的推移，一些模型（如 SOTA（最先进的）[LLM](https://thenewstack.io/what-is-a-large-language-model/)）的大小不断增加，已经达到了[1T+ 个参数](https://www.semianalysis.com/p/gpt-4-architecture-infrastructure)。这些（以及可能正在开发的更大模型）不会很快在智能手机上运行。

然而，大多数生成式应用程序只需要可以在消费级硬件上运行的模型。对于非 LLM 应用程序，例如转录（例如，[Whisper](https://huggingface.co/openai/whisper-large-v3) 大约 15 亿参数）和图像生成（例如，[Flux](https://huggingface.co/black-forest-labs/FLUX.1-dev) 大约 120 亿参数），只要最先进的模型足够小，可以放入设备的内存中，这种情况就成立。对于 LLM 来说，情况就不那么明显了，因为有些 LLM 可以运行在 iPhone 上（例如，[Llama-3.1-8B](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)），但它们的性能明显不如 SOTA。

但这并不是故事的结束。虽然小型 LLM 对世界的了解更少（即它们更容易产生幻觉）并且在遵循提示指令方面不太可靠，但它们可以通过[图灵测试](https://thenewstack.io/happy-birthday-alan-turing-also-sorry/)（即，流畅地对话）。这是一个最近的发展——事实上，在我们看来，这是过去一年中取得的主要进展，与 SOTA LLM 的进展缓慢形成鲜明对比。这是通过在训练中利用更大规模、质量更高的数据集，并应用[量化](https://huggingface.co/docs/optimum/en/concept_guides/quantization)、[剪枝和知识蒸馏](https://arxiv.org/pdf/2407.14679) 等技术来进一步减小模型尺寸实现的。

现在可以通过微调来弥合知识和技能差距——教模型如何处理特定任务，这比提示 SOTA LLM 更具挑战性。一种已知的方法是使用大型 LLM 作为教练。简而言之，如果 SOTA LLM 在该任务上很熟练，它可以用来生成许多成功的完成示例，小型 LLM 可以从这些示例中学习。直到最近，这种方法在实践中还不可用，因为 OpenAI 的 GPT-4 等专有 SOTA 模型的使用条款明确禁止这样做。[Llama-3.1-405B](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct) 等开源 SOTA 模型的出现，没有这种限制，解决了这个问题。

最后，一个潜在的担忧是，替代单一的一站式 1 万亿参数模型的将是一百个特定任务的 100 亿参数模型。现实情况是，特定任务的模型本质上都是相同的。因此，一种名为[LoRA](https://arxiv.org/abs/2106.09685) 的方法可以实现“适配器”，其大小可以小于它们修改的基础模型的 1%。这在很多方面都是一个胜利。除其他事项外，它简化了微调（更轻的硬件要求）、模型分发给最终用户（适配器尺寸小）以及应用程序之间的上下文切换（由于尺寸小，可以快速切换）。

## 催化剂已经出现

[小型模型](https://thenewstack.io/use-small-language-models-to-deploy-ai-on-a-budget/) 可以提供所有上下文（音频、图像和语言）中的最佳能力，同时出现的是运行它们的必要生态系统。

在硬件方面，苹果公司凭借其 ARM 处理器走在了前列。这种架构具有先见之明，使 macOS 和 iOS 设备能够在生成式 AI 模型流行之前运行它们。它们捆绑了一个能够进行计算的 GPU，并配备了高带宽内存，这[通常是推理速度的限制因素](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#understand-perf)。

苹果公司并非孤军奋战，这种转变正在席卷整个硬件阵容。为了不落后，配备微软 Copilot+ 认证的笔记本电脑也可以运行生成式模型。这些机器依赖于高通的骁龙 X Elite 等新型芯片，表明硬件现在正在被设计成能够进行本地推理。

在软件方面，虽然 PyTorch 仍然是云计算领域的王者，但一系列新的库已经做好了充分的准备，可以更好地利用消费级硬件。这些库包括苹果的 MLX 和 GGML。像设备上的 ChatGPT 替代品这样的原生应用程序已经使用这些工具作为后端，而 WASM 绑定的发布使从浏览器加载的任何网站都可以做到这一点。

还有一些问题需要解决，特别是关于开发人员可以在给定设备上找到什么。小型基础模型仍然有几 GB 大，因此对于几乎所有应用程序（Web 或原生）来说，它们都不是实用的独立依赖项。然而，随着 Apple Intelligence 的发布，我们预计 macOS 和 iOS 将在操作系统中捆绑并公开一个 LLM。这将使开发人员能够[以 10 MB 的大小](https://machinelearning.apple.com/research/introducing-apple-foundation-models) 发布 LoRA 适配器，其他操作系统也将效仿。

虽然开发人员可能面临每个设备捆绑的模型之间不一致的问题，但融合很可能发生。我们无法确定这将如何发生，但 Apple 在 [DCLM](https://arxiv.org/abs/2406.11794) 中的决定是开源模型权重和训练数据集，这鼓励并使其他人能够训练行为类似的模型。

## 对应用程序开发人员的影响

向设备上处理的转变对应用程序开发人员具有重大影响。

首先，假设 LLM 推理是免费的，这消除了任何生成式 AI 功能的成本底线。您可以构建哪些新事物，以及这对您现有的产品有何影响？我们预测三种情况：

- 生成式 AI 是一个更大产品的功能时，它将更无缝地融入现有的 SaaS 层，使其与其他推动升级的高级功能处于同一水平。
- 生成式 AI 是核心价值主张，产品以“成本加成”定价（即成本决定价格点）时，产品会更便宜，但这将被更大的销量所抵消。
- 生成式 AI 是核心价值主张，产品以对用户的价值定价（即远高于成本）时，影响将仅限于利润率的改善。

其次，认识到应用程序开发方式正在发生转变，尤其是那些依赖 LLM 的应用程序：“提示工程”和“少样本训练”已经过时，微调现在成为中心。这意味着构建生成式 AI 应用程序的组织将需要不同的能力。SOTA LLM 的一个好处是软件工程师与模型分离，模型被视为像任何其他微服务一样工作的 API。这消除了对内部 ML 工程师和数据科学家团队的依赖，而这些资源是许多组织没有的，或者肯定没有达到在整个组织中引入生成式 AI 所需的规模。另一方面，这些配置文件是许多本地模型工作流程所必需的。虽然没有 ML 背景的软件工程师确实随着对 AI 的关注度提高而提高了他们的 ML 技能，但这是一个更高的提升步骤。从短期来看，产品更难构建，因为它们需要差异化的模型，而不是依赖 SOTA 基础 LLM。然而，从长远来看，差异化的小型模型使最终产品更有价值。

这些都是积极的演变，但只有那些最关注颠覆性动态的人才能领先并获得收益。

## 基础设施创新的机会

最后，向本地模型的转变需要一个经过修改的技术栈。在云托管模型的背景下已经存在的一些类别变得更加必要，并且可能需要扩展其产品：

- 基础模型：基础模型公司最初只有一个目标：创建最好的 SOTA 模型。虽然许多公司已经部分或全部转向构建具有最佳性价比的模型，但针对消费级硬件的目标尚未进入我的脑海。随着本地模型成为主要的消费方式，优先级将发生变化，但现在有很多空白需要填补。
- 可观察性和护栏：随着开发人员将 AI 应用程序投入生产，媒体突出了它们不稳定的行为（例如，幻觉、毒性）。这导致需要提供可观察性的工具，在某些情况下，还需要对模型行为进行严格的限制。随着模型分布式实例的激增，这些挑战加剧了，此类工具的重要性也随之提高。
- 合成数据和微调：虽然微调在 SOTA 模型时代一直是许多应用程序开发人员的次要考虑因素，但在处理更少参数时，它将成为中心。我们认为开源 SOTA 模型使合成微调数据集成为可能，任何人都可以设置自己的微调管道。然而，我们知道能够做这些事情的人员很少，因此我们相信合成数据和按需微调是需求将大幅增长的领域。

与此同时，本地模型的要求使我们相信将出现几个新的类别：

- 模型 CI/CD：我们目前还不清楚开发人员将如何将模型（或模型适配器）交付给应用程序。例如，模型是否会与本机应用程序二进制文件一起交付，或者是在应用程序加载时从某个存储库下载？这带来了其他问题，例如模型更新的频率以及如何处理模型版本。我们相信将出现解决这些问题的解决方案。
- 适配器市场：虽然单个 SOTA LLM 可以服务于所有应用程序，但我们已经确定，使小型模型跨任务工作需要不同的适配器。许多应用程序无疑将依赖于独立开发的适配器，但某些适配器也可以在许多应用程序中发挥作用，例如，摘要和改写。只有部分开发者希望独立管理此类标准适配器的开发和交付生命周期。
- 联合执行：虽然这不是一个全新的类别，但在消费级硬件上运行模型对于那些考虑联合 ML（即分布式训练和推理）的人来说是一种新的范式。这里的重点不是在互联网上连接的大量设备集群，而是更多地关注本地网络中小型设备集群，例如，在同一个办公室或家中。我们已经看到这里正在进行的创新，它使更密集的计算工作负载（如在中等规模模型上的训练或推理）能够通过在两到三台设备之间分配工作来实现。

## 展望未来

未来，人工智能将离开云端，落到用户设备上。了解实现这一可能性所需的要素已经到位，这将以更低的成本带来更好的产品。在这种新的范式中，组织需要更新其市场营销策略、组织技能和开发者工具包。虽然这种演变将产生重大影响，但我们认为这并非故事的终结。

如今，人工智能在供应链上下游仍然高度集中。SOTA GPU 仅由一家公司设计，该公司依赖于一家代工厂进行制造。托管该硬件的超大规模云服务提供商屈指可数，开发人员在寻找 SOTA 模型时所依赖的 LLM 提供商也是如此。在一个模型在商品消费级硬件上运行的世界中，创新潜力要大得多。这是值得期待的事情。
