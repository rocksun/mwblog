Anything that succeeds in production runs exactly as expected: reliably, transparently and without breaking under high pressure. The same should be true for generative AI (GenAI). You build the application, test it and stress it. But moving it into production is rarely simple. [Only about 5%](https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf) of pilots make it to production. Development and production environments are often mismatched.

[Earlier AI models](https://thenewstack.io/what-is-real-artificial-intelligence/) (traditional AI) were easier to test and deploy. Today’s GenAI systems, by contrast, require a lot more collaboration, constant validation and a robust CI/CD pipeline into production. With complexity increasing, teams need a “shift-left” mindset to test earlier, monitor continuously and manage models like code.

For financial institutions, the challenge is even greater. Many have promising proofs of concept, but few have mastered the operational rigor required for production. GenAI introduces many new risks, like hallucinations, unpredictable behavior and unclear accountability. It’s like self-driving cars: Even if accidents are rarer, everyone still asks who was behind the wheel. Moving GenAI into production demands discipline, the right people, clear roles and oversight to ensure it [works in the real world](https://thenewstack.io/beyond-the-code-the-real-work-of-scaling/).

## **From Build to Production**

When organizations scale [GenAI beyond pilot projects](https://thenewstack.io/3-reasons-tech-execs-are-slowing-down-genai-projects/), often the first thing to break is not the technology but the readiness to handle it. Without the right talent, organizational structure and clarity on who leads the transition, it becomes difficult to move into production.

Too often, [teams build the model](https://thenewstack.io/how-block-got-12000-employees-using-ai-agents-in-two-months/) and assume it’s ready to deploy. Then someone raises a red flag because of risk concerns, or an evaluation fails and the organization retreats. This is a big enough change — and a large enough benefit — that it cannot be done halfway. Investments are increasing, but not yet deep or uniform enough to solve the full range of transformation needed to productionize GenAI.

Early [GenAI initiatives also struggle with integration](https://thenewstack.io/why-kubernetes-security-is-critical-for-genai-integrity/) and oversight. Because the technology touches every function — data, risk, operations and compliance — the coordination/collaboration required is greater than most teams are prepared for.

GenAI changes the way work gets done. By [automating previously manual processes](https://thenewstack.io/stage-a-productivity-revolution-with-process-automation-ai-and-data-fabric/), roles and responsibilities shift. Successful deployment depends on cross-functional coordination of teams and a willingness to rebuild workflows. For example, companies like Apple, J.P. Morgan, Samsung and many others banned employee use of public chatbots due to concerns for data leaks, showing how quickly risks can emerge.

At the organizational level, many teams overlook reproducibility and the interdependence between systems. Traditional machine learning (ML) pipelines are more linear and easier to control. GenAI systems, in contrast, involve multiple agents, data pipelines and concurrent feedback loops. This makes orchestration and traceability not only helpful but essential for reliability in production.

## **How To Keep GenAI Running Smoothly**

Orchestration requires a shared environment where teams can work independently while maintaining visibility during the entire life cycle. Data scientists, engineers, regulators and compliance officers each need their own independent space to contribute, but the overall system must remain auditable and connected. The goal is not to automate everything but to give teams tools that improve efficiency while maintaining oversight. That kind of environment is essential to orchestrating GenAI at scale.

Of course, technology alone can’t solve organizational challenges. True transformation must happen from the company as a whole. Tools can’t close the talent gap, but they can empower [teams to learn faster](https://thenewstack.io/what-can-incident-teams-learn-from-crisis-management/) and collaborate more effectively. In large companies, knowledge management becomes a best practice when everyone can access and build on what others have already learned.

Given the newness of GenAI, a [human in the loop](https://thenewstack.io/human-on-the-loop-the-new-ai-control-model-that-actually-works/) is the best practice. Human oversight should occur at every stage. When building solutions like the ones that interact with customers, experts need to stress test them and provide live feedback during the build process. This is called “red teaming.”

As you deploy GenAI, it will be important to [understand the ways a large language model (LLM) will react](https://thenewstack.io/what-is-a-large-language-model/). This exposes weaknesses and validates guardrails. Once deployed, systems should be continuously monitored, with frequent checks. Every check should be logged in a system to build confidence and accountability.

## **The Long Game**

The beauty of [GenAI is that powerful](https://thenewstack.io/harness-genais-power-without-sinking-in-technical-debt/), comprehensive models are available to everyone. In the past, to do something new, you had to build a model from scratch. That’s what makes this such a transformational moment.

Sustaining GenAI in production means keeping it aligned with both business goals and ethical standards. Leaders need to define early what responsible GenAI use looks like: how to measure risk, what level of error is acceptable and when human review is required. These conversations can’t happen after deployment. They need to happen now, in collaboration with regulators, industry peers and even customers, to ensure the benefits outweigh the risks. The right feedback loops help to continuously improve GenAI.

## **Leadership and the Path Forward**

The coming years will be transformative. [LLMs are advancing](https://thenewstack.io/openai-aims-to-make-chatgpt-the-operating-system-of-the-future/) at a pace that surprises even the most experienced teams, and two to three years can feel like an eternity. There’s no longer such a thing as a “fast follower.” The leaders in GenAI adoption will be those who combine technical execution with organizational foresight.

Leaders must tackle governance and organizational challenges upfront. Bring compliance and risk officers into the process early, not as reviewers but as partners. It’s critical that engineering and compliance leaders learn together. That’s how trust in GenAI workflows is built.

The train is moving fast. If you don’t start now — learning, iterating and involving your teams — you’ll get left behind.

[YOUTUBE.COM/THENEWSTACK

Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.

SUBSCRIBE](https://youtube.com/thenewstack?sub_confirmation=1)

Group
Created with Sketch.

[![](https://cdn.thenewstack.io/media/2025/11/b0e6ca17-cropped-a5be6046-manish-gupta-ceo-and-co-founder-of-corridor-platforms.jpeg)

Manish Gupta is the CEO and cofounder at Corridor Platforms, where he’s building advanced AI-powered decisioning capabilities that balance automation with regulatory compliance. Gupta is a seasoned leader in AI-driven risk management, decision automation and financial services innovation. Previously chief...

Read more from Manish Gupta](https://thenewstack.io/author/manish-gupta/)