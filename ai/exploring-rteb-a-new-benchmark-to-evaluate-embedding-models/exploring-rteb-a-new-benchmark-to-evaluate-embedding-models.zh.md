随着大型语言模型（LLM）的兴起，我们接触到的基准测试——更不用说其数量和种类之多——也随之激增。考虑到LLM和其他AI系统的不透明性，基准测试已成为比较其性能的标准方法。

它们是评估模型在特定任务上表现的标准测试或数据集。因此，每一次新模型的发布都会带来更新的排行榜结果，[嵌入模型](https://thenewstack.io/the-secret-sauce-for-vector-search-training-embedding-models/)也不例外。

如今，嵌入[驱动着AI应用的搜索层](https://thenewstack.io/combining-the-power-of-text-based-keyword-and-vector-search/)，但选择正确的模型仍然很困难。2022年发布的[大规模文本嵌入基准](https://huggingface.co/spaces/mteb/leaderboard)（MTEB）已成为评估嵌入的标准，但它是一个宽泛的通用基准，涵盖了许多与检索无关的任务。

MTEB也使用公共数据集，虽然这促进了透明度，但可能导致过拟合——模型在评估数据上进行训练。因此，MTEB分数并不总是反映真实的检索准确性。

检索嵌入基准（RTEB），[一个以检索为先的新基准](https://huggingface.co/blog/rteb)，通过专注于真实世界的检索任务，并使用开放和私有数据集来更好地反映新未见数据的真正泛化能力，从而解决了这些局限性。让我们来探索RTEB，它的重点、数据集以及如何使用它。

## 嵌入模型是如何进行基准测试的？

在深入了解RTEB之前，理解基准测试及其重要性至关重要。由于像嵌入模型这样的AI模型是黑盒，评估其质量具有挑战性。基准测试是用于评估这些模型的一组标准化任务。基准测试有助于衡量性能，识别改进领域，并与标准基线、其他模型或过往性能进行比较。

[![基准测试嵌入模型的RTEB结果示例。](https://cdn.thenewstack.io/media/2025/11/5c8a365b-image3.png)](https://cdn.thenewstack.io/media/2025/11/5c8a365b-image3.png)

图1. 基准测试嵌入模型的RTEB结果示例。

构建有效的基准测试并非易事。数据集和任务定义必须反映真实世界的使用情况，以实现有意义的比较。然而，许多基准测试在此方面失败了，它们使用不代表真实用例的数据集，导致结果无法反映实际应用。

另一个主要问题是过拟合。由于基准测试数据集通常是公开的，模型往往会（有意或无意地）在评估数据上进行训练。这导致基准分数虚高，无法反映对未见数据的真实泛化能力。

除了这些问题，基准测试的覆盖范围也至关重要。例如，MTEB是评估嵌入模型准确性最受欢迎的基准测试，涵盖了八个不同的任务类别。尽管这种广泛的覆盖范围对于通用比较很有用，但如果您关心特定用例的性能，它可能会产生误导。实际上，您应该专注于与您预期应用密切相关的基准测试或任务。

## RTEB：一个新的专注于检索的基准测试

尽管嵌入模型可用于许多任务，但其目前最常见的生产用例是检索——驱动搜索、启用检索增强生成（RAG）系统以及将查询与相关文档匹配。

这就是创建[检索嵌入基准](https://huggingface.co/blog/rteb)的原因。RTEB是一个专门针对检索任务的新基准。它以MTEB为基础，提供了一个专注于检索的评估框架，旨在通过以下方式准确测量嵌入模型的真实检索准确性：

*   **混合方法：** RTEB结合了公共数据集（部分与MTEB共享）和私有数据集。这可以防止过拟合——即“应试教育”——确保模型不会在评估数据上进行训练。私有数据集的纳入为衡量对未见数据的泛化能力提供了更准确的指标。
*   **真实世界和多语言覆盖：** RTEB涵盖了金融、医疗保健和代码等关键企业领域，并评估了20多种语言的检索。这些数据集更好地代表了当今企业中的用例。

[![RTEB（检索嵌入基准）概览](https://cdn.thenewstack.io/media/2025/11/b1b06d63-image4.png)](https://cdn.thenewstack.io/media/2025/11/b1b06d63-image4.png)

图2. RTEB概览。

每个数据集任务的准确性，使用在排名10时的[归一化折现累积增益](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)（nDCG@10）进行测量，用于对模型进行排名，为每个任务生成一个排名。这种指标是衡量检索准确性的首选，因为它同时捕捉了相关性和排名质量，与人类感知搜索结果的方式高度一致。

然后，这些排名通过[Borda计数法](https://en.wikipedia.org/wiki/Borda_count)进行组合，以确定最终的排行榜排名。任务分数的平均值不直接使用，因为原始测量结果在不同任务之间存在差异——有些数据集的分数范围较大或较小，这可能会导致平均值不平衡。Borda计数法标准化了这些量纲差异，并强调了相对性能，从而在不同任务之间提供了更公平的比较。

## 在MTEB中导航RTEB

[RTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard?benchmark_name=RTEB%28beta%29)可在Hugging Face的MTEB排行榜的“检索”（Retrieval）部分找到。

[![MTEB中的RTEB](https://cdn.thenewstack.io/media/2025/11/88b2c15e-image2.png)](https://cdn.thenewstack.io/media/2025/11/88b2c15e-image2.png)

图3. MTEB中的RTEB。

除了主要排名，在查看RTEB排行榜时，还有其他几个重要参数需要考虑：

*   **嵌入维度：** 这表示嵌入向量的长度。较小的嵌入提供更快的推理速度和更低的存储成本，而较大的嵌入可以捕获数据中更细微的关系。目标是在语义深度和计算效率之间取得平衡。
*   **最大标记数：** 这是可以转换为单个嵌入的最大标记数。这取决于您的[数据结构和分块策略](https://thenewstack.io/boost-ai-efficiency-data-chunking-meets-document-databases/)。更大的标记限制允许嵌入更长的文本片段。
*   **参数数量**（如果可用）：这表示模型的大小。更多的参数通常与更高的准确性相关，但也会带来更大的延迟和资源需求。专有模型可能不披露确切大小，但通常提供“小型”、“精简版”或“大型”等选项，并提供不同的定价以满足您的需求。

RTEB的子集可用于不同的领域和语言类别，提供对每个模型在特定领域性能的重点洞察。可以在Hugging Face的MTEB的“检索”（Retrieval）部分访问这些子集。

RTEB是评估用于检索的嵌入模型的重要一步。它结合了公共和私有数据集以防止过拟合的混合方法，以及其对真实企业领域和多语言覆盖的关注，使其成为开发人员评估不同嵌入模型的更准确和实用的工具。