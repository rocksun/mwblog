<!--
title: 影响生产RAG流水线5大瓶颈
cover: https://cdn.thenewstack.io/media/2024/01/e00a5aca-volodymyr-hryshchenko-wu9da3c4r28-unsplash-1-1024x683.jpg
-->

这些是可能阻碍RAG流水线在生产LLM环境中性能的主要潜在瓶颈。

> 译自 [5 Bottlenecks Impacting RAG Pipeline Efficiency in Production](https://thenewstack.io/5-bottlenecks-impacting-rag-pipeline-efficiency-in-production/)，作者 Janakiram MSV。

[检索增强生成](https://thenewstack.io/freshen-up-llms-with-retrieval-augmented-generation/)（Retrieval Augmented Generation，RAG）已成为基于大型语言模型的生成式人工智能应用的关键组成部分。其主要目标是通过将通用语言模型与外部信息检索系统集成，增强通用语言模型的能力。这种混合方法旨在解决传统语言模型在处理复杂、知识密集型任务方面的局限性。通过这样做，RAG显著提高了生成响应的事实准确性和可靠性，尤其是在需要精确或最新信息的情况下。

RAG以其增强语言模型知识的能力脱颖而出，使其能够产生更准确、上下文感知和可靠的输出。其应用范围从增强聊天机器人到驱动复杂的数据分析工具，使其成为构建聊天机器人和人工智能代理的重要工具。

但让我们更仔细地看一看针对生产环境的RAG流水线性能的潜在瓶颈。

## 提示模板

LLM中的提示模板在确定模型响应质量方面起着关键作用。一个结构不良的提示可能导致模糊或无关的响应。

每个LLM都有一个定义良好的提示模板，成为模型的通用语言。为了从模型中获得最佳结果，确保提示按照预训练期间使用的格式正确构造非常重要。

例如，下面的模板确保 Llama 2 对提示做出适当的响应。

```
<s>

[INST] 

   <<SYS>>

      {{ system_prompt }}

   <</SYS>>

   {{ user_message }}

[/INST]
```

OpenAI 的 LLMs 使用以下格式：

```
{“role”: “system”, “content”: “system_prompt“},
{“role”: “user”, “content”: “user_message“}
```

## LLM 上下文长度

LLMs 有一个固定的上下文窗口，限制了它们在一个实例中可以考虑的信息量。这取决于预训练期间使用的参数。标准的 GPT-4 模型提供一个上下文窗口为 8,000 个 token。还有一个扩展版本，具有 32,000 个 token 的上下文窗口。此外，OpenAI 推出了 GPT-4 Turbo 模型，其上下文窗口显著扩大至 128,000 个 token。Mistral 具有一个在技术上无限制的上下文窗口，具有 4,000 个滑动窗口上下文大小。Llama 2 具有 4,096 的上下文窗口。

即使一些 LLMs 具有较大的上下文窗口，这并不意味着我们可以跳过 RAG 流水线的某些阶段，一次性传递整个上下文。“上下文 stuffing” 即在提示中嵌入大量上下文数据，已被证明会降低 LLM 的性能。因为模型支持更大的上下文长度，所以在提示中包含整个 PDF 并不是一个好主意。

确保提示和上下文的组合大小在合理上下文长度的限制内，可确保更快、更准确的响应。

## 分块策略

分块是一种用于处理超过模型最大token限制的长文本的技术。由于LLMs一次只能处理固定数量的token，基于上下文窗口，分块涉及将较长的文本划分为更小、可管理的段落或“块”。每个块都按顺序处理，使模型能够通过一次专注于一个段落来处理广泛的数据。

分块是处理存储在文件中的内容（如PDF和TXT）的重要过程，其中大文本被划分为更小、更易管理的段落，以适应嵌入模型输入限制。这些模型将文本块转换为代表它们语义含义的数值向量。这一步骤对于确保每个文本段保持其上下文相关性并准确表示语义内容至关重要。生成的向量然后存储在向量数据库中，允许在语义搜索和内容推荐等应用中进行高效的向量化数据处理。基本上，分块允许以上下文感知的方式高效处理、分析和检索大量文本数据，克服了嵌入模型的限制。

以下列表突显了一些经过验证的嵌入模型的分块策略。

- **基于句子的分块**：这种策略将文本划分为单独的句子，确保每个块捕捉完整的思想或观点；适用于侧重于句子级语义的模型。
- **基于行的分块**：将文本分割成行，通常用于诗歌或脚本，其中每行的结构和韵律对理解至关重要。
- **基于段落的分块**：这种方法按段落对文本进行分块，非常适合保持每个文本块内的主题连贯性和上下文。
- **固定长度令牌分块**：在这里，文本被划分为包含固定数量token的块，平衡模型输入约束与上下文完整性。
- **滑动窗口分块**：涉及使用“滑动窗口”方法创建重叠块，确保相邻块之间的连续性和上下文，特别适用于具有复杂叙述的长文本。

选择适合文本嵌入模型和语言模型的正确分块策略是RAG流水线中最关键的方面。

## 嵌入模型的维度

嵌入模型的维度指的是用于在向量空间中表示文本的维度数量。在自然语言处理（NLP）中，这些模型，比如Word2Vec这样的词嵌入，或者来自BERT的句子嵌入，将单词、短语或句子转换为数值向量。维度通常从几十到几百，甚至几千，决定了模型捕捉语言语义和句法细微差别的粒度和容量。更高维度的嵌入可以捕捉更多信息和细微差别，但也需要更多计算资源，可能导致机器学习模型中的过拟合等问题。

在LLMs中，嵌入模型的维度影响其捕捉语义细微差别的能力。更高的维度通常意味着更好的性能，但代价是增加的计算资源。

以下是一些常见的文本嵌入模型及其维度的列表：

- [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)：这个适用于一般用途且维度较低的模型，维度为384。它专为在英文文本中嵌入句子和段落而设计。
- [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)：这是性能最好的文本嵌入模型之一，维度为1024，适用于嵌入整个句子和段落。
- [OpenAI text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings)：OpenAI最近宣布的嵌入模型具有3072个维度的嵌入大小。这更大的维度使模型能够捕捉更多的语义信息，提高下游任务的准确性。
- [Cohere Embed v3s](https://huggingface.co/Cohere/Cohere-embed-english-v3.0)：Cohere最新的嵌入模型Embed v3，提供了维度为1024或384的版本。模型提供商声称这是最高效和成本效益最高的嵌入模型。

在性能和计算效率（成本）之间取得平衡至关重要。研究集中于找到在最大化性能的同时最小化资源使用的最佳维度。

## 向量数据库中的相似性搜索算法

向量数据库中相似性搜索算法的效率对于RAG中的语义搜索和文档检索等任务至关重要。

优化索引和选择正确的算法显著影响查询处理机制。一些向量数据库允许用户在创建索引时选择度量或算法：

- **余弦相似度**：该指标测量两个向量之间夹角的余弦，提供了一个相似度分数，不考虑它们的大小。在文本检索应用中特别有效，其中向量的方向（指示它们上下文方向相似性的方向）比它们的大小更为重要。
- **HSNW（Hierarchical Navigable Small World Graphs**）：一种基于图的方法，HSNW构建了多层次的可导航小世界图，实现了高效的最近邻搜索。在高维数据空间中，它以高召回率和搜索速度而闻名。
- **用户定义的算法**：也可以实现定制算法，以适应特定用例。这些算法可以利用领域特定的见解来优化搜索和索引策略，为不同数据集和应用的独特要求提供定制方法。

这些方法共同为向量数据库中的搜索准确性和查询效率的提升做出贡献，满足了在各种数据类型和用例中的多样化要求。

## 总结

RAG（Retrieval Augmented Generation）流水线的瓶颈包括提示模板设计、上下文长度限制、分块策略、嵌入模型的维度以及在向量数据库中用于相似性搜索的算法。这些挑战对RAG模型的效果和效率产生影响，从生成准确响应到处理大量文本和保持上下文连贯性。解决这些瓶颈对于提高基于各种LLM的应用性能至关重要，确保它们能够准确解释和生成语言响应。
