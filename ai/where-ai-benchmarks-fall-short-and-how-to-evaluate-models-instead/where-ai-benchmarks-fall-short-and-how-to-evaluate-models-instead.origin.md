# Where AI Benchmarks Fall Short, and How To Evaluate Models Instead
![Featued image for: Where AI Benchmarks Fall Short, and How To Evaluate Models Instead](https://cdn.thenewstack.io/media/2025/02/388ceb49-charlesdeluvio-pjah2ax4uwk-unsplash-1024x683.jpg)
[charlesdeluvio](https://unsplash.com/@charlesdeluvio?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)on
[Unsplash](https://unsplash.com/photos/person-facing-computer-desktop-pjAH2Ax4uWk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash).
Enterprises face an overwhelming array of large language models (LLMs) from which to choose. With new releases like Meta’s Llama 3.3 alongside models like Google’s Gemma and Microsoft’s Phi, the choices have never been so varied. When you scratch below the surface, the choices also become complex.

For businesses looking to leverage LLMs, chatbots, and Agentic systems, the challenge is to evaluate which model aligns with their unique requirements, cutting through the noise of traditional benchmarks and superficial metrics.

**The Flaws of Standard Metrics**
While most evaluation metrics are academically robust, they fail to account for businesses’ nuanced needs. Tools like Perplexity and BLEU (Bilingual Evaluation Understudy) are commonly used in research to measure predictive accuracy or alignment with reference texts. However, their practical utility for enterprises is limited.

Take Perplexity, for instance. Though it is designed to assess a model’s ability to predict sample text, it says little about how well that model can process industry-specific jargon, interpret complex relationships, or provide actionable insights for expert domains. Similarly, developed initially for machine translation, BLEU often rewards models for strict adherence to reference outputs. This can hinder creativity and flexibility in areas where dynamic responses are critical. A chatbot scoring highly on BLEU might rigidly follow pre-defined scripts but fail to handle nuanced customer queries effectively.

Businesses often find themselves disappointed by models that, on paper, should perform well because they excel in these metrics. In reality, the models fall short when applied to real-world challenges.

**The Synthetic Data Problem**
Another significant hurdle stems from the reliance of many open source [models on synthetic training data](https://thenewstack.io/data-modeling-part-2-method-for-time-series-databases/). Synthetic datasets, often generated by widely used [Large Language Models](https://thenewstack.io/why-large-language-models-wont-replace-human-coders/) (LLMs) such as GPT-4, enable faster development cycles but can introduce systemic biases. If the outputs of GPT-4 are unable to grasp the nuances of legal texts, models trained on these outputs will also likely fail to capture these complexities

This reliance on synthetic data creates the risk of feedback loops, where models trained on such datasets mimic patterns and biases from the original generator rather than developing genuine understanding. This issue is exacerbated by using LLM-as-a-judge capabilities, with this accuracy evaluation method reinforcing the biases from the [synthetic data on which many LLM-as-a-judge models](https://thenewstack.io/the-future-of-ai-and-travel-relies-on-synthetic-data/) are trained.

Businesses may mistakenly trust these models based on seemingly strong evaluation scores, only to discover later that they lack the depth needed for specialized tasks. For most enterprises, the [solution lies in fine-tuning models with domain-specific data](https://thenewstack.io/top-5-vector-database-solutions-for-your-ai-project/). Models trained on bespoke datasets can demonstrate vastly improved performance in specialized tasks. However, fine-tuning is resource-intensive and requires [access to high-quality data](https://thenewstack.io/a-look-at-datastaxs-ai-and-push-cache-for-data-access-at-scale/), making it a challenging but necessary step for many organizations.

**Context Sensitivity**
Different models exhibit varying strengths and weaknesses regarding context sensitivity, a crucial factor for business applications. For instance, [Meta’s Llama models](https://thenewstack.io/why-open-source-developers-are-using-llama-metas-ai-model/) are adept at maintaining contextual understanding over prolonged interactions. They are well-suited for use cases requiring extended reasoning, such as legal or medical analysis.

By contrast, [Google’s Gemma models](https://thenewstack.io/gemma-google-takes-on-small-open-models-llama-2-and-mistral/) excel in general-purpose tasks but struggle with applications requiring deep, domain-specific expertise. Similarly, while strong in creative and exploratory tasks, Microsoft’s Phi models can sometimes deviate from strict instructions. This can be an advantage in some contexts but also liability in industries where regulatory compliance is critical. To accurately assess each model’s value, any evaluation framework must account for each model’s nuances and tendencies.

**Developing an Effective Evaluation Framework**
Models should also be evaluated based on scenarios that reflect the organization’s specific use cases and capabilities. For instance, a financial institution might prioritize testing a model’s ability to analyze regulatory filings, ensuring it can handle the dense, structured [language common in compliance](https://thenewstack.io/building-privacy-aware-ai-software-with-vector-databases/) documents. Similarly, a healthcare provider may need to focus on the model’s capacity to interpret clinical notes, often requiring an understanding of medical terminology and patient-specific context. Tailoring evaluation scenarios to [align with these practical applications ensures the chosen model](https://thenewstack.io/ai-alignment-in-practice-what-it-means-and-how-to-get-it/) delivers meaningful results to users with deep domain expertise.

Organizations should avoid over-reliance on synthetic data during testing. Instead, they should adopt a balanced approach, using a mix of real-world and domain-specific datasets. This method helps uncover potential biases that might go unnoticed and ensures the [model can manage the intricacies and variability of actual business](https://thenewstack.io/apis-are-driving-new-business-models-and-unlocking-revenue-streams/) environments. Real-world [data offers a more accurate reflection of a model’s](https://thenewstack.io/data-unions-offer-a-new-model-for-user-data/) challenges in practice, leading to better long-term performance and reliability.

Once deployed, model [performance should be continuously monitored](https://thenewstack.io/linux-deploy-the-netdata-server-performance-monitor/) to identify and address any deviations from expected behavior. Real-world testing during production environments provides invaluable insights into how a model adapts to dynamic conditions. By regularly reviewing outputs and performance metrics, organizations can make iterative improvements and refine their AI systems, ensuring they remain aligned with evolving business needs.

Finally, retrieval-augmented generation (RAG) techniques can be particularly beneficial in business contexts, [improving the reliability of model outputs by integrating external knowledge](https://thenewstack.io/5-ways-ai-improves-knowledge-management/). Evaluating a model’s ability to incorporate this external data into its responses is critical for understanding its practical utility. Strong performance in context evaluation provides reassurance that the model can adapt effectively to complex, information-rich scenarios and deliver outputs that align with the nuances of specific business requirements

2025 will be the year when organizations increasingly look to gain value from the models they’ve invested so heavily in. Trusting the outputs will be accurate and having sufficient expertise will be key here. Businesses must approach model evaluation with care and precision. Publicly available benchmarks may offer a starting point. Still, real-world success requires a more nuanced strategy prioritizing domain-specific needs, diverse data testing, and a deep understanding of context sensitivity.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)