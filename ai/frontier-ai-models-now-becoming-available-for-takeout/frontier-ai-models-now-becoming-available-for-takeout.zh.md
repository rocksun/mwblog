拥有顶级前沿模型的人工智能就像餐馆的食物——你必须去OpenAI、谷歌或微软才能消费它。但这种情况正在慢慢改变。

顶级人工智能公司正在推出其专有前沿大型语言模型的“外卖”版本，用户可以在自己的数据中心运行这些模型。

公司还可以使用定制的推理模型，无论有没有互联网连接。

谷歌正在定制其 Gemini LLM 的版本，公司可以将其安装在数据中心配备 GPU 的服务器上。这与过去 Gemini 仅在 Google Cloud 上可用相比，是一个巨大的变化。

谷歌 Gemini 诞生于 TPU，但现在已移植到公共云中的 GPU，这使得它们可以安装在私有服务器上，Google Cloud 基础设施和解决方案副总裁兼总经理 [Sachin Gupta](https://www.linkedin.com/in/bayareagupta) 告诉 The New Stack。

> “我们在线下和连接场景中都提供此功能。”
> **– Sachin Gupta, Google Cloud**

Gupta 说：“我们采用这些模型，并针对 GPU 进行了优化，然后我们就能够使用 [Nvidia] Blackwell 系统将其引入本地。“我们在线下和连接场景中都提供此功能。”

人工智能初创公司 Cohere 也在定制其前沿大型语言模型的版本，公司可以在自己的硬件上部署这些模型。本月早些时候，该公司筹集了 5 亿美元资金，估值为 55 亿美元。

Cohere 工程副总裁 [Autumn Moulder](https://www.linkedin.com/in/autumn-moulder/) 告诉 The New Stack，在越来越大的模型上消耗计算资源实际上并不能实现业务价值。

Cohere 为其本地模型提供硬件灵活性。

Moulder 说：“这是我们在构建整个独立系统时重点关注的事情之一。”

许多公司出于成本和安全原因，正寻求放弃云并将工作负载迁回内部。这些定制模型经过微调，可以适应组织的特定应用，并在受限的环境中运行。

## 硬件

在内部硬件上运行专有人工智能模型的概念是一个相当年轻的现象。

OpenAI 和 Anthropic 尚未具备这些能力，并且没有回复置评请求。

从历史上看，流行的前沿模型受制于专有硬件。Google Gemini 使用该公司的 TPU，OpenAI 使用 Nvidia GPU，Anthropic 则在云中使用定制芯片。

想要使用现场人工智能的公司转而使用 DeepSeek、Mistral 和 Llama 等开放模型，这些模型可以下载并在各种系统上支持。这些模型易于定制且免费。

谷歌将 Gemini 从 TPU 移植到 GPU 将扩大 LLM 的覆盖范围，使其比 OpenAI 更具优势，因为 OpenAI 的模型仅限于云服务器。

Cohere 的人工智能计划最初始于谷歌的 TPU，但现在它正在将其模型移植到其他人工智能硬件。

> “我们允许硅片可选性，并与多个提供商合作进行推理。”
> **– Autumn Moulder, Cohere**

Moulder 说：“我们允许硅片可选性，并与多个提供商合作进行推理。我们与 Nvidia 和 AMD 建立了合作伙伴关系。”

可以肯定的是，这不适用于普通用户。定制和部署这些黑盒人工智能模型可能仍然需要数百万美元。

谷歌和 Cohere 押注于易于部署，以此来吸引客户。他们的模型是预先打包的应用程序，不需要客户从头开始开发所有内容。

## 为什么“外卖”会流行？

J. Gold Associates 的首席分析师 [Jack Gold](https://www.linkedin.com/in/jckgld/) 表示，怀疑人工智能的时代已经结束，公司正自信地将人工智能转移到自己数据中心的生产中。

Gold 说，一些公司希望将其 IT 放在内部，并且出于安全和合规原因，更喜欢在自己的系统中使用人工智能模型。

谷歌的 Gupta 举了 ServiceNow 的例子，该公司为 Gemini 开发了一个人工智能代理。

Gupta 说：“该代理处理客户数据……这些数据必须保留在本地。并且存在一些与之相关的合规性要求。”

Forrester Research 副总裁兼首席分析师 [Charlie Dai](https://www.forrester.com/analyst-bio/charlie-dai/BIO5344) 说，数据主权要求还要求公司将客户数据保存在特定地理位置的私有服务器上。

Cohere 的 Moulder 说，为了在人工智能领域投入生产，尤其是在关键基础设施行业，公司需要控制数据的去向。

Moulder 说：“你需要能够知道你的数据去向。你不能完全依赖位于其他人控制的服务器上的第三方数据。这会让你面临太多的监管问题。”

## 模型

内部模型通常是大型语言模型的受限版本，这些模型主要用于推理。两者都使用 Kubernetes 作为计算层。

高管们表示，开发人员需要了解私有计算环境中的效率，尤其是在启用人工智能的系统投入生产时。

对人工智能系统进行压力测试也很重要。

Gupta 说，客户在 Google Cloud 中尝试了 Gemini，他们希望在自己的数据中心中使用 100 万个上下文窗口和多模态功能等功能。

Gupta 说：“我们能够使用我们完全托管的本地云服务 Google Distributed Cloud……来简单地为客户启用 Gemini Pro、Gemini Flash API。他们不必担心堆栈中的任何事情，他们只会获得一个暴露给他们的 API。”

Moulder 说，Cohere 通过识别客户所使用的目标架构来打包和交付模型。

Moulder 说：“我们只是确保他们可以使用它。但是整个系统几乎是相同的，只是为每个不同的架构编译的。”

Cohere 确保不依赖于特定的托管提供商。

Moulder 说：“然后我们允许对该环境进行大量配置。这使我们可以定位虚拟私有云、本地环境。这对我们来说都是一样的。”

## 可定制的模型

客户可以使用自己的数据定制本地模型。

公开的 API 允许客户开始集成代理或机器学习工作流程。

Moulder 说：“我们还发现他们中的一些人确实有非常具体的需求，我们可以定制一个垂直集成的模型，并且与某些合作伙伴在特定主题领域中运行良好。”

谷歌与 Nvidia 合作提供了一个机密计算堆栈，可以保护数据。

Nvidia 企业人工智能软件副总裁 [Justin Boitano](https://www.linkedin.com/in/justinboitano/) 告诉 The New Stack：“该合作伙伴关系使我们能够保护在机密虚拟机中运行的模型的 IP，以便在模型使用时，它可以在加密的虚拟机中得到充分保护。”