“我们的AI原型很出色，但将其投入生产需要六个月——如果我们足够幸运的话。”听起来耳熟吗？

如今，团队能在几天（甚至几小时）内发布原型，但要将其投入生产并获得投资回报，可能需要数月之久。与此同时，竞争对手每周甚至每天都在推出AI驱动的功能，而您的团队却因未知的风险和为静态部署应用而设计的过时流程而陷入停滞。

如果问题不在于AI模型、人才或预算，那么根本原因是什么？企业在速度与信任的交汇点上陷入决策和流程僵局。如果他们行动过快，担心会危及安全和合规政策；但如果行动过慢，又面临变得无关紧要的风险。

那些摆脱这一困境的组织已不再强行将新工作负载通过过时的流程或部署在过时的基础上。相反，他们正在构建和采用专为当今AI发展方向的复杂性而设计的系统，而非五年前静态应用程序的限制。

## **当复合AI遇到碎片化基础设施**

早期的AI驱动应用程序与今天的相比要简单得多。它们通常只使用一个AI模型来完成在单一环境中运行的特定任务。现在情况不再如此。今天的大多数AI工作负载都由[多个代理协调工作流](https://thenewstack.io/4-reasons-agentic-ai-is-failing/)、将大型语言模型（LLM）链接起来完成复杂任务，并跨分布式系统运行。每一个新层都会使复杂性倍增。驱动现代AI堆栈的系统演进速度比支持它们的基础设施更快。

这就是问题开始出现的地方。

这是一个真实的例子。您的数据科学和软件开发团队上周共同构建的出色原型包含了他们选择的50多个开源软件包。[每个软件包都有自己的依赖项](https://thenewstack.io/the-challenges-of-securing-the-open-source-supply-chain/)，每个新依赖项都伴随着潜在的漏洞和许可影响。大多数组织都难以追踪哪些软件包存在漏洞、哪些正在使用、哪些没有更新，或者哪些在关键任务生产环境中悄悄引入了延迟。随着AI工作负载变得越来越复杂，每一个新的AI模型、AI代理和集成都会以比治理更快的速度增加安全风险。

即使安全风险得到了管理，环境本身也会成为障碍。一个在数据科学家笔记本中完美运行的原型可能会[在暂存环境中神秘失败](https://thenewstack.io/ai-code-doesnt-survive-in-production-heres-why/)，或者在生产中表现出不可预测的行为。罪魁祸首通常是“环境漂移”。这是指依赖包版本不同、配置发生变化或编排层行为异常。传统的软件管道建立在稳定和可见的组件之上；然而，在AI工作负载方面，这些边界消失了。团队发现自己调试的不是一个环境，而是许多环境，并且对运行什么、在哪里运行以及为什么运行知之甚少。

几个月后，当出现问题时，故障排除问题会进一步复杂化。一个漏洞浮出水面。AI模型输出开始表现异常。但那一刻是哪个AI模型或代理的哪个版本在运行？事件发生时的AI物料清单（[AIBOM](https://csrc.nist.gov/presentations/2024/securing-ai-ecosystems-the-critical-role-of-aibom)）是什么？使用了哪些数据集？如果没有可靠的血缘关系，团队就无法追溯部署、安全回滚或从失败中学习。在复合AI时代，缺失来源信息不再是小麻烦，而是安全风险的暴露。

## **碎片化工具链的隐性成本**

业务影响不容忽视。[Gartner](https://www.gartner.com/en/newsroom/press-releases/2025-03-25-gartner-survey-reveals-84-percent-of-cmos-report-high-levels-of-strategic-dysfunction#:~:text=Marketing%20Leaders%20Challenged%20By%20Unclear,and%20execute%20effective%20marketing%20strategies.%E2%80%9D)报告称，84%的技术高管在2025年进行了计划外的战略调整，其中许多是由于AI项目在基础设施限制上碰壁。这并非因为AI模型失败，而是因为围绕它们的系统无法在企业规模下维持生产。

但AI将继续扩大规模，[72%的组织报告其团队每周使用AI工具](https://ai.wharton.upenn.edu/wp-content/uploads/2024/11/AI-Report_Full-Report.pdf)。这种高水平的使用可能带来安全风险，尤其是当团队在没有清晰了解AI如何使用或应用的情况下快速行动时。事实上，[只有13%](https://www.cyera.com/research-labs/2025-state-of-ai-data-security-report)的企业对AI在组织中的实际使用情况有清晰的可见性，而且每80个生成式AI（GenAI）提示中就有1个会暴露敏感数据。此外，根据[Check Point Research](https://engage.checkpoint.com/2025-ai-security-report)的数据，所有提示中有7.5%包含敏感或私人信息。

目前，每个新的AI模型、AI代理和AI驱动的工作流都需要从头开始重建信任。但随着领导层要求更快的AI成果，合规团队要求更严格的控制。不幸的是，现有工具链使得两者几乎不可能实现。这导致要么是有前景的原型从未发布，要么是危险的规避措施完全绕过治理。

## **真正改变模式的因素**

那些正在摆脱AI僵局的组织并非修补旧流程，而是在重建基础，从第一天起就将AI视为一个为复合复杂性而构建的统一基础。

这些组织不把治理和安全视为事后考虑。在开始构建之前，每个依赖项都必须可追溯，每个AI模型都经过审查，每个环境都可复现。当信任成为首要考虑时，速度随之而来。安全团队对项目风险有充分的可见性，平台工程团队从第一天就准备就绪，数据科学团队大规模运作，开发团队高效交付成果。在这些组织中，所有团队都不必在速度和信任之间做出选择。

团队还希望构建能够应对复杂性的一致应用程序和服务。“环境漂移”在这种规模下不仅仅是一种烦恼，而是一个阻碍因素，它迫使团队为了快速发布或根本发布而采取非常规手段。对于复合AI来说，“基本相同”意味着缺乏规模或潜在的生产中断，而创建一致性成为团队保持速度的唯一途径。

最后，组织能够建立整个生态系统和工作流的可见性。为了增加信心，团队需要精确地知道什么正在运行、在哪里运行以及在任何给定时刻哪些依赖项构成了他们的AIBOM。当问题出现时，[精确地知道安全问题何时出现以及受到什么影响](https://thenewstack.io/llms-broke-the-sre-runbook-now-what/)，而不是猜测数百个依赖项中的哪一个导致了级联风险，将极大地减少修复事件所需的时间。

应用程序堆栈中这些不起眼的部分，包括依赖管理、按需可复现环境以及自动化安全和治理，将使复合AI工作负载能够扩展，而不会在其自身复杂性下崩溃——同时也不会被卡在速度和信任的交汇点。这就是那些演示AI驱动原型和那些构建、保护、部署和监控AI原生应用程序及工作负载的组织之间的区别，后者能够为他们的团队和客户释放效率和创新。

## **未来的选择**

AI成功的障碍不是能力，而是基础设施和流程的成熟度。那些领先的组织是由选择投资于现代基础的领导者所带领的，在这些基础中，信任和速度共存，内置的安全和治理能够加速而非阻碍。碎片化的工具链和环境迫使你在这两者之间做出选择，但现代、统一的基础设施和[现代AI流程](https://www.anaconda.com/press/anaconda-launches-comprehensive-enterprise-ai-development-suite)让你两者兼得。今天，消除瓶颈正在创造你的竞争优势。这不仅仅是一个技术抱负；它是当今市场中竞争和生存的业务 imperative。