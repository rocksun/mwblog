在过去几年中，AutoGPT、BabyAGI 等智能体AI系统通过提示，证明了大型[语言模型（LLM）](https://thenewstack.io/introduction-to-llms/)能够以极少的人工干预完成广泛的任务，包括规划、推理和执行多步骤流程。它们表明，LLM不仅能回答单个问题，还具备承担更全面的自主解决问题能力。

其吸引力既迅速又强烈。[智能体系统](https://thenewstack.io/ai-agents-vs-agentic-ai-a-kubernetes-developers-guide/)承诺降低人工监督水平、根据不断变化的目标进行自适应行为，以及以史无前例的方式编排工具。智能体系统浏览网页、编写代码、修改自身计划，并以有限的指导迭代地朝着目标努力的病毒式演示，暗示了开发复杂系统可能存在一种全新的方式。

随着组织开始考虑将智能体系统部署到生产环境的潜在用例，一个共同的主题浮现出来。那些使[智能体系统在演示中如此引人注目](https://thenewstack.io/system-two-ai-the-dawn-of-reasoning-agents-in-business/)的相同特性——开放式推理循环、自适应决策和涌现行为——在实际生产约束（如成本控制、延迟要求、可靠性预期和监管义务）下评估时，却变成了重大的缺陷。

原型成功与智能体LLM系统在生产部署中失败之间的差距，通常归结为未能考虑到控制、确定性以及系统之间明确边界需求的架构设计决策。

## **自主性的幻觉**

尽管智能体系统被广泛描述为自主的，但智能体关于下一步行动的决策更多地由提供给智能体的输入（提示）、与工具的接口以及智能体所处的更广泛上下文决定，而非真正的意识。

智能体系统不同于传统软件系统（其控制流由代码明确定义），它们使用概率推理来确定行动顺序。因此，每次运行智能体时，完全相同的输入会产生不同的结果，原因在于模型输出的随机性以及/或智能体运行上下文中的微小差异。这种可变性并非错误，而是基于LLM推理的自然组成部分。

在少数情况下，可变性会带来益处。例如，可变性可以帮助智能体更好地管理模糊性、解释非结构化数据并应对突发事件。然而，在生产环境中，确定性对于成功测试、调试和验证系统行为是必需的。然而，难以可靠地确定在完全相同环境中运行的两个独立系统实例是否会产生相同的结果。

> 智能体内存的分布式特性使得追踪推理路径和解释智能体为何选择特定响应变得困难。

智能体复杂、隐藏的依赖关系常常是关于系统自主性误解的基础。业务实践、安全约束以及其他管理智能体运作的运行策略[几乎总是嵌入在发送给智能体的提示中](https://arxiv.org/abs/2510.11588)，而不是写入智能体的明确逻辑中。这些策略表面上的稳定性具有误导性。它们极易受到提示语言、与智能体交互工具生成的响应格式以及智能体生成响应时所提供上下文量微小变化的影响。

智能体状态管理也增加了这些复杂性。智能体的记忆分布在其接收提示的历史、与工具交互的输出以及生成响应的上下文之中。智能体记忆的分布式特性使得追踪推理路径和解释智能体为何选择特定响应变得困难。当问题发生时，团队常常难以确定问题是由提示、模型、工具用户界面中的缺陷还是三者的组合造成的。

因此，尽管智能体系统经常被视为能够稳健地提供自主行为，但实际上，大多数只是提示和工具的松散耦合组合，其行为源于系统组件之间的概率性交互，而非任何有意设计。如果没有严格的架构纪律，智能体系统表面上的自主性会增加漏洞，而非降低复杂性。

## **智能体系统的架构故障模式**

失控执行是最常遇到的问题之一。当智能体被赋予开放式目标（例如没有特定的终止标准）时，它们通常会一直运行，直到智能体的推理过程确定有其他事情可做。智能体不会知道任务是否完成或何时达到了瓶颈。如果没有明确的迭代、时间或资源限制来告知它们何时停止，即使对目标的单一错误解释也可能导致潜在的无限执行。

成本放大是一个密切相关的问题。每一步推理、每一次模型调用以及每一次工具调用都会消耗资源。如果智能体执行重复循环任务并探索所有潜在路径（而不仅仅是那些看起来合理的路径），系统的总运营成本将呈指数级增长。在测试期间看起来便宜的操作在生产环境中可能非常昂贵。

相同的请求可能一天成功，第二天失败，因为长期运行的智能体可能表现出非确定性的任务完成行为。即使系统配置没有修改，结果也可能不同。这使得难以可靠地预测某个特定请求是否会成功。

> 隐性策略违规发生在约束被暗示但未明确定义执行方式时。

隐性策略违规比其他这些行为稍微不那么明显但可能更具破坏性。隐性策略违规发生在约束被暗示但未明确定义执行方式时。因此，智能体可以轻易违反公司[政策或法规合规规则](https://thenewstack.io/how-to-create-an-effective-ai-usage-policy/)，而不会产生任何错误。[隐性策略违规](https://doi.org/10.1016/j.automatica.2022.110558)通常只有当有人发现问题时才会浮出水面，而当他们发现时，损害可能已经造成。

长期运行的智能体也常表现出状态漂移。[状态漂移](https://arxiv.org/abs/2505.16894)指的是随着时间的推移上下文的累积以及早期请求或先前指令重要性的最终丧失。最终，智能体的行动可能与最初的意图大相径庭，或者智能体可能在与用户的多次先前对话中表现不一致。

这些行为并非孤立事件。它们是设计决策的直接结果，这些决策在没有足够保障措施的情况下赋予概率模型过多的权限。

## **控制优先的架构视角**

为了使智能体系统在生产环境中有效运行，它们需要以不同的方式设计。第一步是从专注于最大化智能体自主性转向创建具有最大控制、可观测性和有界执行的系统。

智能体必须具有明确的执行边界，以便它们了解它们的时间、迭代和资源限制。这些边界需要通过更大的系统来强制执行；智能体不能根据自身的表现来决定这些边界。

在流程继续之前需要某种形式的验证时，您还需要设置检查点。这些检查点可以包括确定性规则、二级验证模型或人工审查，具体取决于所采取行动的风险水平。

> 人工战略监督并不会削弱自动化；它能增强弹性和问责制。

除了限制智能体的能力之外，将系统的确定性部分（如控制流和合规性强制执行）与LLM的概率性特性（LLM最适合解释/判断性任务）分离，对于开发稳健系统也至关重要。

一如既往，让人类参与其中至关重要。人工战略监督并不会削弱自动化；它能增强弹性和问责制。通过将人工审查适当地整合到智能体输出中，您可以限制昂贵错误的财务影响并随着时间推移提高系统性能。

控制优先的方法并不会丧失灵活性；它只是将这种灵活性引导到更富有成效的路径。

## **将智能体重新定义为编排系统**

未来，可持续的系统设计将依靠把智能体视为人类工程师编排系统中的协作方。工程师将负责定义工作流、建立系统边界，并明确智能体在何处进行额外推理是合理的。

虽然智能体在需要解释、综合或非结构化输入的任务上表现良好，但在执行规则、控制执行流或确保一致性方面表现不佳。事实上，简单管道，甚至单个LLM调用，[通常比完全智能体配置更可靠、成本更低](https://arxiv.org/abs/2508.02694)。

这一趋势正在推动[智能体开发框架](https://thenewstack.io/a-developers-guide-to-the-autogen-ai-agent-framework/)的演变，这些框架越来越强调结构化工作流、明确角色和基于图的执行。

## **对企业系统的影响**

这里吸取的教训非常清楚：没有结构的智能无法扩展。这强调了规模化下的合规性、可审计性、成本可预测性和可靠性至关重要。

合规性是强制性的。它应该通过规则和代码验证（从而通过系统）明确实施，而不是隐藏在动态提示中。

[可审计性依赖于透明度](https://doi.org/10.48550/arXiv.2512.11984)。团队必须能够看到系统做了什么、为什么做出这些决策、使用了哪些输入以及达到输出所采取的步骤。缺乏可审计性使得企业难以支持或修复自动化决策过程。

成本可预测性同样重要。企业希望系统能够实施预算控制并模拟最坏情况以预测潜在财务风险。企业无法承担无限制自主性带来的财务风险。

规模化下的可靠系统需要一种预期故障的设计方法。问题在测试期间似乎很少出现，但在生产环境中会迅速显现。[智能体系统以多种方式失败](https://doi.org/10.48550/arXiv.2512.07094)，这使得监控、熔断器和回退机制对于可靠运行至关重要。

## **区分演示与生产的运行信号**

对自主系统运行信号的审查提供了一种切实可行的方法来确定其是否准备好投入生产。处于演示模式的系统侧重于输出。生产系统则呈现行为。

第一个运行信号是执行透明度，它表明系统是否已达到成熟的执行透明度水平。生产系统采取的每一个行动都可以与输入和决策点相关联，因此团队可以识别输入了哪些提示、使用了哪些工具以及系统为何选择特定路径。没有透明度，故障就变成了毫无根据的猜测。

> 如果没有明确的解释说明为何延迟、成本和/或执行时间在每次运行时都不同，那么该系统尚未达到生产就绪状态。

生产系统不需要始终如一的结果（有界方差）；然而，生产系统确实需要有界可变性。如果没有明确的解释说明为何延迟、成本和/或执行时间在每次运行时都不同，那么该系统尚未达到生产就绪状态。

还需要故障可见性。当生产系统出现问题时，它应该快速且干净地失败。性能下降、隐性故障或其他形式的故障，或无限重试循环增加了发现问题所需的时间并提高了解决问题的成本。

了解每个任务或每个用户的成本也至关重要。如果系统无法解释[为什么某个请求的成本比其他请求高得多](https://doi.org/10.48550/arXiv.2511.11761)，那么将难以负责任地发展业务。

无法回答关于它们采取了哪些行动、为何采取这些行动、每项行动花费多少以及每项行动如何/为何失败的系统，无论输出质量如何，都尚未超越演示阶段。

## **结论**

当智能体LLM系统在生产环境中不能自主工作时，通常不是模型能力的体现，而是支持智能体自主性所需的架构被忽视的结果。缺乏架构约束会增加失败的可能性。

当成功的[系统将智能体作为计划架构中的强大子系统](https://thenewstack.io/4-data-architecture-decisions-that-make-or-break-agentic-systems/)时，它们会为智能体创建边界、验证其所有决策、限制成本并提供对其所有行动的可见性。在这些条件下，智能体在保持可靠性的同时为系统增加价值。智能体AI的未来将不取决于系统能实现多少自主性，而取决于系统自主性如何被适当控制和约束。在生产环境中，架构依然至关重要。

YOUTUBE.COM/THENEWSTACK

技术发展迅速，不要错过任何一集。订阅我们的YouTube频道，收看我们所有的播客、访谈、演示等。

订阅

![](https://cdn.thenewstack.io/media/2026/01/f229d3d7-cropped-14b94b36-ibrahim-kamal.jpeg)

Ibrahim Kamal 是 Oracle 的高级应用工程师，在 Oracle Recruiting Cloud 中构建大规模 AI 系统。他负责设计和部署由LLM驱动的企业功能，包括AI智能体和智能候选人沟通工作流。他的重点是提供生产级的AI集成，这些集成……

阅读更多 Ibrahim Kamal 的文章