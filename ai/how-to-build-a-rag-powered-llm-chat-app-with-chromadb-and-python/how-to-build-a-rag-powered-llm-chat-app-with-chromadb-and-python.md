
<!--
title: ä½¿ç”¨ChromaDBå’ŒPythonæ„å»ºRAGé©±åŠ¨çš„LLMèŠå¤©åº”ç”¨
cover: https://cdn.thenewstack.io/media/2024/03/dfa91eb2-rag-llm-chat-app-python.jpg
-->

åˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) å’Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„å¼ºå¤§åŠŸèƒ½æ¥åˆ›å»ºç”Ÿæˆå¼ AI åº”ç”¨ç¨‹åºã€‚

> è¯‘è‡ª [How to Build a RAG-Powered LLM Chat App with ChromaDB and Python](https://thenewstack.io/how-to-build-a-rag-powered-llm-chat-app-with-chromadb-and-python/)ï¼Œä½œè€… Oladimeji Sowoleã€‚

ç”Ÿæˆå¼ AI æ­£ä»¥å…¶åˆ›å»ºä¸Šä¸‹æ–‡ç›¸å…³å†…å®¹çš„èƒ½åŠ›å½»åº•æ”¹å˜æŠ€æœ¯ï¼Œå¼€åˆ›äº† AI å¯èƒ½æ€§çš„æ–°æ—¶ä»£ã€‚å…¶æ ¸å¿ƒæ˜¯ [æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG)](https://thenewstack.io/retrieval-augmented-generation-for-llms/)ï¼Œå®ƒå°†ä¿¡æ¯æ£€ç´¢ä¸ [å¤§å‹è¯­è¨€æ¨¡å‹ (LLM)](https://www.cloudflare.com/en-gb/learning/ai/what-is-large-language-model/) ç›¸ç»“åˆï¼Œä»å¤–éƒ¨æ–‡æ¡£ä¸­ç”Ÿæˆæ™ºèƒ½ã€æ˜æ™ºçš„å“åº”ã€‚

æœ¬æ•™ç¨‹è§£é‡Šäº†å¦‚ä½•ä½¿ç”¨ [ChromaDB](https://www.trychroma.com/) æ„å»º RAG é©±åŠ¨çš„ LLM åº”ç”¨ç¨‹åºï¼ŒChromaDB æ˜¯ä¸€æ¬¾ä»¥ AI ä¸ºæœ¬ã€å¼€æºçš„åµŒå…¥å¼æ•°æ®åº“ï¼Œä»¥å…¶é«˜æ•ˆå¤„ç†å¤§å‹æ•°æ®é›†è€Œé—»åã€‚æˆ‘å°†æŒ‡å¯¼ä½ å®Œæˆæ¯ä¸€æ­¥ï¼Œå±•ç¤º RAG åœ¨åˆ›å»ºé«˜çº§ LLM åº”ç”¨ç¨‹åºä¸­çš„å®é™…é€‚ç”¨æ€§ã€‚

## ä½ éœ€è¦ä»€ä¹ˆ

è¦å¼€å§‹æ„å»ºä½ çš„ LLM åº”ç”¨ç¨‹åºï¼Œä½ éœ€è¦ [Python](https://thenewstack.io/python/)ï¼ˆå¯ä» [Python å®˜æ–¹ç½‘ç«™](https://www.python.org/downloads/) ä¸‹è½½ï¼‰ã€OpenAI API å¯†é’¥ï¼ˆå¯åœ¨ [OpenAI å¹³å°](https://platform.openai.com/signup) ä¸Šè·å¾—ï¼‰ä»¥åŠå¯¹ Python å’Œ Web API çš„åŸºæœ¬äº†è§£ã€‚è¿™äº›æŠ€æœ¯å°†å¸®åŠ©ç¡®ä¿åœ¨éµå¾ªæœ¬æ•™ç¨‹å’Œå¼€å‘ç”Ÿæˆå¼ AI é©±åŠ¨çš„èŠå¤©åº”ç”¨ç¨‹åºæ—¶è·å¾—é¡ºç•…çš„ä½“éªŒã€‚

## è®¾ç½®é¡¹ç›®

ä¸‹è½½å®Œæ‰€éœ€çš„åº”ç”¨ç¨‹åºå’ŒæŠ€æœ¯åï¼Œå¼€å§‹è®¾ç½®ä½ çš„é¡¹ç›®ç¯å¢ƒã€‚

**1. åˆ›å»ºå¹¶å¯¼èˆªåˆ°é¡¹ç›®ç›®å½•ï¼š** åœ¨ä½ çš„ç»ˆç«¯ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªæ–°ç›®å½•ï¼š

```
mkdir rag_lmm_application
```

å°†ä½ çš„å·¥ä½œç›®å½•æ›´æ”¹ä¸ºé¡¹ç›®æ–‡ä»¶å¤¹ï¼š

```
cd rag_lmm_application
```

**2. åˆ›å»ºä½ çš„è™šæ‹Ÿç¯å¢ƒï¼š** è¿™æ˜¯ä¾èµ–é¡¹ç®¡ç†çš„å…³é”®æ­¥éª¤ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼š

```
python -m venv venv
```

å¹¶æ¿€æ´»å®ƒã€‚

å¯¹äº Mac æˆ– Linuxï¼š

```
source venv/bin/activate
```

å¯¹äº Windowsï¼š

```
Venv\Scripts\activate
```

**3. å®‰è£…å¿…éœ€çš„åŒ…ï¼š** ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸ºä½ çš„é¡¹ç›®å®‰è£…å¿…è¦çš„åº“ï¼š

```
pip install -r requirements.txt
```

requirements.txt å†…å®¹ä¸ºï¼š

```
openai
langchain
docx2txt
pypdf
streamlit
chromadb
tiktoken
```

å®Œæˆè¿™äº›æ­¥éª¤åï¼Œä½ çš„ç¯å¢ƒå·²å‡†å¤‡å°±ç»ªï¼Œä½ å¯ä»¥å¼€å§‹ä½¿ç”¨ ChromaDB æ„å»ºæœ€å…ˆè¿›çš„ RAG èŠå¤©åº”ç”¨ç¨‹åºã€‚

## åŠ è½½å’Œå¤„ç†æ–‡æ¡£

æ­¤ LLM åº”ç”¨ç¨‹åºä½¿ç”¨ [LangChain](https://thenewstack.io/how-to-build-a-qa-llm-application-with-langchain-and-gemini/) åŠ è½½å™¨ç†Ÿç»ƒåœ°å¤„ç†å„ç§æ–‡æ¡£æ ¼å¼ï¼ŒåŒ…æ‹¬ PDFã€DOCX å’Œ TXTã€‚è¿™å¯¹äºå¯ç”¨å¤–éƒ¨æ•°æ®å¯è®¿é—®æ€§ã€ç¡®ä¿é«˜æ•ˆçš„æ•°æ®å¤„ç†å’Œç»´æŠ¤åç»­é˜¶æ®µçš„ç»Ÿä¸€æ•°æ®å‡†å¤‡è‡³å…³é‡è¦ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µè¯´æ˜äº†è¯¥è¿‡ç¨‹ï¼š

```python
# loading PDF, DOCX and TXT files as LangChain Documents 
def load_document(file): 
	import os 
	name, extension = os.path.splitext(file) 
 
	if extension == '.pdf': 
    	from langchain.document_loaders import PyPDFLoader 
    	print(f'Loading {file}') 
    	loader = PyPDFLoader(file) 
	elif extension == '.docx': 
    	from langchain.document_loaders import Docx2txtLoader 
    	print(f'Loading {file}') 
    	loader = Docx2txtLoader(file) 
	elif extension == '.txt': 
    	from langchain.document_loaders import TextLoader 
    	loader = TextLoader(file) 
	else: 
    	print('Document format is not supported!') 
    	return None 
 
	data = loader.load() 
	return data
```

æ•°æ®åˆ†å—â€”â€”å°†ä¸åŒä¿¡æ¯åˆ†ç»„åˆ°æ›´æ˜“äºç®¡ç†æˆ–æ›´æœ‰æ„ä¹‰çš„å—ä¸­â€”â€”ç®€åŒ–äº†å¤„ç†å’ŒåµŒå…¥ï¼Œå¹¶å®ç°äº†é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ä¿ç•™å’Œä¿¡æ¯æ£€ç´¢ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¼”ç¤ºäº†è¿™ä¸€é‡è¦è¿‡ç¨‹ï¼š

```python
# splitting data in chunks 
def chunk_data(data, chunk_size=256, chunk_overlap=20): 
	from langchain.text_splitter import RecursiveCharacterTextSplitter 
	text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap) 
	chunks = text_splitter.split_documents(data) 
	return chunks 
```

## ä½¿ç”¨ OpenAI å’Œ ChromaDB åˆ›å»ºåµŒå…¥

åœ¨æ­¤åº”ç”¨ç¨‹åºä¸­ï¼ŒRAG ä½¿ç”¨ [OpenAI](https://thenewstack.io/why-microsoft-has-to-save-openai/) è¯­è¨€æ¨¡å‹åˆ›å»ºåµŒå…¥â€”â€”æ–‡æœ¬çš„åŸºæœ¬å‘é‡è¡¨ç¤ºï¼Œä»¥ä¾¿é«˜æ•ˆåœ°ç†è§£æ•°æ®ã€‚è¿™äº›åµŒå…¥å¯¹äº RAG çš„æ£€ç´¢è‡³å…³é‡è¦ï¼Œå…è®¸è®¿é—®ç›¸å…³å¤–éƒ¨æ•°æ®ã€‚å®ƒä»¬æœ‰æ•ˆåœ°å­˜å‚¨åœ¨ ChromaDB ä¸­ï¼Œå¯ä»¥å¿«é€Ÿæ£€ç´¢ä¿¡æ¯ï¼Œå¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤ºã€‚æ­¤è¿‡ç¨‹æå¤§åœ°å¢å¼ºäº†åº”ç”¨ç¨‹åºçš„ AI èƒ½åŠ›ã€‚

```python
# create embeddings using OpenAIEmbeddings() and save them in a Chroma vector store 
def create_embeddings(chunks): 
	embeddings = OpenAIEmbeddings() 
	vector_store = Chroma.from_documents(chunks, embeddings) 
 
	# if you want to use a specific directory for chromadb 
	# vector_store = Chroma.from_documents(chunks, embeddings, persist_directory='./mychroma_db') 
	return vector_store 
```

## ä½¿ç”¨ Streamlit æ„å»ºèŠå¤©ç•Œé¢

[Streamlit](https://streamlit.io/) æ˜¯ä¸€æ¬¾åº”ç”¨ç¨‹åºï¼Œå®ƒå¯ä»¥åœ¨[å‡ åˆ†é’Ÿå†…å°†æ•°æ®è„šæœ¬è½¬æ¢ä¸ºå¯å…±äº«çš„ Web åº”ç”¨ç¨‹åº](https://thenewstack.io/streamlit-an-app-builder-for-the-data-science-team/)ã€‚æ­¤ RAG LLM åº”ç”¨ç¨‹åºå°†ç”¨æˆ·è¾“å…¥é“¾æ¥åˆ°åç«¯å¤„ç†ã€‚é€šè¿‡ Streamlit çš„åˆå§‹åŒ–å’Œå¸ƒå±€è®¾è®¡ï¼Œç”¨æˆ·å¯ä»¥ä¸Šä¼ æ–‡æ¡£å’Œç®¡ç†æ•°æ®ã€‚åç«¯å¤„ç†è¿™äº›è¾“å…¥ï¼Œå¹¶ç›´æ¥åœ¨ Streamlit ç•Œé¢ä¸­è¿”å›å“åº”ï¼Œæ˜¾ç¤ºå‰ç«¯å’Œåç«¯æ“ä½œçš„æ— ç¼é›†æˆã€‚

ä»¥ä¸‹ä»£ç æ˜¾ç¤ºäº†å¦‚ä½•åœ¨ Streamlit ä¸­åˆ›å»ºæ–‡æœ¬è¾“å…¥å­—æ®µå’Œå¤„ç†ç”¨æˆ·è¾“å…¥ã€‚

```python
def ask_and_get_answer(vector_store, q, k=3): 
	from langchain.chains import RetrievalQA 
	from langchain.chat_models import ChatOpenAI 
 
	llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1) 
	retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k}) 
	chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever) 
 
	answer = chain.run(q) 
	return answer 
 
# calculate embedding cost using tiktoken 
def calculate_embedding_cost(texts): 
	import tiktoken 
	enc = tiktoken.encoding_for_model('text-embedding-ada-002') 
	total_tokens = sum([len(enc.encode(page.page_content)) for page in texts]) 
	# print(f'Total Tokens: {total_tokens}') 
	# print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}') 
	return total_tokens, total_tokens / 1000 * 0.0004 
 
# clear the chat history from streamlit session state 
def clear_history(): 
	if 'history' in st.session_state: 
    	del st.session_state['history'] 
 
 
if __name__ == "__main__": 
	import os 
 
	# loading the OpenAI api key from .env 
	from dotenv import load_dotenv, find_dotenv 
	load_dotenv(find_dotenv(), override=True) 
 
	st.image('img.png') 
	st.subheader('LLM Question-Answering Application ğŸ¤–') 
	with st.sidebar: 
    	# text_input for the OpenAI API key (alternative to python-dotenv and .env) 
    	api_key = st.text_input('OpenAI API Key:', type='password') 
    	if api_key: 
        	os.environ['OPENAI_API_KEY'] = api_key 
 
    	# file uploader widget 
    	uploaded_file = st.file_uploader('Upload a file:', type=['pdf', 'docx', 'txt']) 
 
    	# chunk size number widget 
    	chunk_size = st.number_input('Chunk size:', min_value=100, max_value=2048, value=512, on_change=clear_history) 
 
    	# k number input widget 
    	k = st.number_input('k', min_value=1, max_value=20, value=3, on_change=clear_history) 
 
    	# add data button widget 
    	add_data = st.button('Add Data', on_click=clear_history) 
 
    	if uploaded_file and add_data: # if the user browsed a file 
        	with st.spinner('Reading, chunking and embedding file ...'): 
 
            	# writing the file from RAM to the current directory on disk 
            	bytes_data = uploaded_file.read() 
            	file_name = os.path.join('./', uploaded_file.name) 
            	with open(file_name, 'wb') as f: 
                	f.write(bytes_data) 
 
            	data = load_document(file_name) 
            	chunks = chunk_data(data, chunk_size=chunk_size) 
            	st.write(f'Chunk size: {chunk_size}, Chunks: {len(chunks)}') 
 
            	tokens, embedding_cost = calculate_embedding_cost(chunks) 
            	st.write(f'Embedding cost: ${embedding_cost:.4f}') 
 
            	# creating the embeddings and returning the Chroma vector store 
            	vector_store = create_embeddings(chunks) 
 
            	# saving the vector store in the streamlit session state (to be persistent between reruns) 
            	st.session_state.vs = vector_store 
            	st.success('File uploaded, chunked and embedded successfully.') 
```

å®Œæˆæ­¤è®¾ç½®åï¼Œç”¨æˆ·å¯ä»¥ä¸ AI åº”ç”¨ç¨‹åºæ— ç¼ç›´è§‚åœ°äº¤äº’ã€‚

## æ£€ç´¢ç­”æ¡ˆå¹¶å¢å¼ºç”¨æˆ·äº¤äº’

æ­¤ RAG èŠå¤©åº”ç”¨ç¨‹åºåˆ©ç”¨ [LangChain çš„ RetrievalQA](https://python.langchain.com/docs/modules/data_connection/) å’Œ ChromaDBï¼Œä» ChromaDB çš„åµŒå…¥æ•°æ®ä¸­æå–ç›¸å…³ã€å‡†ç¡®çš„ä¿¡æ¯ï¼Œé«˜æ•ˆåœ°å“åº”ç”¨æˆ·æŸ¥è¯¢ï¼Œå±•ç¤ºäº†é«˜çº§ç”Ÿæˆå¼ AI èƒ½åŠ›ã€‚

ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¼”ç¤ºäº†åœ¨ Streamlit åº”ç”¨ç¨‹åºä¸­å®é™…å®ç°æ­¤è¿‡ç¨‹ï¼š

```python
# user's question text input widget 
	q = st.text_input('Ask a question about the content of your file:') 
	if q: # if the user entered a question and hit enter 
    	if 'vs' in st.session_state: # if there's the vector store (user uploaded, split and embedded a file) 
        	vector_store = st.session_state.vs 
        	st.write(f'k: {k}') 
        	answer = ask_and_get_answer(vector_store, q, k) 
 
        	# text area widget for the LLM answer 
        	st.text_area('LLM Answer: ', value=answer) 
 
        	st.divider() 
 
        	# if there's no chat history in the session state, create it 
        	if 'history' not in st.session_state: 
            	st.session_state.history = '' 
 
        	# the current question and answer 
        	value = f'Q: {q} \nA: {answer}' 
 
        	st.session_state.history = f'{value} \n {"-" * 100} \n {st.session_state.history}' 
        	h = st.session_state.history 
 
        	# text area widget for the chat history 
        	st.text_area(label='Chat History', value=h, key='history', height=400) 
```

æ­¤ä»£ç é›†æˆäº† Streamlit ä¸­çš„ç”¨æˆ·è¾“å…¥å’Œå“åº”ç”Ÿæˆã€‚å®ƒä½¿ç”¨ ChromaDB çš„çŸ¢é‡æ•°æ®è·å–å‡†ç¡®çš„ç­”æ¡ˆï¼Œå¢å¼ºäº†èŠå¤©åº”ç”¨ç¨‹åºçš„äº¤äº’æ€§ï¼Œå¹¶æä¾›äº†ä¿¡æ¯ä¸°å¯Œçš„ AI å¯¹è¯ã€‚

## ç»“è®º

æœ¬æ•™ç¨‹æ¢è®¨äº†ä½¿ç”¨ OpenAIã€ChromaDB å’Œ Streamlit æ„å»º LLM åº”ç”¨ç¨‹åºçš„å¤æ‚æ€§ã€‚å®ƒè§£é‡Šäº†å¦‚ä½•è®¾ç½®ç¯å¢ƒã€å¤„ç†æ–‡æ¡£ã€åˆ›å»ºå’Œå­˜å‚¨åµŒå…¥ï¼Œä»¥åŠæ„å»ºç”¨æˆ·å‹å¥½çš„èŠå¤©ç•Œé¢ï¼Œçªå‡ºäº† RAG å’Œ ChromaDB åœ¨ç”Ÿæˆå¼ AI ä¸­çš„å¼ºå¤§ç»„åˆã€‚

æ­¤ [GitHub å­˜å‚¨åº“](https://github.com/sowole-aims/chat-with-documents-app) æ¶µç›–äº†è¯¥è¿‡ç¨‹ã€‚è¦è¿è¡Œè¯¥åº”ç”¨ç¨‹åºï¼Œè¯·åœ¨ç»ˆç«¯ä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```
streamlit run ./chat_with_documents.py
```

æ‚¨ç°åœ¨å¯ä»¥é€šè¿‡å¯¼èˆªåˆ° http://localhost:8501 æ¥æµ‹è¯•è¯¥åº”ç”¨ç¨‹åºã€‚

æˆ‘é¼“åŠ±æ‚¨å°è¯•æ­¤åº”ç”¨ç¨‹åºï¼Œä½¿å…¶æˆä¸ºæ‚¨è‡ªå·±çš„åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸ä»–äººåˆ†äº«æ‚¨çš„ç»éªŒï¼
