我们可能永远无法消除幻觉，但我们可以降低其风险，建立防护措施，并在实践中学习。

向任何 GenAI 代理提出问题，你都有可能收到不准确的回复或幻觉。AI 幻觉以多种代价高昂的方式给企业带来重大风险。根据 [Vectara 最近的一项研究](https://github.com/vectara/hallucination-leaderboard)，根据大型语言模型 (LLM) 的不同，AI 幻觉的发生率在 0.7% 到 29.9% 之间。

幻觉的影响可能会扰乱运营，削弱效率和信任，并导致代价高昂的挫折。如果公众发现企业依赖虚假信息，例如银行聊天机器人提供不正确的贷款条款，AI 机器人顾问推荐不适当的投资，或呼叫中心散布虚假信息，那么客户和利益相关者的信任就会迅速丧失。

## AI 缓解的真实案例

最近的一个例子发生在我们与一家领先的机电设备制造商合作时。他们的客户支持团队在一个高流失率的环境中运作，经验水平各异的代表每天处理大量关于设备故障排除的电话。主要问题出现在 AI 助手表现出“过于热情”的行为，[为其知识库中未包含的产品提供故障排除建议](https://thenewstack.io/new-laravel-related-offerings-include-octane-alternative/)。这导致 AI 从类似产品中推断解决方案或生成虚构的响应。鉴于制造商设备的细微差别，这一点尤其成问题，因为看似相同的产品可能具有显着的功能差异。

为了解决幻觉问题，我们实施了一种侧重于精确度和验证的综合方法。该团队[在 AI 代理中开发了额外的检查和防护措施](https://thenewstack.io/ai-agents-transform-platform-engineering-at-microsoft/)，以确保在知识库搜索期间更准确地分析产品变体。这些保障措施旨在确保用于故障排除的知识库段与有问题的特定设备高度精确地匹配。此外，该解决方案还在故障排除工作流程中加入了额外的验证步骤，从而在 AI 通过用户界面向呼叫中心代表提出建议之前创建了多个检查点。

增强的防护措施和验证流程的实施成功解决了幻觉问题，同时在客户支持运营方面带来了可衡量的改进。改进后的 AI 助手现在提供更准确、特定于设备的故障排除指导，从而显着降低了误诊率。额外的精确控制确保建议基于精确的产品匹配，而不是可能产生误导的推断。因此，客户体验到更快的问题解决时间、更高的支持质量以及对 AI 辅助故障排除过程的增强信心，最终为代表和客户创造了更有效的支持环境。

## Agentic AI 如何帮助防止幻觉

AI Agents 正在成为缓解 AI 幻觉风险的常用方法。凭借其感知环境、推理可能采取的行动、采取行动以实现目标以及从反馈中学习以不断提高其性能的能力。

正如我们的呼叫中心示例中一样，Agentic AI 可以在最终确定答案之前评估响应是否与已知事实或逻辑推理相符，这是 AI 代理执行的一种自我评估技术。这种迭代推理有助于在内部识别和纠正错误。Agentic AI 还会从经过验证的外部资源检索实时信息，而不是依赖于预先训练的数据。这确保了响应是最新且准确的。进一步扩展这个概念，AI 代理可以交叉引用[多个来源或跨不同数据集运行查询](https://thenewstack.io/best-practices-collect-and-query-data-from-multiple-sources/)，以确认其答案的一致性和可靠性，从而减少信息被捏造的可能性。

[Agentic AI 模型还可以采用 Chain-of-Thought Prompting](https://thenewstack.io/how-to-add-reasoning-to-ai-agents-via-prompt-engineering/)，该方法将复杂问题分解为逻辑步骤或一种思维链推理；这种技术在推理密集型任务中尤其有效。

最后，利用检索增强生成 (RAG) 解决方案已被证明是有帮助的。Agentic [RAG 结合了检索增强生成的优势](https://thenewstack.io/advanced-retrieval-augmented-generation-rag-techniques/) 与自主代理推理，使 AI 能够管理复杂任务、个性化响应并优先处理相关信息，同时将输出建立在可信的知识来源之上。

## 数据量与数据质量的神话

虽然 AI 需要大量数据，但认为更多数据会导致幻觉是一种错误的假设。数据量与 AI 幻觉之间的关系是微妙的，很大程度上取决于数据的质量和[类型](https://thenewstack.io/redis-data-types-the-basics/)。虽然企业通常认为“更多数据 = 更好的 AI”，但关键因素是验证（消除不准确之处）、来源控制（优先考虑人工生成的数据）和多样性（避免领域过度代表）。当满足这些条件时，扩展数据可以提高可靠性；如果没有这些条件，幻觉会恶化。

## 预防 AI 幻觉的有效策略

虽然不能完全消除幻觉，但可以采取一些策略来大大减轻其发生和影响。

不足为奇的是，确保 AI 模型在多样化、准确和全面的数据集上进行训练可以最大限度地降低幻觉的风险。定期更新和清理训练数据使模型能够适应新信息并避免产生过时或不正确的输出。制定清晰、具体和详细的提示可以引导 AI 获得更准确的响应并减少歧义，否则可能会导致不准确或误导性的结果。

结合人工监督，尤其是在高风险或受监管的环境中，使专家能够在 AI 生成的输出被使用或发布之前对其进行验证和纠正，从而作为防止错误和不准确的最后一道防线。明确定义 AI 系统的预期用途和边界，确保它仅应用于为其设计和训练的任务，从而最大限度地降低在面对不熟悉的情况下产生幻觉的风险。