
<!--
title: CISO警报：软件智能体时代来临，今日布局未来安全！
cover: https://cdn.thenewstack.io/media/2025/09/af5f2a7e-planning.jpg
summary: 人工智能在软件开发中普及，带来安全挑战，但治理滞后。需将代理活动归因于人类、追踪代理行为并提升团队技能。正确使用人工智能可增强软件安全，补充人类专业知识。
-->

人工智能在软件开发中普及，带来安全挑战，但治理滞后。需将代理活动归因于人类、追踪代理行为并提升团队技能。正确使用人工智能可增强软件安全，补充人类专业知识。

> 译自：[CISOs: Prepare for Software’s Agentic Future Today](https://thenewstack.io/cisos-prepare-for-softwares-agentic-future-today/)
> 
> 作者：Josh Lemos

[GitLab 的](https://about.gitlab.com/?utm_content=inline+mention) [最近 C 级别高管调查](https://about.gitlab.com/software-innovation-report/)发现，89% 的高管预计，在三年内，代理式人工智能将成为[明确的软件开发标准](https://thenewstack.io/ai-has-won-googles-dora-study-shows-universal-dev-adoption/)。这种采用轨迹与一个严峻的现实相冲突：85% 的高管也认识到代理式人工智能将带来前所未有的安全挑战。

首席信息安全官们（CISO）面临压力，需要在两个看似矛盾的目标之间找到平衡。他们无法阻止组织在软件开发中采用人工智能，但必须将该技术的潜在安全风险降至最低。鉴于 91% 的高管表示他们计划在未来 18 个月内增加软件开发中人工智能的支出，这种安全压力将持续上升。

## **治理落后于人工智能的采用**

大多数安全负责人痛苦地意识到受访者提及的顶级代理式人工智能风险：网络安全威胁（52%）、数据隐私和安全（51%）以及维护治理（45%）。这些风险的格局乃至定义都在不断演变并深度交织。

建立人工智能治理模型是组织使其安全战略与新兴人工智能风险同步发展的必要条件。然而，这并非易事，因为人工智能涵盖从数据治理到身份和访问管理的众多技术和安全领域。尽管如此，近一半受访者承认他们的组织尚未为人工智能实施符合法规的治理（47%）或内部政策（48%）。

人工智能治理的滞后源于行业范围内合法的挑战，使得领导者难以确定投入时间和精力的最有效之处。代理的非确定性性质导致它们以意想不到的方式行事，这已被证明会扰乱现有的安全边界。此外，随着通用协议（例如[模型上下文协议（MCP）](https://thenewstack.io/model-context-protocol-a-primer-for-the-developers/)和 [Agent2Agent](https://thenewstack.io/googles-agent2agent-protocol-helps-ai-agents-talk-to-each-other/)）的引入，安全复杂性正在增加，这些协议简化了数据访问并增强了代理互操作性以构建生态系统。

但这些挑战不能阻止安全领导者优先考虑人工智能治理。如果您正在等待这种动态技术的全面最佳实践，那么您将永远处于追赶状态。任何完全避免采用人工智能的组织，仍将通过供应商和其环境中的影子人工智能使用而面临人工智能风险。

## **开始建立人工智能治理的 3 种方法**

首席信息安全官们可以通过建立能够跨环境跟踪、审计和归因代理行为的人工智能可观测性，来规划代理安全风险。以下是首先需要关注的几个领域：

### **1. 将代理活动归因于人类操作员**

随着人工智能系统的普及，跟踪和保护这些非人类身份变得与管理人类用户访问同样重要。实现这一目标的一种方法是采用复合身份，它将人工智能代理的身份与其指导者（人类用户）的身份关联起来。因此，当人工智能代理尝试访问资源时，您可以对代理进行身份验证和授权，并将活动明确归因于负责任的人类用户。

### **2. 跟踪组织内的代理行为**

运营、开发和安全团队需要[监控人工智能代理活动的方法](https://thenewstack.io/3-ways-security-teams-can-tame-autonomous-ai-agents/)在多个工作流、流程和系统中的活动。仅仅知道代理在您的代码库中做了什么是不够的。您还需要能够监控它在预演和生产环境中的活动，以及在相关数据库和它访问的任何应用程序中的活动。

### **3. 投资于团队技能提升**

现在的[安全文化](https://thenewstack.io/beyond-culture-addressing-common-security-frustrations/)需要具备人工智能素养。43% 的受访者承认人工智能技能差距正在扩大，除非技术领导者优先考虑[提升团队技能](https://thenewstack.io/sustainable-scale-how-to-grow-engineering-teams-strategically/)，使其理解模型行为、提示工程以及如何批判性地评估模型输入和输出，否则这种差距可能会继续扩大。

了解模型在哪里表现良好以及在哪里使用效果不佳，有助于团队避免不必要的安全风险和技术债务。例如，在反模式上训练的模型在检测这些模式方面表现良好，但对于它从未遇到过的逻辑错误则无效。团队还应该认识到，任何模型都不能取代人类的独创性。如果模型在[安全工程师或开发人员不太熟悉](https://thenewstack.io/three-software-development-challenges-slowing-ai-progress/)的领域表现不佳，他们将无法识别模型遗留的安全漏洞。

首席信息安全官们应考虑将一部分学习和发展预算用于持续的技术教育。这将在内部培养人工智能安全专业知识，让新晋的人工智能倡导者能够教育他们的同事并加强最佳实践。

## **正确使用时，人工智能有益于软件安全**

战略性部署人工智能的组织，与那些临时实施的组织相比，在安全方面取得了显著更强的成果。调查结果支持这一结论，45% 的高管受访者将[安全识别为人工智能](https://thenewstack.io/software-security-imperative-forging-a-unified-standard-of-care/)在软件开发中的首要潜在用例。

当组织将人工智能定位为人类专业知识的补充而非替代时，其对安全的价值达到顶峰。这种方法使人工智能能够通过提供例行安全自动化、智能编码建议以及直接嵌入开发者工作流中的有价值的安全上下文，帮助在开发团队中普及安全知识。实施这些能力的组织报告称，安全成果得到改善，风险降低，开发和安全团队之间的协作更加紧密。

如果组织想要获得竞争优势，它们就不会完全避免人工智能，也不会在不充分考虑的情况下采用它。相反，它们将在实施之初就建立基础安全控制。即使是不完善的初步措施，也将使安全团队更容易应对风险格局的变化。

如果调查中高管的预测被证明是正确的，我们已经进入软件代理式未来的三年倒计时。那些引导团队正确使用人工智能的领导者将获得超越降低风险的好处。他们将更快地生产出高质量、安全的软件。