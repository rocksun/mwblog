# RAG 和模型优化：AI 实践指南

![Featued image for: RAG and Model Optimization: A Practical Guide to AI](https://cdn.thenewstack.io/media/2025/03/4d0bc1c7-ai-rag-model-optimization-practical-guide-1024x576.jpg)

工程领导者面临着越来越大的压力，需要将 AI 集成到软件开发中，同时平衡模型选择、性能优化、安全性和成本效益。传统的微调方法需要大量的资源，并且难以跟上不断发展的企业代码库。与此同时，智能路由系统和专家模型引入了复杂性和可扩展性问题。

挑战在于[部署 AI 解决方案](https://thenewstack.io/ai-agents-a-comprehensive-introduction-for-developers/)，这些解决方案提供准确、具有上下文感知能力的建议，同时在不同的开发环境中保持灵活性和效率。本文探讨了各种 AI 模型策略的优势和局限性——[专家混合模型](https://towardsdatascience.com/tag/mixture-of-experts/) (MoE)、[微调](https://thenewstack.io/is-fine-tuning-or-prompt-engineering-the-right-approach-for-ai/)、[检索增强生成 (RAG)](https://thenewstack.io/retrieval-augmented-generation-for-llms/) 和混合方法——为企业软件工程选择最有效的解决方案提供了一个框架。

## 集成 LLM 和 SLM

通过利用两者的优势，将小型语言模型 (SLM) 和[大型语言模型 (LLM)](https://thenewstack.io/what-is-a-large-language-model/) 集成到软件工程任务中，可以优化效率。这种混合方法通过 MoE 模型、特定于任务的适配器和协作算法等方法，使代码生成、调试和文档编制等任务受益。

### 专家混合模型 (MoE) 和特定于任务的适配器

MoE 架构采用门控机制，将任务动态分配给最合适的模型。这种方法通过将较简单的任务分配给较小的模型，将复杂的任务分配给较大的模型来优化效率。同样，特定于任务的适配器通过使较小的模型充当软件开发生命周期 (SDLC) 中专门任务的中介，从而增强 LLM 性能。

### 协作算法和 Co-LLM

协作算法（例如由 [MIT CSAIL](https://imes.mit.edu/news-events/enhancing-llm-collaboration-smarter-more-efficient-solutions) 开发的 Co-LLM）通过在需要时选择性地调用专家模型来提高 LLM 的准确性。“开关变量”决定何时引入专家输入，从而提高事实准确性，同时最大限度地减少计算开销。

与需要同时执行模型的传统方法不同，Co-LLM 选择性地激活特定令牌的专家模型，从而优化资源使用。这种方法已在生物医学数据和数学等领域证明了成功，优于独立的微调 LLM。

## MoE 系统中的智能路由

智能路由评估查询的复杂性，以确定通用模型是否可以处理该查询，或者是否需要专用模型。在令牌级别，此技术有选择地调用专家模型来处理对准确性要求很高的查询。但是，在企业软件开发中实施此方法会带来挑战。有效使用需要持续的反馈机制、对路由配置的细粒度控制以及覆盖不正确响应的能力。

这种方法的一个根本限制是 LLM 和 SLM 依赖于静态训练数据，这使得它们本质上是过时的，并且缺乏对不断发展的企业代码库的上下文感知。为了保持有效性，企业团队需要不断地跨各种编程语言、库、依赖项、安全策略和架构模式微调多个专家模型。此过程会产生高昂的计算成本和资源开销，因此考虑到代码库的快速发展，这是不切实际的。

## RAG 作为一种可扩展的替代方案

RAG 通过实时动态检索外部数据，提供了一种更高效且可扩展的替代方案。这确保了响应的准确性、及时性和上下文相关性，而无需进行广泛的微调。与静态模型微调不同，RAG 能够适应特定于任务、用户、项目和组织的上下文，而无需多个专用模型。

### 企业软件开发中的 RAG

假设一位开发人员被分配了一个 Jira 问题，需要更新前端、后端和微服务架构。使用智能路由方法，开发人员需要访问多个微调模型，并且由于正在进行的开发更改，每个模型可能已经过时。就计算资源和部署复杂性而言，这种方法效率低下。
或者，借助基于 RAG 的系统，数千名开发人员可以依赖于单个高性能 LLM。如果安全和隐私是首要考虑因素，企业可以在本地部署开源 LLM，从而将运营成本降低到仅硬件和能源消耗。虚拟私有云 (VPC) 部署还提供了一种完全私有和安全的部署方法，该方法具有成本效益，而无需采购硬件。

当模型部署在 AI 软件开发平台中时，开发人员可以利用上下文感知选择机制，将 RAG 架构导向最相关的上下文源组合，以用于处理来自其本地工作区、图像以及非代码和代码库源的任务。这种受控的上下文关联提高了代码质量，而无需承担模型微调的资源负担。

[新加坡国立大学](https://arxiv.org/abs/2401.11817)的研究表明，RAG 是减少 LLM 响应中幻觉的最有效方法之一。在实际的企业环境中，复杂的 RAG 实施已证明代码质量提高了高达 80%，同时在独立的本地或基于 VPC 的部署模型中运行。

通过 AI 软件开发平台利用 RAG 的上下文代理工作流程在实践中如下所示：

- 将前端项目克隆到工作区中，允许 AI 平台索引项目的上下文。
- 选择 Jira 问题和图像上下文，以将验收标准馈送到 LLM 中，以实现准确的初始实施。
- 使用存储库上下文来识别相关的微服务文件，从而减少冗余代码生成并防止技术债务。
- 动态选择上下文数据源，确保精确且与任务相关的代码建议。
通过以这种方式构建上下文输入，RAG 有效地提供了微调的优势，而无需进行广泛的模型重新训练。这种方法使开发人员可以直接控制上下文关联 LLM 响应，从而提高准确性和效率。

## 通过 RAG 和微调实现上下文感知
[RAG 和微调之间的选择](https://thenewstack.io/rag-vs-fine-tuning-models-whats-the-right-approach/)取决于特定的工程用例。企业受益于灵活、可配置的混合方法，这些方法正在推动 AI 软件开发平台的发展。这些平台提供对模型选择、上下文源、部署配置和代理工作流程的控制，从而使工程团队可以根据自己的需求定制 AI 实施。

### 用于专门代码完成的 SLM
SLM 特别适合在专门领域中进行微调的代码完成。半导体制造（使用 Verilog）、航空航天和国防（使用汇编、Ada 或 [Rust](https://roadmap.sh/rust)）以及政府（使用 COBOL）等行业的数千名工程师都依赖于微调的 SLM 来实现其精度和可审计性。这些模型对于本地部署具有成本效益，甚至可以在气隙环境中运行。

推理模型的最新进展，例如 OpenAI o3-mini，证明了将 SLM 与 RAG 结合用于代理代码验证和审查的有效性。通过提取基于规则的数据库，推理模型可以根据预定义的架构、安全性和性能标准验证代码，从而在 pull 请求中提供可操作的建议。

### 用于深度推理和广泛 SDLC 应用的 LLM
LLM 在深度推理、复杂调试和全面代码生成方面表现出色。由于其广泛的知识库，单个 LLM 可以支持多种编程语言、范例和架构，从而减少了对多个微调模型的需求。在 AI 软件开发平台中部署 LLM 并使用 RAG 对其进行增强可以提高准确性，消除幻觉并优化资源效率。

通过集成上下文引擎和 SDLC 代理，AI 软件开发平台可以实现精确且具有上下文感知的 AI 驱动的软件开发。这些平台改进了代码翻译、复杂调试、架构一致性、重构、测试生成、文档和开发人员入职，而无需进行广泛的模型微调。

## 可配置的混合方法
对于高度监管和专业化的行业，AI 软件开发平台提供了最可行的解决方案。它们的可配置性使企业可以管理模型选择、微调、代理工作流程和基于 RAG 的上下文关联。这种方法可以完全控制 AI 部署，同时有助于确保企业软件开发环境中的效率、安全性和适应性。

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)