与云原生不同，“AI原生”这个术语定义模糊，甚至可以说尚未定义。许多人将其视为云原生的继任者，旨在从其前身那里获得光环效应。

然而，问题在于“云原生”这个术语是自然而然地产生的，旨在描述一套架构、实践、模式、技术以及相关创新，这些标志着系统架构的巨大转变。这种转变最简洁的描述是著名的“[宠物与家畜](http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/)”类比。

不幸的是，如今“AI原生”是一个抱有期望的营销术语，更多地被用来表示“嘿，我们正在使用AI”，而不是为了突出系统架构的转变。这是为什么呢？因为我们正处于自主时代（又称“AI时代”）的早期。

进入这个时代才短短几年，许多这些实践、模式和技术要么是全新的，要么被误解，要么根本还没有出现。

在新技术中，一些亮点如[模型上下文协议（MCP）](https://modelcontextprotocol.io/)正在引领潮流。即便如此，我们离明确定义“AI原生”还有很长的路要走。

尽管AI发展迅速，但许多待定义的模式只有通过AI和代理工作流的采用才会出现。对于自主时代，许多前沿技术的发展速度远超其采用速度。为了展望未来，我们必须回顾过去。

云时代的崛起和云原生应用程序是我们未来最好的指导。密切关注MCP的快速出现和演进——它才诞生一年多一点——也为我们提供了关于我们目前在定义“AI原生”应意味什么方面所处位置的一些想法。

## 云原生的兴起

早在2010年，专家和早期云采用者就已经在讨论云原生架构的核心原则。这些讨论早于实际术语的出现，但这些想法已经开始相当完善。

“云原生”作为一个术语，[直到2014年左右才开始流行](https://trends.google.com/trends/explore?date=all&q=%22cloud%20native%22&hl=en)。2010年，我们使用“云就绪”或“云中心”等术语。事实上，这里引用了我与Netflix当时的首席架构师Adrian Cockcroft进行[采访](https://cloudscaling.com/blog/cloud-computing/cloud-innovators-netflix-strategy-reflects-google-philosophy/)时的一段话。当时，Cockcroft正致力于将Netflix的视频服务重构并迁移到[AWS](https://aws.amazon.com/?utm_content=inline+mention)上的云原生架构，从其自家数据中心的老旧“宠物”基础设施中迁移出来。

Cockcroft：“关键挑战在于要拥有与世界上的[谷歌们](https://cloud.google.com/?utm_content=inline+mention)相同的思维模式，你的应用和服务的可用性和健壮性必须融入到你的软件架构中，你必须假设硬件和底层服务是短暂的、不可靠的，并且可能随时损坏或不可用，多租户公共云中的其他租户会增加随机的拥堵和变化。实际上，即使使用最可靠的硬件，你在大规模部署时也总是会遇到这个问题，因此*云就绪架构*就是将你在大规模部署中必须使用的模式，应用于较小规模以利用成本最低的基础设施。”

大约在2014-2015年，随着Kubernetes的兴起以及[云原生计算基金会（CNCF）](https://cncf.io/?utm_content=inline+mention)的成立，“云原生”这个术语进入了常用语。Kubernetes作为首批真正的云原生应用交付平台之一，从谷歌脱离出来，以打包形式带来了超大规模厂商的理念和架构，标准化了模式，其余的都已成为历史。

[![云原生时间线。](https://cdn.thenewstack.io/media/2025/12/7a98faf9-image1.png)](https://cdn.thenewstack.io/media/2025/12/7a98faf9-image1.png)

因此，无论你将云计算的起点定义为Salesforce.com在2000年开创的“按需付费”商业模式，还是AWS EC2在2008年的公开发布，从云计算作为一项创新到“云原生”成为一个完全成熟的行业术语之间，至少存在六年的时间滞后。这是为什么呢？

这是因为在21世纪初到2014年期间，随着云的普及，我们正在发现、实施和规范这些模式。在超大规模厂商之外，关于CAP定理（选择两个：一致性、可用性或分区容忍度）、松耦合、最终一致性、熔断模式、退避和重试、水平扩展等讨论寥寥无几。如今，这些都已是老生常谈，并在整个行业中广为人知。

这就引出了我们今天面临的情况：在弄清楚这些模式究竟是什么之前，就已经有人过早地尝试定义“AI原生”了。

## 深入审视MCP和“AI原生”

毫无疑问，AI正在改变整个IT堆栈，我们正面临着一场巨大的架构转变。在基础设施层，过去仅用于超级计算/高性能计算中心的利基技术，现在正被[广泛部署到企业和政府机构的新AI工厂中](https://www.mirantis.com/resources/mirantis-ai-factory-reference-architecture/)。

水平扩展已经转向更趋近于“恰当扩展”（right-scaling）模型：可以是水平、垂直或两者兼有。所谓的“加速计算”意味着从传统CPU转向GPU、张量处理单元（TPU）或为基于向量计算设计的专用集成电路（ASIC）/现场可编程门阵列（FPGA）。解耦的白盒系统正让位于垂直整合的、以供应商为中心的系统，这些系统旨在以最低成本每秒输出最多的token。

与此同时，AI代理（AI中价值最大的部分）在很大程度上依赖生成式AI（GenAI）/机器学习（ML）推理引擎作为它们的“模糊逻辑”大脑。这些代理由极简的支架和工具组成，围绕推理引擎（“推理大型语言模型[LLM]”）封装，提供所有核心业务逻辑。

高度确定性的经典代码正在让位于非确定性的“思维机器”，这些机器——在当前阶段——仍然需要护栏、评估以及人工干预来确保高质量的输出。

然而，其优点显而易见：这些系统虽然确定性较低，但能够同时处理不确定性、边缘情况和意想不到的状况，这是确定性经典代码永远无法做到的。最重要的是，就像之前的[云原生技术](https://thenewstack.io/cloud-native/ "云原生技术")兴起一样，并非所有事物都会改变，这一点很明确。

经典代码必须与AI逻辑交织并无缝协作。我们甚至看到前沿工作正在动态地[插值经典代码和LLM逻辑](https://dl.acm.org/doi/10.1145/3763092)。

尽管AI已经伴随我们几十年了，就像“云”背后的许多模式在形成之前也伴随我们几十年一样，但直到最近几年，随着生成式AI的出现，AI才真正形成并可供大规模采用。

现在，在这个自主时代，我们正在理解它将如何改造整个IT堆栈，而AI代理的采用才是真正学习的发生地。

现在理解“AI原生”的含义还为时过早。仅仅几年过去，我们只了解少数新兴模式。我们现在才刚刚开始发现哪些有效，哪些无效。让我们以MCP最近的出现及其快速演变为例。

首先，MCP似乎是源于Anthropic内部发现的直接需求。需要是发明之母，这句精辟的格言一次又一次被证明是正确的。

MCP推出仅一年，便得到了快速采用，尽管不乏瑕疵。在短短几个月内，Anthropic发布了MCP的一个重大更新，加入了认证和基本安全性，这弥补了原始版本中的一个严重疏漏。

几个月后，又发布了一个版本，提供了MCP注册表，这是一个小小的疏漏，但也是另一个明显的不足。最近，Anthropic已开始从最初专注于MCP服务器为代理提供“工具”以供调用，转向使用经典确定性代码调用多个工具的模型，从而节省上下文窗口/数据降级并显著提高效率。

这并非批评。这就是事情的运作方式。如果Anthropic等到MCP“完美”才发布，它可能就不会获得如此快的采用。这是硅谷圈子中备受推崇的典型最小可行产品（MVP）路线，随后是基于反馈、客户需求和实践学习的快速迭代。这也是我们学习最终将成为“AI原生”的模式、技术和架构的方式。

我们关于“AI原生”的所有学习并非都将通过MCP。学习将发生在各个领域，但MCP将成为核心，这既是因为Anthropic展示了快速创新的能力，也是因为最近宣布成立了[开源代理AI基金会](https://thenewstack.io/anthropic-donates-the-mcp-protocol-to-the-agentic-ai-foundation/)。

我想起了OpenStack发布时是如何“吸走了所有氧气”，给CloudStack、Eucalyptus、OpenNebula等前身敲响了丧钟。同样，一个开源的MCP只会加速发展，推动进一步创新，创建一个MCP生态系统，并帮助我们更快、更远地迈向“AI原生”。毫无疑问，Anthropic已经向Google的Agent2Agent（A2A）等替代方案发出通知，它打算让MCP成为标准，而非众多选项之一。