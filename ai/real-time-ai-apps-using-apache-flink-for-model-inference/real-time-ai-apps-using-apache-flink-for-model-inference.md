
<!--
title: 实时AI应用：使用Apache Flink进行模型推理
cover: https://cdn.thenewstack.io/media/2025/01/eaa74fe8-data.jpg
-->

使用Apache Flink进行远程模型推理，为AI和生成式AI用例提供了一种可扩展、灵活且具有弹性的数据驱动决策方法。

> 译自 [Real-Time AI Apps: Using Apache Flink for Model Inference](https://thenewstack.io/real-time-ai-apps-using-apache-flink-for-model-inference/)，作者 Kai Waehner。

在当今快节奏的数字环境中，企业面临着越来越大的实时处理数据和做出决策的需求。传统的批量处理和请求-响应API模型难以满足需要即时洞察的应用程序的需求，无论是检测欺诈交易、提供个性化的客户体验，还是优化工业物联网中的运营。

实时数据处理和人工智能的融合不再仅仅是竞争优势；它是释放现代应用程序全部潜力的必要条件。

这种迫切的需求凸显了为什么像[Apache Flink](https://www.youtube.com/watch?v=PVoc5tRr6to&t=129s)这样的框架（它可以实现持续的实时数据处理）对于克服这些挑战和实现卓越运营至关重要。

Flink使开发人员能够通过远程推理将实时数据流连接到外部机器学习模型，其中模型托管在专用模型服务器上并通过API访问。这种方法非常适合集中模型操作，允许简化更新、版本控制和监控，而Flink则处理实时数据流、预处理、数据管理和后处理验证。

![](https://cdn.thenewstack.io/media/2025/01/6f9c67a7-image1a-1024x517.png)

## 理解实时应用中的远程模型推理

在机器学习工作流程中，远程模型推理是指将实时数据流馈送到托管在外部服务器上的模型的过程。Flink应用程序向该服务器发出API调用，接收响应，并可以在几毫秒内对其进行操作。此设置确保模型更新、A/B测试和监控集中管理，简化了高吞吐量应用程序的维护和可扩展性，其中延迟是灵活性的权衡。

远程模型推理也可以在混合云设置中实现，其中模型可能托管在基于云的基础设施上，并由边缘或本地Flink应用程序访问。这种灵活性使企业能够[跨多个地理位置或系统架构扩展模型推理能力](https://thenewstack.io/finding-the-right-data-architecture-for-rag-pipelines/)，同时保持对模型生命周期的持续性和控制。

## 使用Apache Flink进行远程模型推理的关键优势

1. **集中式模型管理**: 通过远程推理，模型集中在模型服务器中进行管理，从而可以轻松进行更新和版本控制。[开发人员可以实现新的模型迭代](https://thenewstack.io/3-ai-trends-developers-need-to-know-in-2025/)，而不会中断Flink流式应用程序，从而最大限度地减少停机时间并确保无缝更新。
2. **可扩展性和灵活性**: 远程模型推理可以利用云基础设施实现可扩展性。随着需求的增加，模型可以通过向模型服务器添加资源来独立于Flink应用程序进行扩展，从而可以处理大量并发推理请求，而无需更改流式管道。无论如何，模型处理与Flink执行的数据编排工作是隔离和解耦的。
3. **高效的资源分配**: 通过将模型计算卸载到单独的模型服务器或云服务，远程推理释放了[Flink的资源以专注于数据处理](https://thenewstack.io/3-reasons-why-you-need-apache-flink-for-stream-processing/)。这在处理需要大量计算能力的复杂模型时尤其有利，允许Flink节点保持精简和高效。
4. **无缝监控和优化**: 集中式模型托管允许团队实时监控模型性能，使用分析仪表板跟踪准确性、延迟和使用情况指标。Flink应用程序可以使用此反馈循环来调整[数据处理参数并提高推理管道的整体性能](https://thenewstack.io/a-call-to-use-generative-ai-to-create-more-trustworthy-data/)。

## 用于实时客户支持的生成式AI：深入探讨

由大型语言模型 (LLM) 提供支持的生成式 AI 通过提供个性化的[大规模实时响应](https://thenewstack.io/why-we-use-apache-kafka-for-real-time-data-at-scale/)彻底改变了客户支持。将此功能与 Apache Flink 集成提供了一种无缝、高效的方式来处理高吞吐量的客户查询，同时保持低延迟和集中式模型管理。以下是实际操作方法，分解为详细的真实案例：

## 真实案例：电子商务客户支持

想象一下，一个全球性的电子商务平台每天处理数百万次的客户互动。客户打开实时聊天并询问退货事宜。以下是Flink如何与LLM集成以实时处理和响应此查询：

1. **数据摄取和预处理**: 查询通过Apache Kafka进入Flink，Kafka从各种客户互动渠道（例如网络聊天、电子邮件或通话转录服务）持续流式传输数据。Kafka Connect提供与实时、批处理和基于API的接口的连接。Flink预处理传入的客户查询，方法是将文本标记化，删除不相关信息，并使用元数据（例如客户的互动历史记录、情感分析或订单详细信息）对其进行丰富。
2. **异步远程推理调用**: 预处理查询后，Flink使用其异步I/O操作符向LLM服务器发送API请求以进行推理。这种异步方法确保Flink可以在等待LLM响应的同时继续处理其他传入查询，从而保持高吞吐量并避免阻塞操作造成的延迟。
3. **响应处理和后处理**: LLM服务器生成定制的响应，例如详细的退货说明或指向退货门户的链接。Flink验证响应并根据需要对其进行后处理，这可能包括重新格式化、附加其他上下文信息或确保符合业务规则（例如确认产品符合退货条件）。
4. **输出到下游系统**: 最终响应通过一个或多个Kafka主题从Flink转发到适当的下游系统。对于实时聊天，这可能是客户支持平台；对于电子邮件，这可能是自动化消息服务。这确保客户在几毫秒内收到答案，从而增强他们的支持体验。

## 使用Flink进行远程模型推理的最佳实践

1. **利用异步处理**: 在Flink中使用异步I/O处理远程推理请求，而不会减慢数据流速度，从而确保高吞吐量和高效的资源利用率。
2. **实现强大的错误处理**: 网络调用会引入潜在的故障点。设置重试、回退和超时以处理模型服务器可能暂时不可用的情况。
3. **使用高效的数据编码**: 以压缩格式（如Protocol Buffers或Avro）传输数据，以减少网络通信中的有效负载大小和延迟，尤其是在高频推理请求的情况下。
4. **监控模型漂移**: 在模型服务器上设置监控，以检测模型性能随时间的任何变化，确保预测在传入数据变化时保持准确。
5. **优化云资源**: 对于混合和云原生部署，确保模型服务器和流处理引擎都可以根据请求量动态扩展，使用自动扩展和负载均衡来保持成本效益，而不会牺牲性能。

## 结论：释放全部潜力

使用Apache Flink进行远程模型推理正在改变组织部署机器学习的方式，[用于预测性AI和GenAI用例的实时应用程序](https://thenewstack.io/4-steps-for-building-event-driven-genai-applications/)，提供了一种可扩展、灵活且弹性的方法来做出数据驱动的决策。通过将模型服务器与流应用程序分离，开发人员可以利用强大的AI功能，同时使Flink应用程序专注于高效的数据处理。这种方法在混合云设置中也很有益，允许企业在不同的环境中部署可扩展、高性能的推理。

Apache Flink对远程推理的强大支持使其成为构建实时、AI驱动型应用程序（这些应用程序以业务速度响应数据）的通用且必不可少的工具。要了解更多信息，请访问Confluent的[GenAI资源中心](https://www.confluent.io/generative-ai/)。
