# What AI Agents Do in the Shadows
![Featued image for: What AI Agents Do in the Shadows](https://cdn.thenewstack.io/media/2025/03/a492ca14-serkan-yildiz-yjxd2mccedu-unsplash-1024x683.jpg)
[Serkan Yildiz](https://unsplash.com/@serkanyldz?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)on
[Unsplash](https://unsplash.com/photos/silhouette-of-man-standing-beside-brick-wall-yJxd2MCcEdU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash).
[Marc Benioff](https://www.linkedin.com/in/marcbenioff/), CEO of Salesforce, [recently stated](https://finance.yahoo.com/news/benioff-ceos-today-last-manage-204459142.html?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAHDzf6TSxlpSKDOOnOKR0NZ376ix3w9P3zpaSOLyvoX5cDPl55KiAOgcIv9RUOw9a3-xq30p6gNzHxz_pv31HCwpQLXRjvnp0xIdS__YkwGSgfe4x3YYDbFCfGuIXkkuUoMtaQy_AZ9tbrYF1WA8v0oCndYvrwssosBNyUJCZV-r), “We’re the last generation of CEOs to only manage humans. Every CEO going forward is going to manage humans and agents together.” AI agents are evolving and progressing tremendously, but it’s important to note that autonomous systems didn’t just fall out of the sky one day. AI agents are the byproduct of techniques, tools, and principles from previous technological revolutions. Not the least of which is low-code/no-code (LCNC) development.
LCNC has long enabled business users, developers, and IT teams to democratize application development by eliminating the need for deep coding expertise. But its real power comes from empowering those who deeply understand business needs to build apps.

**AI Agents Are Everywhere**
AI agents take this concept further by infusing intelligence, autonomy, and [dynamic decision-making into the previously static application](https://thenewstack.io/why-you-still-need-dynamic-application-security-testing/) workflows built and used by business users across the enterprise. Off-the-shelf AI agents like Microsoft 365 Copilot, OpenAI Operator, Google Gemini, and Salesforce Einstein can autonomously take actions on behalf of users, and those actions are largely built using LCNC development platforms.

It’s not just Software as a Service (SaaS) providers; it’s also taking force in the cloud with AWS Bedrock, Azure AI Studio, Vertex AI, and more. These tools are being used by business users and can also be customized and extended using LCNC development. Further, the [methods for building actions and data](https://thenewstack.io/data-modeling-part-2-method-for-time-series-databases/) connections to other systems, apps, and knowledge sources are built with low code.

Security can no longer afford to be considered a blocker; instead, it needs to act as an enabler for the business. However, these new intelligent and action-oriented AI agents raise serious questions about governance, security, and the unintended consequences of organizations empowering AI-driven decision-making that can easily evade traditional security controls.

**LCNC Set the Stage for AI Agents**
AI agents rely on low-code tooling to enable large language models (LLMs) and other AI models to become agentic by enabling anyone to connect agents to different data sets and knowledge bases to give agents various actions. More than that, LCNC enables building agents from scratch, empowering anyone to cherry-pick the right LLMs, actions, flows, and so on.

AI agents are being embraced by more and more enterprises as they seek an edge for their business users. Researchers at Gartner estimate that by 2028, at least 15% of daily business decisions will be made autonomously through agentic AI. Let’s consider:

**User-driven development:**LCNC and AI agents allow nontechnical users to[build powerful applications](https://thenewstack.io/build-an-ai-powered-question-answering-application/), shifting innovation outside traditional development teams.**Autonomous workflows:**AI agents require access to various LCNC automation and flows to make real-time decisions steeped in business data and context rather than executing static logic.**Integration ecosystems:**AI agents thrive in environments where they can interact across multiple platforms, with numerous knowledge bases, triggers, and actions. Like LCNC, agents rely on prebuilt connectors and APIs to “go.”
**Agentic Platforms Introduce New Risks**
Anyone can also build, customize, and use their standalone AI agents that dynamically assist, execute, and integrate across business workflows.

This revolution makes it easy for anyone to build powerful AI agents that can think, reason, and act like humans to help offload tasks, automate processes, and improve efficiencies. Still, it also introduces security challenges at scale. Consider:

**Shadow AI risks:**Business users can create AI[agents without security](https://thenewstack.io/styra-extends-open-policy-agent-security-to-public-clouds/)or IT oversight, leading to unknown and unmonitored systems continuously churning and taking actions without humans in the loop.**Vulnerabilities:**AI agents require access to enterprise systems and data, but when less technical users configure those integrations, they often give them overly broad permissions that increase attack surfaces and data leakage risks.**Data exposure:**These AI-driven workflows interact with multiple systems, other agents, applications, and corporate data, increasing the risk of unintended data sharing, leakage, and exfiltration.**Promptware:**As AI agents get adopted across the business, bad actors can use direct and[indirect prompt injection](https://labs.zenity.io/p/indirect-prompt-injection-advanced-manipulation-techniques)aimed at jailbreaking or compromising AI agents with prompts, hidden instructions, and sophisticated attacks.**Jailbreaks:**Not only can AI agents be manipulated by prompts or have their underlying data poisoned, but agents can also be turned into[dangerous phishing insiders](https://www.youtube.com/watch?v=pZY-Xkyd1_I)or social engineering machines without even requiring account compromise.
[Agentic AI platforms](https://thenewstack.io/agentic-ai-the-missing-piece-in-platform-engineering/) extend the low-code revolution by enabling anyone to build or extend AI agents capable of dynamic, autonomous decision-making. However, the more accessible AI agents become, the harder they are to secure.
**The Security Challenge: More Power, More Risk**
With greater accessibility and automation comes greater risk. LCNC introduced new security and governance challenges by allowing non-security practitioners to create and deploy business-critical applications, thus evading the [software development lifecycle](https://thenewstack.io/zero-trust-security-and-the-software-development-lifecycle/), CI/CD tools, and AppSec tooling that rely on scanning code for vulnerabilities. Agentic AI raises the stakes, amplifying those security concerns with:

**Autonomous decision-making risks:**AI agents act dynamically, sometimes in ways their creators don’t anticipate.**Data exposure:**[AI agents rely on business-critical data](https://thenewstack.io/agentic-ai-for-enterprises-4-key-benefits-driving-innovation/), increasing the likelihood of sensitive data leakage when guardrails are misconfigured or bypassed.**Shadow AI and compliance risks:**As with shadow IT in LCNC, AI agents often emerge outside security oversight, making governance even more complex.
Even Yoshua Bengio, an AI pioneer often called the “godfather of AI,” has sounded the alarm on agentic AI. [At the World Economic Forum in Davos](https://www.businessinsider.com/yoshua-bengio-ai-godfather-agents-2025-1?), he emphasized the need for [strict regulations](https://www.zenity.io/blog/security/navigating-ai-agent-security-amid-evolving-regulations/) and proactive security before AI agents become widely embedded in enterprises. He cautioned that the lack of oversight could lead to unintended and unsafe behavior, making [governance and runtime inspection](https://www.zenity.io/resources/white-papers/trism-market-guide/) critical before scaling.

The ability of AI agents to act autonomously introduces new attack vectors, governance challenges, and regulatory risks.

## Securing AI Agents Requires a Defense-in-Depth Approach
While history doesn’t repeat itself, it does rhyme. We can look for clues from past [security trends](https://thenewstack.io/container-security-a-troubling-tale-but-hope-on-the-horizon/) to see if a Data Loss Prevention (DLP) or firewall-type approach will work for securing agents. AI Detection and Response (AIDR) approaches require behavioral understanding of agents that can tie exploitability to runtime agent behaviors. While information and data governance can be foundational, to truly secure AI agents, we need in-depth defense.

For AI agents, this means going deeper than prompts and responses, understanding, and gaining visibility into what and why agents behave as they do. This includes a comprehensive AI Security Posture Management (AISPM) [program to dive deep to understand AI agent](https://thenewstack.io/llms-and-ai-agents-evolving-like-programming-languages/) behavior, meaning the steps and actions it takes when prompted, what applications and agents it interacts with and touches, and what data it accesses and processes. AI agents inherently act and process information like humans, so security teams need to develop insider threat models to develop a baseline for normal behavior that can easily spot anomalous behavior and truly see what agents do in the shadows.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)