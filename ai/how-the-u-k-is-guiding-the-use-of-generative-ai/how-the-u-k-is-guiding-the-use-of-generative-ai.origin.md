# How the UK Is Guiding the Use of Generative AI
![Featued image for: How the UK Is Guiding the Use of Generative AI](https://cdn.thenewstack.io/media/2025/04/4e7b04d2-uk-ai-playbook-2-1024x576.jpg)
I hope a few more people looked at the Vatican’s [Antiqua et Nova](https://www.vatican.va/roman_curia/congregations/cfaith/documents/rc_ddf_doc_20250128_antiqua-et-nova_en.html) after the Pope’s passing, as it has a fairly mature [ethical examination of generative AI](https://thenewstack.io/cloud-service-what-the-pope-thinks-about-ai/). Weighing in at over 13,000 words, I wouldn’t expect it to be heavily read, but there are fairly sensible positions separating abuse from potential.

However, for this post I’m looking at the U.K. Government Digital Service’s [generative AI playbook,](https://gds.blog.gov.uk/2025/02/10/launching-the-artificial-intelligence-playbook-for-the-uk-government/) as this is active advice for real departments and public bodies. While this should be of interest to any public-facing organization, publicly funded bodies have their own problems and predilections. I’ll steer through this document via its 10 principles, but the overall point is that the playbook exists as a guide.

While the GDS never quite got to create Government as a Platform, the agency was the first serious exponent of that approach. It has always been open in documenting its own technical processes.

The explicit purpose of this playbook is to “help government departments and public sector organisations harness the power of a wider range of AI technologies safely, effectively, and responsibly.” But behind that there is a vague political backstory, as the British government has stated that AI should be embraced to save money. (I suspect the overall political framework was much less hectic when this strategy was being formed.)

Reading and adhering to guidelines is something we normally sign up to do, but not with any gusto. This document uses fairly straightforward language to form a consistent narrative. Starting from the definition [What is AI?](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#what-is-ai), the playbook avoids starting their timeline at LLMs and looks further back:

![A word diagram showing the types of AI, according to the "Artificial Intelligence Playbook for the UK Government."](https://cdn.thenewstack.io/media/2025/04/7e99ce7a-image.png)
The types of AI, according to the “Artificial Intelligence Playbook for the UK Government.”

**Agentic AI **is also mentioned, so the strategy is kept fresh. and I feel this makes a useful reference area for others to use.
## The 10 Principles
Of note are the [10 principles](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principles) to guide the use of AI. These often point to more detailed sections further down the document. Apart from the obvious things like “find out what it is before using it,” these principles are a strong attempt to let people examine their own reasoning.

Not all of it is timely; the sentence “AI systems currently lack reasoning and contextual awareness” in the [first Principle](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-1-you-know-what-ai-is-and-what-its-limitations-are) looks unlikely to age well. The Vatican guidelines by comparison sensibly isolate the perennial weakness in AI early on: it is limited to functional responses and cannot obtain an embodied unique human perspective.

Good reasoning and contextual awareness get large language model-generated responses into hot water, as they are good enough to be highly annoying when wrong. Using [Google’s](https://cloud.google.com/?utm_content=inline+mention) [Gemini](https://thenewstack.io/gemini-code-assist-review-code-completions-need-improvement/) responses can attest to that.

Principle 2, on [ethics and responsibility](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-2-you-use-ai-lawfully-ethically-and-responsibly) is understandably far weaker than the Vatican’s statements, but does at least underline the problem with biased training data. However, the industry has offered up plenty of ways to enhance training data. And, of course, the Vatican isn’t actually implementing anything.

I do like the way the playbook notes specific [security risks](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-3-you-know-how-to-use-ai-securely) that are unique to Generative AI in Principle 3. This part shelves the weak generalities and points specifically to [data poisoning](https://thenewstack.io/poisoning-the-well-and-other-generative-ai-risks/), [perturbation attacks](https://machine-learning-made-simple.medium.com/an-introduction-to-adversarial-perturbation-5e6c61d84b71), [prompt injection](https://thenewstack.io/when-prompt-injections-attack-bing-and-ai-vulnerabilities/) as well as hallucination. It notes that much of the AI tool chain “[w]as developed at pace by AI researchers and data scientists not following secure coding practices. AI tools often have elevated access rights to the systems they’re running on”.

[Principle 4](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-4-you-have-meaningful-human-control-at-the-right-stages) covers human overall control. This always sounds alarmist, because no one would question that a human wielding any tool generally retains overall control. But once you accept that GenAI simply isn’t reliable or consistent in output, whenever you use it in a tool chain you cede control to a completely [opaque system](https://thenewstack.io/why-llms-within-software-development-may-be-a-dead-end/). The [section](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#data-protection-and-privacy) of the playbook covering human oversight admits directly that autonomous decisions can infringe the UK General Data Protection Regulation (GDPR), which is based on the EU version.
[Principle 5](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-5-you-understand-how-to-manage-the-full-ai-life-cycle) which refers to the “full AI life cycle,” alludes (I think) to using generative AI within the standard software product life cycle, which is definitely [problematic](https://thenewstack.io/why-llms-within-software-development-may-be-a-dead-end/). There are also service standards to think about if deployment beckons.
But initially setting up LLMs via vendor APIs or hosting them locally is generally [well documented](https://thenewstack.io/how-to-set-up-and-run-a-local-llm-with-ollama-and-llama-2/), and apart from security concerns, can be rapid to prototype – certainly no worse than getting permission to host any other software within government. Managing the life cycle, may just refer to managing the software developers as opposed to [“vibe coding.”](https://thenewstack.io/vibe-coding-where-everyone-can-speak-computer-programming/)

[Principle 6](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-6-you-use-the-right-tool-for-the-job) has the misleading title “use the right tool for the job,” but has the right message underneath. Too many organizations are trying to fit one answer to various questions. LLMs are just another way to force an organizing principle throughout your work; separating repetitive tasks from the forking decisions between them.
Breaking down your work and rebuilding it is usually instructive — whether you can squeeze in some LLM solutions or not. This is where a [community of practice](https://thenewstack.io/developers-need-a-community-of-practice-and-wikis-still-work/) can help compare and contrast how current LLM solutions help with workflows in your organization.

As it happens, [Principle 7](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-7-you-are-open-and-collaborative) directly refers to a [cross-government community of practice](https://www.gov.uk/service-manual/communities/artificial-intelligence-community) but is primarily encouraging openness in communication. This is an attempt to form an open AI community for the public sector, and I can’t recommend that approach highly enough. The documents include findings about a “[GOV.UK chat.](https://insidegovuk.blog.gov.uk/2024/01/18/the-findings-of-our-first-generative-ai-experiment-gov-uk-chat/)” Unfortunately, this does not seem to have continued through the recent change of administration.

![A timeline showing how the GOV.UK Chat experiment was conducted, from a January 2024 U.K. Government Digital Service blog post.](https://cdn.thenewstack.io/media/2025/04/ba9ff9a7-image-1-1024x334.png)
A timeline showing how the GOV.UK Chat experiment was conducted, from a January 2024 U.K. Government Digital Service blog post.

For a public-facing body to remain transparent and open is nearly always much harder than keen proponents imagine. ([Principle 8,](https://www.notion.so/Working-inside-the-Panopticon-1ceba3b2748680f7a9c8d0f27647c4a4?pvs=21) about working with commercial colleagues, is a message from the U.K. government to itself, and is not particularly serious. Too many people in public-facing bodies work there explicitly to avoid the commercial sector.)

[Principle 9](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-9-you-have-the-skills-and-expertise-needed-to-implement-and-use-ai-solutions) checks that you have the team available to work with LLMs. The meaning of “training your own models” has changed recently, but it is still a fairly specialist activity even for software developers. The various [retrieval augmented generation (RAG)](https://thenewstack.io/why-rag-is-essential-for-next-gen-ai-development/) and vector embedding tools are slowly democratizing the area. There is an “acquiring skills and talent” section of the playbook, which links to open training for civil servants.
Finally, [Principle 10](https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-10-you-use-these-principles-alongside-your-organisations-policies-and-have-the-right-assurance-in-place) (using these principles alongside your organization’s policies) introduces the murky world of working with inter-organization silo politics and butting up against other panels, boards and pressure groups. Unsurprisingly, with such a hot topic, careful risk assessment and process documentation are essential. But against that, this very open project proves that there is encouragement from the government to embrace generative AI, which is commendable.

## Conclusion
I think most formal public organizations should at least adopt a set of principles as done by GDS, and should have little problem improving on them. These are certainly not written in stone. Most businesses are still too close to the frontline to spend time reflecting on their uses of AI, and don’t have the responsibility of government departments.

While we all wish we knew exactly where OpenAI and other tech leaders are heading, it is better to strategically plan LLM and generative AI use before committing to it in the first place.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)