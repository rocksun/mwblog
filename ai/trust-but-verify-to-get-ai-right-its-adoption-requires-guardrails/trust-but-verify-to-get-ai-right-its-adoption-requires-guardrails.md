<!--
# 相信但验证：确保正确使用AI，须建立防护措施
https://cdn.thenewstack.io/media/2023/09/35d3279e-hogarth-de-la-plante-7-plwj1zf58-unsplash-1024x611.jpg

Feature image by Hogarth de la Plante on Unsplash.    
 -->

译自 [Trust but Verify: To Get AI Right， Its Adoption Requires Guardrails](https://thenewstack.io/trust-but-verify-to-get-ai-right-its-adoption-requires-guardrails/) 。

各行业公司现今正处在AI应用的关键时刻。我们制定的政策、创建的策略，以及调整工作流程以纳入AI的方式，将有助塑造商业的未来。

要负责任地应用AI，组织必须找出方法将其与自己的目标保持一致，同时考虑可能需要更新的安全与隐私政策。如果战略性实施，AI可增强组织多个职能，从软件开发到营销、财务等。

尽管许多组织急于将AI融入工作流程，但真正受益的公司是那些对AI应用采取谨慎、战略性方法的公司。让我们看看组织如何确保AI应用的成功。

## 采取以隐私为先的方法

使用AI需建立防护措施，才能负责任、可持续地实现应用，这对组织和客户都非常重要。

GitLab最近一项调查显示，近半数(48%)受访者担心AI生成的代码可能不受人类生成代码相同的版权保护，42%的受访者担心AI生成的代码可能会引入安全漏洞。

如果不仔细考虑AI工具如何存储和保护专有公司数据、客户数据和合作伙伴数据，组织可能面临安全风险、罚款、客户流失和声誉受损。这对必须严格遵守外部监管和合规义务的组织尤其重要，比如公共部门、金融服务或医疗保健。

为确保知识产权得到约束和保护，组织必须制定严格政策，概述AI生成代码的允许用途。引入第三方AI平台时，组织应彻底尽职调查，确保其数据(包括模型提示和输出)不会被用于AI/ML模型训练和微调，这可能无意中向其他组织暴露其知识产权。

尽管许多流行的AI工具背后的公司在模型训练数据来源方面不够透明，但透明度是AI的基石。当模型、训练数据和可接受使用政策不透明且不允许检查时，组织将难以安全负责任地使用这些模型。

## 从小处着手

为安全、策略性地从AI效率中受益，组织可以通过先识别风险最低的领域来避免数据泄露和安全漏洞等陷阱。这可以让他们先在一个低风险领域建立最佳实践，然后再允许其他团队应用AI，以确保可安全扩展。

组织领导者可以从促进技术团队、法律团队和AI服务提供商之间的对话开始。建立共同目标基线对决定重点和如何最大限度降低AI风险至关重要。从那里，组织可以开始为AI实施制定防护措施和政策，如员工使用、数据消毒、产品披露和审核能力。组织还必须愿意参与经过良好测试的漏洞检测和修复计划。

## 找到合适的合作伙伴

组织可以寻找能帮助安全应用AI并确保他们建立在安全与隐私最佳实践之上的合作伙伴。这将使组织成功应用AI，而不会损害遵守合规标准，或损害与客户和利益相关者的关系。

组织关于AI和数据隐私的担忧通常分三类：用于训练AI/ML模型的数据集、专有数据的使用方式以及是否会保留专有数据(包括模型输出)。合作伙伴或供应商越透明，组织评估业务关系时就能更多了解信息。

## 制定积极的应急计划

最后，领导者可以制定围绕AI使用的安全政策和应急计划，并审视AI服务如何处理专有和客户数据，包括发送到AI模型的提示及从中收到的输出。

如果没有这些防护措施，后果可能严重影响组织未来对AI的应用。虽然AI可能改造公司，但它也带来真实风险，技术专家和企业领导者都负有责任地管理这些风险。

我们今天应用AI技术的方式将影响AI未来的作用。通过思考和策略性地识别优先应用AI的领域，组织可以获得AI的好处，而不会产生漏洞，冒险违反合规标准，或冒险与客户、合作伙伴、投资者和其他利益相关者的关系。
