# 新的影子 IT：野生的 LLM

几个月前，一家中型金融科技公司的平台工程师将 OpenAI 连接到了 Terraform 管道中。他并没有试图隐瞒。他只是想自动标记云资源，而且 LLM 处理得比团队内部构建的任何东西都更好、更快。但是当安全部门发现它时呢？审计恐慌。风险审查。追溯批准。听起来很熟悉？

LLM 现在代表着新的影子 IT。它们进入环境的速度比大多数组织能够追踪的速度还要快。

影子 AI 意味着开发人员在未经批准的情况下将[大型语言模型](https://thenewstack.io/what-is-a-large-language-model/) (LLM) 或 GenAI 系统集成到生产工作流程中。他们测试[在实时客户数据上的提示](https://thenewstack.io/with-leftoverlocals-gpus-can-leak-llm-prompt-data/)，从流氓脚本调用 OpenAI，并在 GPU 集群上启动微调模型，而无需通知安全或治理部门。虽然[开发人员可能无意引入风险](https://thenewstack.io/are-your-development-practices-introducing-api-security-risks/)，但这些操作完全绕过了可见性、控制和策略。

为什么这种情况不断发生？因为[开发人员不需要权限来安装 Python](https://thenewstack.io/why-every-python-dev-needs-virtual-environments-now/) 包。因为使用 API 密钥比提交 Jira 工单更容易。因为 AI 有效。这是没有人愿意说的部分：这些工具通常比批准的替代方案更有用。

我们已经看到 LLM 用于自动标记基础设施、分类警报、生成合规性文档存根，以及在知识库之上启动内部搜索工具。我们还看到它们悄悄地嵌入到 CI/CD 工作流程中，埋藏在无人审查的脚本中。有时，它们甚至在没有任何人标记依赖项的情况下进入生产管道。这些不是假设的极端情况——它们发生在我们交谈的每家公司。

影子 AI 不仅仅是开发人员偷工减料。这是一个信号。它告诉你你的工具太慢、太死板，或者根本缺失了。但如果你忽略它，或者更糟的是，试图在不提供替代方案的情况下进行压制，你就会失去信任。你会将这种活动推向更深的地下。最终，你会因为漏洞、审计失败或支持噩梦而付出代价，因为没有人知道该提示管道是如何工作的。

风险是真实存在的。开发人员可以通过将提示发送到外部 API 来泄露 PII 或专有逻辑。他们可以创建模型蔓延——无人拥有或维护的僵尸工作流程。他们可以引入在环境之间静默漂移的不可见依赖项。他们这样做时没有日志记录、没有审查流程，也没有审计跟踪。

一些组织开始赶上。他们构建了检测规则，以标记到 OpenAI 或 Anthropic 的出站流量。他们扫描[源代码或可疑运行时](https://thenewstack.io/new-open-source-runtime-for-web-developers-uses-p2p/)行为中的模型 API。他们查看遥测模式并跟踪类似于提示使用的异常情况。有时，他们甚至将 LLM **可观测性**构建到云安全工具或 EDR 平台中。这还处于早期阶段，但它正在开始发生。

最成熟的团队超越了点检测。他们绘制 AI 工作负载的运行位置、它们的作用以及它们如何与敏感系统和数据交互。他们不仅构建了模型运行时的可见性，还构建了谁构建了它、它位于何处以及它是否符合策略的可见性。这就是你如何从“打地鼠”式的反应转变为可持续治理的方式。如果你能看到 AI 在哪里被使用，你就可以管理它、引导它，并在不扼杀创新的情况下将其纳入其中。

尽管如此，仅靠检测并不能解决问题。如果你的唯一举措是关闭一切，你将会疏远那些试图解决实际问题的开发人员。一个更明智的策略是提供护栏，而不是路障。为批准的模型提供安全、内部的网关。启动内置**可观测性**的沙箱。为团队提供一种试验 AI 的方式，而不会让他们暴露在外——或隐形。

你仍然需要策略。你仍然需要监督。但如果你创建合理的默认设置，人们就会使用它们。如果你以控制为主导，他们就会绕过你。

无论你是否计划好，LLM 已经成为你堆栈的一部分。它们嵌入在你组织的脚本、管道和工作流程中。你无法阻止开发人员使用 LLM。但你可以确保你不是最后一个发现的人。

[
YOUTUBE.COM/THENEWSTACK
技术发展迅速，不要错过任何一集。订阅我们的 YouTube
频道，以流式传输我们所有的播客、访谈、演示等。
](https://youtube.com/thenewstack?sub_confirmation=1)