<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>The app that A.I. made<!-- --> | blog | alexandria redmon</title><meta property="og:title" content="The app that A.I. made | blog | alexandria redmon"/><meta property="og:description" content="blog of alexandria redmon"/><meta property="og:url" content="https://alexredmon.com/blog/the-app-that-ai-made"/><meta property="og:image" content="https://alexredmon.com/api/og?post=the-app-that-ai-made&amp;title=The%20app%20that%20A.I.%20made"/><meta property="og:image:type" content="image/jpeg"/><meta name="next-head-count" content="8"/><link rel="preload" href="/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2" as="font" type="font/woff2" crossorigin="anonymous" data-next-font="size-adjust"/><link rel="preload" href="/_next/static/css/7fbd6ef91808d786.css" as="style"/><link rel="stylesheet" href="/_next/static/css/7fbd6ef91808d786.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-78c92fac7aa8fdd8.js"></script><script src="/_next/static/chunks/webpack-7063c448bd5f7cbd.js" defer=""></script><script src="/_next/static/chunks/framework-b7ba9a8e7304c68b.js" defer=""></script><script src="/_next/static/chunks/main-606f7583eb239011.js" defer=""></script><script src="/_next/static/chunks/pages/_app-35ce2ad02c4b4461.js" defer=""></script><script src="/_next/static/chunks/61-ac5d35ac47d395bd.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bid%5D-6c30806101b761ec.js" defer=""></script><script src="/_next/static/L2lfjCBs1lh_D9M-FNz8J/_buildManifest.js" defer=""></script><script src="/_next/static/L2lfjCBs1lh_D9M-FNz8J/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="__className_aaf875"><div class="
        h-full w-full min-w-screen min-h-screen
        bg-black
        bg-cover
        flex flex-col
        items-center justify-start
        pb-10
      " style="background:url(&#x27;/img/blog/the-app-that-ai-made/bg.jpg&#x27;);background-size:cover;background-color:rgba(0,0,0,.6);background-blend-mode:multiply"><h1 class=" font-paytone-one tracking-tighter text-4xl sm:text-6xl text-white text-left w-full px-4 sm:px-10 pt-10 bg-[#000000cc] "><a class="text-white" href="/">alexandria redmon</a></h1><h2 class=" text-2xl font-paytone-one text-left w-full px-4 sm:px-10 pb-4 bg-[#000000cc] "><a class="text-sky-300" href="/blog">blog</a> <!-- -->&gt;<!-- --> <a class="text-violet-300" href="/blog/the-app-that-ai-made">the app that ai made</a></h2><div class=" my-10 pl-2 sm:pl-24 sm:pl-24 w-full "><div class=" bg-[#00000099] rounded-l-xl p-4 pr-0 text-white "><h2 class=" font-paytone-one text-4xl text-amber-400 ">The app that A.I. made</h2><h3 class=" font-saira text-white pr-2 "><i>Holistic engineering using large language models<!-- -->  <span class="text-[#999999]">-  <!-- -->8 min<!-- --> read</span></i></h3><div class=" font-saira my-4  bg-[#ffffffdd] text-slate-800  p-10 pl-4 pt-2 pr-4 sm:pt-10 sm:pl-10 sm:pr-10 pt-6 overflow-x-auto blog-post pr-10 sm:pr-24 font-taviraj text-lg "><div><h3 id="intro--tldr">Intro / TLDR</h3>
<p>I made the multimodal multitool mobile application <a target="_blank" href="https://www.crayeye.com">CrayEye</a> in a language &#x26; framework with which I'm unfamiliar by relying on modern large language models to write not just snippets but the entirety of the code.  While I later on made minimal tweaks by hand (e.g. changing element colors or swapping their positions), LLMs did all the early &#x26; heavy lifting.</p>
<p>For the purpose of this exercise, I used the latest state-of-the-art models available to me via web UI:</p>
<ul>
<li>OpenAI's <a target="_blank" href="https://cdn.openai.com/papers/GPTV_System_Card.pdf">GPT-4</a></li>
<li>Anthropic's <a target="_blank" href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf">Claude 3 Opus</a></li>
<li>Google's <a target="_blank" href="https://support.google.com/gemini/answer/14517446?hl=en&#x26;co=GENIE.Platform%3DAndroid">Gemini Advanced</a></li>
</ul>
<h3 id="how-it-started">How it Started</h3>
<p>Ever since the early pre-release demos of <a target="_blank" href="https://openai.com/research/gpt-4v-system-card">GPT-4V</a>, imaginations have run wild with what sorts of magical new things we'll be able to build with it.  The implications of the technology for vision based user experience can't be understated - what once took a dedicated team of specialists a substantial amount of time and resources can now be done with even greater proficiency and detail with a trivial call to a multimodal LLM.  I wanted to explore what they were capable of without spinning up <code>create-react-app</code> or <code>create-next-app</code> frontends for every idea and truly explore what <a target="_blank" href="https://www.oneusefulthing.org/">Ethan Mollick</a> calls the <a target="_blank" href="https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged">jagged frontier</a> (i.e. the line between what this tech is and isn't capable of which can shift rapidly even among seemingly adjacent or comparable domains and tasks).</p>
<h3 id="the-multitool">The Multitool</h3>
<p>My requirements were largely:</p>
<ul>
<li>Quick interface for capturing input</li>
<li>Ability to use all cameras with minimal friction</li>
<li>Configurable prompts that could be edited and shared</li>
<li>Incorporation of on-board sensor data (e.g. location) into prompts</li>
</ul>
<p>I decided to create an app.  It had been a while since I'd created a native app and I'd been wanting to kick the tires on the landscape again, and this use case of a multimodal multitool offered the perfect opportunity.</p>
<p>Since my last run at making native apps, <a target="_blank" href="https://flutter.dev/">Flutter</a> had grown in popularity, so I decided to give it a go even though I hadn't worked with <a target="_blank" href="https://dart.dev/">Dart</a> before.  My unfamiliarity with the language was actually going to be useful here, because another thing I wanted to get my feet wet with was testing today's LLMs' capabilities for holistic development.</p>
<h3 id="rubber-meet-road">Rubber, Meet Road</h3>
<h4 id="setup">Setup</h4>
<p>I kicked things off by following <a target="_blank" href="https://flutter.dev/learn">Flutter documentation</a> to set up the <a target="_blank" href="https://docs.flutter.dev/get-started/install/macos/mobile-ios?tab=download">Flutter dev tools for iOS</a> and firing off a <code>flutter create</code>.</p>
<p>At this point, the boilerplate app's core logic was contained entirely in <code>lib/main.dart</code> - this made it particularly easy to start working with immediately.  I began prompting to add simple functionality - a camera preview, a remote HTTP request to analyze the image via GPT, and the app's functionality (and lines of code) quickly began growing.  My prompts leaned heavily on requesting:</p>
<blockquote>
<p>"the complete modified files after these changes with no truncation"</p>
</blockquote>
<p>This was critical because I wanted to both reduce the friction of shuttling responses between the LLM and disk as well as ensure it was fully explicitly accounting for areas of change in context with the rest of the code as it generated its responses.  My evidence is purely anecdotal, but it generally seemed to produce higher quality results with fewer regression issues than letting it pass along patch updates to partial sections of files.</p>
<p><img src="/img/blog/the-app-that-ai-made/claude-1.jpg" alt="alt text" title="Claude Chat Demo"></p>
<p>Shortly after my <a target="_blank" href="https://github.com/alexdredmon/crayeye/commit/76841ef2a105817d7062baf5055147bd8842a27c">initial commit (76841ef)</a> of a minimally functional POC, I had the LLM perform a <a target="_blank" href="https://github.com/alexdredmon/crayeye/commit/624797532388cd24e46041f97914378fcc15bcf2">modular refactor (6247975)</a> to split things into separate files.  This was helpful both from a best practices and workflow performance standpoint, given that I had to wait less for it to output smaller chunks of more modularly split files.</p>
<p>Now that things were in separate modules, I needed to distinguish the different files/modules from each other as I passed the codebase along to the LLM.  At this point, I added comments to the beginning of each file containing its name and an <code>// eof</code> comment at the end.  My prompt looked something like this:</p>
<blockquote>
<p>You are a software development team.  Ask for the codebase if not provided, and base all your changes around this codebase.  When provided with the codebase, reply only with "I'm in" unless a new feature has been requested.</p>
<p>The user will ask for features - respond first with the files that need to be changed followed by a brief summary of the changes. Only output files that require changes to implement the current feature. When you output the files, begin with the filename of the file being updated and then output the entire un-truncated file after modifications. Do not remove any comments. Ensure that the first line of each file is "// FILENAME" where "FILENAME" is the file's name, and that the last line of each file is "// eof" - and most importantly, make sure to surround each file with triple backticks "```" so that they will be entirely formatted as code.</p>
<p>Here's my codebase:</p>
</blockquote>
<p>By saving the prompt in the lib directory prefixed by an <code>_</code> (specifically <code>lib/_autodev_prompt.txt</code>) to ensure it floated to the top of sorted lists of files, I could easily <code>cat lib/* | pbcopy</code> and have my prompt alongside my entire codebase, delimited by filename identifiers, ready to paste in my LLM of choice.  In the running were:</p>
<ul>
<li>OpenAI's <a target="_blank" href="https://cdn.openai.com/papers/GPTV_System_Card.pdf">GPT-4</a></li>
<li>Anthropic's <a target="_blank" href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf">Claude Opus</a></li>
<li>Google's <a target="_blank" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15">Gemini Advanced</a></li>
</ul>
<h4 id="bake-off">Bake Off</h4>
<p>I found that the various providers excelled at different things.  Anthropic's Claude was perfectly suited to the copy-pasta workflow, automatically collapsing a large paste into an attachment style inteface.  ChatGPT and Gemnini neither collapsed nor auto-formatted the code input, leading to a bit messier of a UX out the gate:</p>
<p><img src="/img/blog/the-app-that-ai-made/gpt4-1.jpg" alt="alt text" title="GPT-4 Demo"></p>
<p>Gemini rendered pretty much exactly the same, although it also ultimately ran into a character count constraint at ~31,000 characters.  This was pretty limiting once the app was off the ground.</p>
<p><img src="/img/blog/the-app-that-ai-made/gemini-1.jpg" alt="alt text" title="Gemini Advanced Demo"></p>
<p>Gemini also seemed consistently keen to dive into suggesting changes before any features had been requested, although with some adjustment to the prompt this could be somewhat avoided.</p>
<p><img src="/img/blog/the-app-that-ai-made/gemini-2.jpg" alt="alt text" title="Gemini Advanced Demo"></p>
<p>Claude generally seemed to do the best with the prompt I had at producing complete changes without introducing regression issues, as well as properly responding "I'm in" at the start rather than diving into un-requested changes.  The larger the code base grew, the less this seemed to be the case - I ultimately wound up adding another reminder at the end of my prompt in later requests:</p>
<p><img src="/img/blog/the-app-that-ai-made/claude-2.jpg" alt="alt text" title="Claude Demo"></p>
<p>I frequently began hitting the Claude message throttle, which would reset every ~8hrs - this became my primary bottleneck as the features acculuated and codebase grew.  Claude 3 Opus showed itself to be hands-down the champ at consistently producing complete untruncated files and changes with few to no bugs or regressions.</p>
<p>Eventually, the codebase grew large enough that even Claude Opus would start to suggest changes before any features were described, just as Gemini had done.  It seemed the context window or at least the size of the prompt was the cause here, as it consistently happened over a certain line/character count.</p>
<p>In <a target="_blank" href="https://github.com/alexdredmon/crayeye/commit/5539cfb428770785779f3ba78599ed5972cb1e93">autodev prompt tweaks (5539cfb)</a>, I finally broke the prompt out into two parts - sandwiching the codebase with a <a target="_blank" href="https://github.com/alexdredmon/crayeye/blob/5539cfb428770785779f3ba78599ed5972cb1e93/flutter/lib/_autodev_prompt.txt">before</a> and <a target="_blank" href="https://github.com/alexdredmon/crayeye/blob/5539cfb428770785779f3ba78599ed5972cb1e93/flutter/lib/z_autodev_prompt_end.txt">after</a> prompt.  This seemed to resolve the issues with suggesting changes before features were requested, as well as ensuring more consistent adherence to the "complete files after these changes with no truncations" rule.</p>
<p>With the sandwich prompt in place, it was off to the races again - iterating quickly was again a breeze and feature requests readily turned into code.</p>
<h3 id="minimum-viable-product">Minimum Viable Product</h3>
<h4 id="the-good">The Good</h4>
<p>The MVP, which allowed me to add/edit prompts with location data interpolated in, came out quite usable and useful:</p>
<p><img src="/img/blog/the-app-that-ai-made/iphone-triple.jpg?v=3" alt="alt text" title="iPhone CrayEye Demo 1"></p>
<p>It supported interpolating the user's location values using the tokens <code>{location.lat}</code>, <code>{location.long}</code>, and <code>{location.orienation}</code> to represent their current latitude, longitude, and north/south/east/west orientation at the time of executing the prompt.</p>
<p>I initially assumed I migh need to use an API call like I do for <a target="_blank" href="https://whatsmyhood.com/">WhatsMyHood</a> to interpret the user's neighborhood from their latitude/longitude, but it turned out just providing the raw values to the LLM was sufficient - it was able to work out which neighborhood you were in just as well as Google Maps' API.</p>
<p>There were areas for improvement, such as improving the cramped "Add/Edit Prompt" dialog, but I could easily manage and share my prompts and test them in the field - and even save my favorite responses.</p>
<h4 id="the-bad">The Bad</h4>
<p>I was ready to share my app.  I prepared to test on Android and submit to the Google Play Store and Apple App Store.  That's when I first encountered my first serious setback - after setting up the <a target="_blank" href="https://docs.flutter.dev/get-started/install/macos/mobile-android?tab=download">Android dev tools</a> I fired up <code>flutter emulators</code> and tried to run my app on an Android emulator.  It was a no go - it turned out several packages I was using weren't compatible with my target Android SDK version, and after several attempts of getting an LLM to properly address I finally landed on a solution involving <a target="_blank" href="https://github.com/alexdredmon/crayeye/commit/f18c8b2e276f12b21f7e586f46d7581e34ceb1ed">dropping a dependency (f18c8b2)</a> (and in doing so, support for the <code>{location.orientation}</code> interpolation values in prompts).</p>
<h3 id="takeaways">Takeaways</h3>
<h4 id="advantages">Advantages</h4>
<ul>
<li>I was able to quickly make a feature-complete cross-platform MVP with minimal effort/input thanks to the force multiplier of modern LLMs - the initial MVP took ~10hrs of human work/input.</li>
<li>For my purposes, Claude Opus 3 did far and away the best job of consistently producing functional code without regressions</li>
</ul>
<h4 id="limitations">Limitations</h4>
<ul>
<li>Gemini was limited to building an app beyond ~31k characters (including all prompts)</li>
<li>ChatGPT had the largest tendency to introduce regression errors or ignore instructions to output complete untruncated files</li>
<li>Business limitations outpaced technical ones (i.e. Anthropic message throttle, initial App Store rejection)</li>
</ul>
<h4 id="areas-for-improvement">Areas for Improvement</h4>
<ul>
<li>The workflow utilized could obviously lend to further automation, particularly using autonous agents (e.g. Autogen) to write and test changes with a human in the loop purely for requirements input and validation of acceptance criteria.</li>
<li>Code search and ability to map / use code maps or documentation would be ideal for larger projects</li>
<li>While the MVP took ~10hrs of hands-on input/work, this was spread across a number of days/weekends given the message cap for Claude 3 Opus.  By using the API rather than the web UI or otherwise circumventing message caps, the timeline to delivery could have been accelerated.</li>
</ul>
<h4 id="higher-level-language">Higher Level Language</h4>
<p>Large language models as they're used for code generation can be conceptualized similarly to the latest higher level language for development - just as the existence of Python has not supplanted all development in C, LLMs don't necessarily obviate 100% of lower level language development - even if it undeniably accelerates the ability to execute on said lower level development.</p>
<p>They do however take a big bite - a strikingly large and growing percentage of use cases can be handled exclusively by a modern foundational LLM <em>today</em> and that number is only going to go up.</p>
<h4 id="get-the-app">Get the App</h4>
<p><a target="_blank" href="https://crayeye.com/"><img src="/img/blog/the-app-that-ai-made/crayeye3.png" alt="alt text" title="CrayEye"></a></p>
<ul>
<li><a target="_blank" href="https://www.crayeye.com/">CrayEye.com</a></li>
<li><a target="_blank" href="https://apps.apple.com/us/app/crayeye/id6480090992">Download for iOS</a></li>
<li><a target="_blank" href="https://play.google.com/store/apps/details?id=com.crayeye.app">Download for Android</a></li>
<li><a target="_blank" href="http://github.com/alexdredmon/crayeye">View Source on GitHub</a></li>
</ul>
</div></div></div></div><div class=" font-saira my-4 mt-14 text-xl text-slate-300 w-full text-center plop ">find me on:</div><div class=" flex "><a class="project my-3 mx-4 plop" target="_blank" href="https://github.com/alexdredmon"><img alt="@alexdredmon on GitHub" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=%2Fimg%2Fgh.png&amp;w=64&amp;q=75 1x, /_next/image?url=%2Fimg%2Fgh.png&amp;w=128&amp;q=75 2x" src="/_next/image?url=%2Fimg%2Fgh.png&amp;w=128&amp;q=75"/></a><a class="project my-3 mx-4 plop" target="_blank" href="https://www.linkedin.com/in/alex-redmon-5078865"><img alt="Alexandria Redmon on LinkedIn" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=%2Fimg%2Flinkedin.png&amp;w=64&amp;q=75 1x, /_next/image?url=%2Fimg%2Flinkedin.png&amp;w=128&amp;q=75 2x" src="/_next/image?url=%2Fimg%2Flinkedin.png&amp;w=128&amp;q=75"/></a><a class="project my-3 mx-4 plop" target="_blank" href="https://www.instagram.com/alexdredmon"><img alt="@alexdredmon on Instagram" loading="lazy" width="50" height="50" decoding="async" data-nimg="1" style="color:transparent" srcSet="/_next/image?url=%2Fimg%2Fig.png&amp;w=64&amp;q=75 1x, /_next/image?url=%2Fimg%2Fig.png&amp;w=128&amp;q=75 2x" src="/_next/image?url=%2Fimg%2Fig.png&amp;w=128&amp;q=75"/></a></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":"the-app-that-ai-made","contentHtml":"\u003ch3 id=\"user-content-intro--tldr\"\u003eIntro / TLDR\u003c/h3\u003e\n\u003cp\u003eI made the multimodal multitool mobile application \u003ca href=\"https://www.crayeye.com\"\u003eCrayEye\u003c/a\u003e in a language \u0026#x26; framework with which I'm unfamiliar by relying on modern large language models to write not just snippets but the entirety of the code.  While I later on made minimal tweaks by hand (e.g. changing element colors or swapping their positions), LLMs did all the early \u0026#x26; heavy lifting.\u003c/p\u003e\n\u003cp\u003eFor the purpose of this exercise, I used the latest state-of-the-art models available to me via web UI:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpenAI's \u003ca href=\"https://cdn.openai.com/papers/GPTV_System_Card.pdf\"\u003eGPT-4\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eAnthropic's \u003ca href=\"https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf\"\u003eClaude 3 Opus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGoogle's \u003ca href=\"https://support.google.com/gemini/answer/14517446?hl=en\u0026#x26;co=GENIE.Platform%3DAndroid\"\u003eGemini Advanced\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"user-content-how-it-started\"\u003eHow it Started\u003c/h3\u003e\n\u003cp\u003eEver since the early pre-release demos of \u003ca href=\"https://openai.com/research/gpt-4v-system-card\"\u003eGPT-4V\u003c/a\u003e, imaginations have run wild with what sorts of magical new things we'll be able to build with it.  The implications of the technology for vision based user experience can't be understated - what once took a dedicated team of specialists a substantial amount of time and resources can now be done with even greater proficiency and detail with a trivial call to a multimodal LLM.  I wanted to explore what they were capable of without spinning up \u003ccode\u003ecreate-react-app\u003c/code\u003e or \u003ccode\u003ecreate-next-app\u003c/code\u003e frontends for every idea and truly explore what \u003ca href=\"https://www.oneusefulthing.org/\"\u003eEthan Mollick\u003c/a\u003e calls the \u003ca href=\"https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged\"\u003ejagged frontier\u003c/a\u003e (i.e. the line between what this tech is and isn't capable of which can shift rapidly even among seemingly adjacent or comparable domains and tasks).\u003c/p\u003e\n\u003ch3 id=\"user-content-the-multitool\"\u003eThe Multitool\u003c/h3\u003e\n\u003cp\u003eMy requirements were largely:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eQuick interface for capturing input\u003c/li\u003e\n\u003cli\u003eAbility to use all cameras with minimal friction\u003c/li\u003e\n\u003cli\u003eConfigurable prompts that could be edited and shared\u003c/li\u003e\n\u003cli\u003eIncorporation of on-board sensor data (e.g. location) into prompts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI decided to create an app.  It had been a while since I'd created a native app and I'd been wanting to kick the tires on the landscape again, and this use case of a multimodal multitool offered the perfect opportunity.\u003c/p\u003e\n\u003cp\u003eSince my last run at making native apps, \u003ca href=\"https://flutter.dev/\"\u003eFlutter\u003c/a\u003e had grown in popularity, so I decided to give it a go even though I hadn't worked with \u003ca href=\"https://dart.dev/\"\u003eDart\u003c/a\u003e before.  My unfamiliarity with the language was actually going to be useful here, because another thing I wanted to get my feet wet with was testing today's LLMs' capabilities for holistic development.\u003c/p\u003e\n\u003ch3 id=\"user-content-rubber-meet-road\"\u003eRubber, Meet Road\u003c/h3\u003e\n\u003ch4 id=\"user-content-setup\"\u003eSetup\u003c/h4\u003e\n\u003cp\u003eI kicked things off by following \u003ca href=\"https://flutter.dev/learn\"\u003eFlutter documentation\u003c/a\u003e to set up the \u003ca href=\"https://docs.flutter.dev/get-started/install/macos/mobile-ios?tab=download\"\u003eFlutter dev tools for iOS\u003c/a\u003e and firing off a \u003ccode\u003eflutter create\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAt this point, the boilerplate app's core logic was contained entirely in \u003ccode\u003elib/main.dart\u003c/code\u003e - this made it particularly easy to start working with immediately.  I began prompting to add simple functionality - a camera preview, a remote HTTP request to analyze the image via GPT, and the app's functionality (and lines of code) quickly began growing.  My prompts leaned heavily on requesting:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\"the complete modified files after these changes with no truncation\"\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis was critical because I wanted to both reduce the friction of shuttling responses between the LLM and disk as well as ensure it was fully explicitly accounting for areas of change in context with the rest of the code as it generated its responses.  My evidence is purely anecdotal, but it generally seemed to produce higher quality results with fewer regression issues than letting it pass along patch updates to partial sections of files.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/blog/the-app-that-ai-made/claude-1.jpg\" alt=\"alt text\" title=\"Claude Chat Demo\"\u003e\u003c/p\u003e\n\u003cp\u003eShortly after my \u003ca href=\"https://github.com/alexdredmon/crayeye/commit/76841ef2a105817d7062baf5055147bd8842a27c\"\u003einitial commit (76841ef)\u003c/a\u003e of a minimally functional POC, I had the LLM perform a \u003ca href=\"https://github.com/alexdredmon/crayeye/commit/624797532388cd24e46041f97914378fcc15bcf2\"\u003emodular refactor (6247975)\u003c/a\u003e to split things into separate files.  This was helpful both from a best practices and workflow performance standpoint, given that I had to wait less for it to output smaller chunks of more modularly split files.\u003c/p\u003e\n\u003cp\u003eNow that things were in separate modules, I needed to distinguish the different files/modules from each other as I passed the codebase along to the LLM.  At this point, I added comments to the beginning of each file containing its name and an \u003ccode\u003e// eof\u003c/code\u003e comment at the end.  My prompt looked something like this:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eYou are a software development team.  Ask for the codebase if not provided, and base all your changes around this codebase.  When provided with the codebase, reply only with \"I'm in\" unless a new feature has been requested.\u003c/p\u003e\n\u003cp\u003eThe user will ask for features - respond first with the files that need to be changed followed by a brief summary of the changes. Only output files that require changes to implement the current feature. When you output the files, begin with the filename of the file being updated and then output the entire un-truncated file after modifications. Do not remove any comments. Ensure that the first line of each file is \"// FILENAME\" where \"FILENAME\" is the file's name, and that the last line of each file is \"// eof\" - and most importantly, make sure to surround each file with triple backticks \"```\" so that they will be entirely formatted as code.\u003c/p\u003e\n\u003cp\u003eHere's my codebase:\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eBy saving the prompt in the lib directory prefixed by an \u003ccode\u003e_\u003c/code\u003e (specifically \u003ccode\u003elib/_autodev_prompt.txt\u003c/code\u003e) to ensure it floated to the top of sorted lists of files, I could easily \u003ccode\u003ecat lib/* | pbcopy\u003c/code\u003e and have my prompt alongside my entire codebase, delimited by filename identifiers, ready to paste in my LLM of choice.  In the running were:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpenAI's \u003ca href=\"https://cdn.openai.com/papers/GPTV_System_Card.pdf\"\u003eGPT-4\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eAnthropic's \u003ca href=\"https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf\"\u003eClaude Opus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGoogle's \u003ca href=\"https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/#gemini-15\"\u003eGemini Advanced\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"user-content-bake-off\"\u003eBake Off\u003c/h4\u003e\n\u003cp\u003eI found that the various providers excelled at different things.  Anthropic's Claude was perfectly suited to the copy-pasta workflow, automatically collapsing a large paste into an attachment style inteface.  ChatGPT and Gemnini neither collapsed nor auto-formatted the code input, leading to a bit messier of a UX out the gate:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/blog/the-app-that-ai-made/gpt4-1.jpg\" alt=\"alt text\" title=\"GPT-4 Demo\"\u003e\u003c/p\u003e\n\u003cp\u003eGemini rendered pretty much exactly the same, although it also ultimately ran into a character count constraint at ~31,000 characters.  This was pretty limiting once the app was off the ground.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/blog/the-app-that-ai-made/gemini-1.jpg\" alt=\"alt text\" title=\"Gemini Advanced Demo\"\u003e\u003c/p\u003e\n\u003cp\u003eGemini also seemed consistently keen to dive into suggesting changes before any features had been requested, although with some adjustment to the prompt this could be somewhat avoided.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/blog/the-app-that-ai-made/gemini-2.jpg\" alt=\"alt text\" title=\"Gemini Advanced Demo\"\u003e\u003c/p\u003e\n\u003cp\u003eClaude generally seemed to do the best with the prompt I had at producing complete changes without introducing regression issues, as well as properly responding \"I'm in\" at the start rather than diving into un-requested changes.  The larger the code base grew, the less this seemed to be the case - I ultimately wound up adding another reminder at the end of my prompt in later requests:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/blog/the-app-that-ai-made/claude-2.jpg\" alt=\"alt text\" title=\"Claude Demo\"\u003e\u003c/p\u003e\n\u003cp\u003eI frequently began hitting the Claude message throttle, which would reset every ~8hrs - this became my primary bottleneck as the features acculuated and codebase grew.  Claude 3 Opus showed itself to be hands-down the champ at consistently producing complete untruncated files and changes with few to no bugs or regressions.\u003c/p\u003e\n\u003cp\u003eEventually, the codebase grew large enough that even Claude Opus would start to suggest changes before any features were described, just as Gemini had done.  It seemed the context window or at least the size of the prompt was the cause here, as it consistently happened over a certain line/character count.\u003c/p\u003e\n\u003cp\u003eIn \u003ca href=\"https://github.com/alexdredmon/crayeye/commit/5539cfb428770785779f3ba78599ed5972cb1e93\"\u003eautodev prompt tweaks (5539cfb)\u003c/a\u003e, I finally broke the prompt out into two parts - sandwiching the codebase with a \u003ca href=\"https://github.com/alexdredmon/crayeye/blob/5539cfb428770785779f3ba78599ed5972cb1e93/flutter/lib/_autodev_prompt.txt\"\u003ebefore\u003c/a\u003e and \u003ca href=\"https://github.com/alexdredmon/crayeye/blob/5539cfb428770785779f3ba78599ed5972cb1e93/flutter/lib/z_autodev_prompt_end.txt\"\u003eafter\u003c/a\u003e prompt.  This seemed to resolve the issues with suggesting changes before features were requested, as well as ensuring more consistent adherence to the \"complete files after these changes with no truncations\" rule.\u003c/p\u003e\n\u003cp\u003eWith the sandwich prompt in place, it was off to the races again - iterating quickly was again a breeze and feature requests readily turned into code.\u003c/p\u003e\n\u003ch3 id=\"user-content-minimum-viable-product\"\u003eMinimum Viable Product\u003c/h3\u003e\n\u003ch4 id=\"user-content-the-good\"\u003eThe Good\u003c/h4\u003e\n\u003cp\u003eThe MVP, which allowed me to add/edit prompts with location data interpolated in, came out quite usable and useful:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/blog/the-app-that-ai-made/iphone-triple.jpg?v=3\" alt=\"alt text\" title=\"iPhone CrayEye Demo 1\"\u003e\u003c/p\u003e\n\u003cp\u003eIt supported interpolating the user's location values using the tokens \u003ccode\u003e{location.lat}\u003c/code\u003e, \u003ccode\u003e{location.long}\u003c/code\u003e, and \u003ccode\u003e{location.orienation}\u003c/code\u003e to represent their current latitude, longitude, and north/south/east/west orientation at the time of executing the prompt.\u003c/p\u003e\n\u003cp\u003eI initially assumed I migh need to use an API call like I do for \u003ca href=\"https://whatsmyhood.com/\"\u003eWhatsMyHood\u003c/a\u003e to interpret the user's neighborhood from their latitude/longitude, but it turned out just providing the raw values to the LLM was sufficient - it was able to work out which neighborhood you were in just as well as Google Maps' API.\u003c/p\u003e\n\u003cp\u003eThere were areas for improvement, such as improving the cramped \"Add/Edit Prompt\" dialog, but I could easily manage and share my prompts and test them in the field - and even save my favorite responses.\u003c/p\u003e\n\u003ch4 id=\"user-content-the-bad\"\u003eThe Bad\u003c/h4\u003e\n\u003cp\u003eI was ready to share my app.  I prepared to test on Android and submit to the Google Play Store and Apple App Store.  That's when I first encountered my first serious setback - after setting up the \u003ca href=\"https://docs.flutter.dev/get-started/install/macos/mobile-android?tab=download\"\u003eAndroid dev tools\u003c/a\u003e I fired up \u003ccode\u003eflutter emulators\u003c/code\u003e and tried to run my app on an Android emulator.  It was a no go - it turned out several packages I was using weren't compatible with my target Android SDK version, and after several attempts of getting an LLM to properly address I finally landed on a solution involving \u003ca href=\"https://github.com/alexdredmon/crayeye/commit/f18c8b2e276f12b21f7e586f46d7581e34ceb1ed\"\u003edropping a dependency (f18c8b2)\u003c/a\u003e (and in doing so, support for the \u003ccode\u003e{location.orientation}\u003c/code\u003e interpolation values in prompts).\u003c/p\u003e\n\u003ch3 id=\"user-content-takeaways\"\u003eTakeaways\u003c/h3\u003e\n\u003ch4 id=\"user-content-advantages\"\u003eAdvantages\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eI was able to quickly make a feature-complete cross-platform MVP with minimal effort/input thanks to the force multiplier of modern LLMs - the initial MVP took ~10hrs of human work/input.\u003c/li\u003e\n\u003cli\u003eFor my purposes, Claude Opus 3 did far and away the best job of consistently producing functional code without regressions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"user-content-limitations\"\u003eLimitations\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eGemini was limited to building an app beyond ~31k characters (including all prompts)\u003c/li\u003e\n\u003cli\u003eChatGPT had the largest tendency to introduce regression errors or ignore instructions to output complete untruncated files\u003c/li\u003e\n\u003cli\u003eBusiness limitations outpaced technical ones (i.e. Anthropic message throttle, initial App Store rejection)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"user-content-areas-for-improvement\"\u003eAreas for Improvement\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eThe workflow utilized could obviously lend to further automation, particularly using autonous agents (e.g. Autogen) to write and test changes with a human in the loop purely for requirements input and validation of acceptance criteria.\u003c/li\u003e\n\u003cli\u003eCode search and ability to map / use code maps or documentation would be ideal for larger projects\u003c/li\u003e\n\u003cli\u003eWhile the MVP took ~10hrs of hands-on input/work, this was spread across a number of days/weekends given the message cap for Claude 3 Opus.  By using the API rather than the web UI or otherwise circumventing message caps, the timeline to delivery could have been accelerated.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"user-content-higher-level-language\"\u003eHigher Level Language\u003c/h4\u003e\n\u003cp\u003eLarge language models as they're used for code generation can be conceptualized similarly to the latest higher level language for development - just as the existence of Python has not supplanted all development in C, LLMs don't necessarily obviate 100% of lower level language development - even if it undeniably accelerates the ability to execute on said lower level development.\u003c/p\u003e\n\u003cp\u003eThey do however take a big bite - a strikingly large and growing percentage of use cases can be handled exclusively by a modern foundational LLM \u003cem\u003etoday\u003c/em\u003e and that number is only going to go up.\u003c/p\u003e\n\u003ch4 id=\"user-content-get-the-app\"\u003eGet the App\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://crayeye.com/\"\u003e\u003cimg src=\"/img/blog/the-app-that-ai-made/crayeye3.png\" alt=\"alt text\" title=\"CrayEye\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.crayeye.com/\"\u003eCrayEye.com\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://apps.apple.com/us/app/crayeye/id6480090992\"\u003eDownload for iOS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://play.google.com/store/apps/details?id=com.crayeye.app\"\u003eDownload for Android\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://github.com/alexdredmon/crayeye\"\u003eView Source on GitHub\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n","title":"The app that A.I. made","date":"2024-04-13","description":"Holistic engineering using large language models","published":true,"estimate":"8 min"}},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"the-app-that-ai-made"},"buildId":"L2lfjCBs1lh_D9M-FNz8J","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>