
<!--
title: 智领AI未来：打造更精简高效的存储基石
cover: https://cdn.thenewstack.io/media/2025/10/7dcca862-datacenter.jpg
summary: AI基础设施需可扩展、高效、成本效益。训练推荐多挂载块存储，简化管理。推理优化KV缓存，采用NVMe闪存和解耦架构，提升效率。
-->

AI基础设施需可扩展、高效、成本效益。训练推荐多挂载块存储，简化管理。推理优化KV缓存，采用NVMe闪存和解耦架构，提升效率。

> 译自：[Leaner, More Efficient Storage Infrastructure for the AI Era](https://thenewstack.io/leaner-more-efficient-storage-infrastructure-for-the-ai-era/)
> 
> 作者：Carol Platz

AI时代需要一种简单的基础设施策略，优先考虑AI数据管道管理中的可扩展性、性能和成本效益。一个关键挑战是支持大型语言模型（LLM）训练，这需要海量数据、计算和存储资源。高效的训练依赖于大型数据集的持续供给以及模型参数、中间结果和检查点的存储。最重要的是，基础设施策略必须确保AI资源是可扩展的、可靠的且具有成本效益的。

## 扩展AI训练基础设施

随着模型的增长，对[AI存储](https://www.lightbitslabs.com/solutions/ai-cloud-data-platform-lightbits/?utm_source=TNS&utm_medium=article&utm_campaign=oct)的需求也随之增加，使得高效的数据管理在整个训练管道中至关重要。AI训练，尤其是针对大型模型，涉及将工作负载分布到多个节点以加速处理，这需要访问相同的数据集。许多组织使用并行文件系统将数据分布到存储节点。然而，这种方法可能导致基础设施蔓延、成本增加和操作复杂性——这些挑战直接影响DevOps团队。

### 多挂载块存储：一个简单的解决方案

一个更精简且具成本效益的解决方案涉及使用一个本地文件系统，该文件系统以只读方式挂载到多个节点。这通过将数据集存储在具有多挂载功能的[高性能块存储](https://www.lightbitslabs.com/product/)系统上实现。块存储卷同时连接到多个节点，并以只读模式挂载为本地文件系统。

优势：

*   降低了管理大型分布式文件系统的复杂性。
*   通过利用块存储的多挂载功能，轻松扩展存储容量和性能。
*   优化资源利用率并降低基础设施成本。
*   支持基础设施即代码实践，用于调配和管理存储资源。

这种方法为处理AI训练存储提供了更具可扩展性和效率的方法，与现代基础设施需求和[DevOps原则](https://thenewstack.io/devops/ "DevOps原则")保持一致。

## 推理：下一个前沿

虽然AI训练通常是集中式的，但推理却经常部署在不同的环境中。现代推理涉及复杂的多步骤过程，增加了对内存、计算和快速数据访问的需求。

推理中的挑战：

*   大规模部署和管理推理需要强大的基础设施，能够处理波动的工作负载和低延迟要求。
*   平衡性能与成本效益至关重要，既要避免过度配置，又要满足实时需求。
*   推理工作负载通常不可预测，需要自动化和监控来处理需求高峰。

### 键值缓存和存储优化

LLM的推理严重依赖键值（KV）缓存来存储中间结果，减少冗余计算并提高资源效率。然而，将KV缓存存储在GPU或主机内存中存在容量和可扩展性方面的限制。

### 可扩展推理的存储解决方案

为了克服这些限制，将KV缓存存储扩展到高性能存储解决方案至关重要。

*   NVMe闪存：为KV缓存存储提供经济高效、高容量的层级，减轻GPU和内存的限制。基础设施团队可以利用[NVMe over TCP](https://www.lightbitslabs.com/nvme-over-tcp/)（即基于标准以太网的NVME）来构建可扩展且高性能的推理基础设施。
*   解耦的KV缓存架构：将KV缓存分布在NVMe闪存上并使其可从任何GPU服务器访问，这对于实现最佳性能至关重要。
*   缓存分层：智能管理GPU RAM、主机RAM、本地SSD和解耦SSD之间的缓存层次结构是关键。

### LLM蒸馏和高效部署

LLM蒸馏，即创建更小、更快的模型，是实现高效部署的重要趋势。将KV缓存卸载到远程存储可以进一步释放本地内存，使蒸馏模型能够处理更长的上下文并更有效地扩展。这对于在边缘或资源受限的云环境中部署AI的基础设施团队尤其重要。

通过专注于这些存储优化策略并采用DevOps实践，组织可以构建可扩展、经济高效且节能的AI基础设施，以支持训练和推理工作负载。

如果您有兴趣了解更多关于AI和机器学习工作负载的基础设施优化，请阅读我们的博客，“[停止在AI存储上浪费金钱：一种更智能、更精简的方法](https://www.lightbitslabs.com/blog/stop-wasting-money-on-ai-storage-a-smarter-leaner-approach/?utm_source=TNS&utm_medium=article&utm_campaign=oct)。”