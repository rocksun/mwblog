# Why Metadata and Fine-Tuning Are Key To Scaling NLQ to SQL
![Featued image for: Why Metadata and Fine-Tuning Are Key To Scaling NLQ to SQL](https://cdn.thenewstack.io/media/2024/10/48bddca9-kerde-severin-4ghhdwjwuak-unsplash-1024x683.jpg)
[Kerde Severin](https://unsplash.com/@kseverin?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)on
[Unsplash](https://unsplash.com/photos/turned-on-macbook-4ghHDwjwUAk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
A significant amount of data is stored in structured data across hundreds of thousands of organizations. Millions of business users use this structured data daily to run the business, deriving insights that can help improve its running. The most popular way to store this structured data is using MySQL. The other popular relational databases are PostgreSQL, Microsoft SQL Server, and Oracle Database. It is estimated that around 60% to 70% of business users do not have the [technical expertise to write](https://thenewstack.io/a-software-developers-guide-to-technical-writing/) the queries that will allow them to interact with relational databases effectively.

One of the most common [applications of large language models](https://thenewstack.io/building-llm-based-genai-applications-with-amazon-bedrock/) (LLMs) is enabling business users to ask their queries as natural language queries (NLQs) and convert them into SQL queries that can be executed. The overall solution will then render the results back to the business user. LLMs generate charts and summaries that the business user can easily consume along with the results.

An example of an NQL is “What are the top five sub-brands by sales revenue?” The LLM will use the input query along with the underlying table metadata to produce the appropriate SQL query:

Converted SQL Query: “SELECT sub_brand, sum(net_sales) AS total_sales FROM Sales_Details
_Table where item_quantity > 0 group by sub_brand order by total_sales DESC LIMIT 5”

Now, let’s explore the top challenges you are likely to face when scaling NLQ to SQL implementations based on my personal experience working with Fortune 50 companies over the past two years. We’ll also discuss actionable solutions to help you overcome these obstacles.

**Challenge 1: Cost Shoots Up Tremendously With Thousands of Tables**
For an NLQ to be converted into SQL, the input for an LLM is the input question, along with the metadata of the tables. The metadata typically describes the columns in the table. They are independent of the number of rows in the table, hence the size of the table in terms of rows does not impact the number of tokens passed on to the LLM. However, imagine having thousands of tables and passing the metadata for these thousands of tables for every question. The number of tokens consumed by the LLM and the associated costs explode.

To mitigate this issue, we implemented a table selection strategy. This classifier takes in a query and table pair and determines if the table can answer the given question. When a question is asked, this table selection module is run, and only the top five to 10 relevant table metadata, along with the query, is passed on to the LLM. The LLM uses this information to come up with the appropriate SQL query. Since we reduce the number of tokens passed to the LLM, the associated cost also comes down.

**Challenge 2: There Is No Single Outperformer LLM, but Prompting Helps**
We have [worked on various RDBMS systems](https://thenewstack.io/putting-ai-to-work-systems-of-intelligence-and-actionable-agency/), and there is no clear winner in terms of the best-performing LLM. A GPT-X model is a great baseline to start with. However, depending on business requirements (for example, an organization has GCP as its preferred partner), the choice of models varies a lot. Instead of choosing a pre-trained general model, selecting a domain-specific version of these models is better. We see clear outperformance of these models over their counterparts (for example, CodeLLama vs. Llama and CodeGemma vs. Gemma).

Independent of the model chosen, prompting helps [achieve a ~10% to 20% improvement in performance](https://thenewstack.io/5-tips-to-achieve-performance-engineering-at-scale/). Below is an example of a prompt for NLQ to SQL: “Column names in the table might not directly match the information asked in the question. You will need to select columns using the provided column descriptions, and you might also need to derive new columns using existing columns, depending on the user’s question, by doing some mathematical operations.” Proper model selection combined with effective prompting is critical to good performance.

**Challenge 3: Hallucination Occurs in NLQ to SQL**
Hallucination in answering questions by GenAI models is a well-known and studied phenomenon. However, hallucination generated by LLMs when generating SQL queries from NLQ is an under-studied topic. Here is an example of an error when generating SQL:

Original NLQ | Generated SQL Query | Correct SQL Query |
---|---|---|
Find all projects that have been around for more than six months and the employees assigned to each project. | select t1.project_name, t1.employee_id from projects as t1 join employees as t2 on t1.employee_id = t2.employee_id where t1.end_date > 6 months | SELECT p.project_name, e.employee_id FROM projects p LEFT JOIN employees e ON p.employee_id = e.employee_id WHERE DATEDIFF(p.end_date, p.start_date) > 180 |
For this example, there are two tables: Projects and Employees. For the given query, a join needs to be on the employee ID, and the project’s duration must be over six months. The LLM-generated query, as shown, gets the duration calculation incorrect. Another example of [hallucination is when the LLM](https://thenewstack.io/3-ways-to-stop-llm-hallucinations/) cannot determine the right column to make up a column name in the table.

To address hallucination, consider the following methods:

- Create a test set of queries, iteratively examine the results, and improve the prompt to ensure that hallucination does not occur.
- When a column name is made up, create explicit instructions in the prompt not to make up column names if they do not exist.
- Fine-tune the model (with a few hundred examples) so that the LLM gets familiar with the schema and the business logic of how the
[SQL queries need](https://thenewstack.io/sql-and-complex-queries-are-needed-for-real-time-analytics/)to be formed. - In our experiments, fine-tuning reduces hallucination by up to 10%.
**Challenge 4: Simple Evaluation Metrics Are Insufficient**
A traditional way of evaluating the queries by their correctness is not sufficient with NLQ to SQL. Creating datasets of varying complexity is essential, such as easy, medium, and hard. In the table below, we have given one sample definition for each dataset category. These definitions should be tweaked based on the underlying schema, business queries, etc.

The motivation for creating the datasets is that, depending on the complexity of queries from business users, it might be sufficient for the model to achieve very high accuracy only in the easy and medium categories. This ensures that the solution can be used without achieving high accuracy.

Question Type | Definition |
---|---|
Easy |
- Simple SELECT statements with 0, 1, or 2 WHERE conditions using OR and AND
|
Medium |
- GROUP BY with more than 1 WHERE or HAVING conditions
- SELECT statements with more than 2 WHERE conditions
|
Hard |
- Nested queries
- CTEs
- GROUP BY on multiple columns
- DATE operations
|
In addition to accuracy, consistency is another metric that should be included. Consistency is defined as the ability of the model to produce the same results for a given NLQ. It is important to note that the SQL query might be different, but the result will remain the same. Testing that the SQL query remains the same will be interesting. Finally, another metric to evaluate is the efficiency of the SQL query generated.

**Challenge 5: Metadata Is Available Only for ~10% of the Tables**
A primary driver for the quality of the solution built is the quality and coverage of the metadata for the underlying tables. As we work with various enterprises in building solutions, a chief challenge is the lack of metadata for the tables.

Different approaches to tackle this:

- Identify the tables most frequently used by business users and ensure that metadata is manually created for these tables.
- Create metadata in an automated fashion.
- Create the draft of the metadata for all the tables using a GenAI system that examines a few randomly sampled rows from each table.
- Have an expert examine and edit the metadata to ensure they provide the right description for the columns in the table.
This article summarizes the top five challenges in implementing a production-grade enterprise-level NLQ to SQL system. It is possible to achieve over 90% accuracy using these systems consistently. To achieve this level of performance, it is important to experiment with different LLM models, fine-tune these models, and ensure that the underlying tables have metadata descriptions.

Additionally, having suitable datasets for evaluation with the right metrics is crucial in measuring the system’s quality. Finally, managing costs by ensuring that the correct metadata is shared with queries will help organizations derive sufficient return on investment from the implemented system.

*This article is part of The New Stack’s contributor network. Have insights on the latest challenges and innovations affecting developers? We’d love to hear from you. Become a contributor and share your expertise by filling out this form or emailing Matt Burns at mattburns@thenewstack.io.*
[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)