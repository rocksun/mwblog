
<!--
title: 如何处理事件流中的不良数据
cover: https://cdn.thenewstack.io/media/2024/10/6cdbca9d-handle-bad-data-scaled.jpg
-->

Apache Kafka 主题是不可变的，因此您无法编辑或删除其数据。但是，您可以采取一些措施来修复事件流中的错误数据。

> 译自 [How to Handle Bad Data in Event Streams](https://thenewstack.io/how-to-handle-bad-data-in-event-streams/)，作者 Adam Bellemare。

最近的一项[Gartner 调查](https://www.gartner.com/smarterwithgartner/how-to-improve-your-data-quality)发现，数据质量差每年会给组织造成平均 1290 万美元的损失，并可能增加数据生态系统的复杂性。“不良数据”被定义为不符合开发人员期望的损坏或格式错误的数据。它会为数据科学家、分析师、机器学习、人工智能和其他数据从业者造成中断和其他破坏性影响。

[Apache Kafka 主题](https://developer.confluent.io/courses/apache-kafka/topics/)是[不可变的](https://thenewstack.io/a-brief-look-at-immutable-infrastructure-and-why-it-is-such-a-quest/)。一旦事件被写入事件流，就不能编辑或删除。这种设计权衡确保每个数据消费者最终都会获得完全相同的副本，并且数据在读取后不会被编辑或更改。但是，由于不良数据一旦写入流就不能编辑，因此必须防止[不良数据](https://thenewstack.io/a-call-to-use-generative-ai-to-create-more-trustworthy-data/)进入[流](https://thenewstack.io/stream-processing-101-whats-right-for-you/)。但是，如果不良数据确实进入了流，即使您无法就地编辑它，也可以做一些事情。

以下四个技巧可以帮助您有效地防止和修复事件流中的不良数据。

## 1. 使用模式来防止不良数据进入

[模式](https://thenewstack.io/sql-schema-generation-with-large-language-models/)明确定义了事件中应该和不应该包含哪些数据，包括字段名称、类型、默认值、可接受值的范围以及人类可读的文档。事件流的流行模式技术包括[Avro](https://avro.apache.org/)、[Protobuf](https://protobuf.dev/)和[JSON Schema](https://json-schema.org/)。

模式通过防止生产者写入不良数据来显着减少数据错误。如果数据不符合模式，应用程序将抛出异常并让模式知道。模式允许消费者专注于使用数据，而不是尽力尝试解析生产者的实际含义。

定义明确的显式模式对于确保明确的含义非常重要。在事件驱动的系统中，通常会有不同的独立消费者读取同一个主题。

![](https://cdn.thenewstack.io/media/2024/10/238b7fa9-consumers-data-interpretation.png)

在上图中，消费者有八种可能的机会错误地解释来自事件流的数据。消费者和主题越多，他们错误地解释数据与同行相比的可能性就越大，除非您使用明确定义的显式模式。

风险在于您的消费者对数据的解释略微不同，导致计算和结果彼此偏离。这会导致大量努力来协调哪个系统正在错误地解释数据。相反，通过使用模式来消除这种可能性。

## 2. 使用您的应用程序测试您的模式

测试对于防止不良数据进入您的流至关重要。虽然来自生产服务的运行时异常可能会阻止不良数据进入流，但它可能会降低依赖该服务的其他应用程序和用户的体验。

模式为您提供了一切所需来模拟测试数据以测试您的代码。您的生产者服务测试可以执行所有代码路径，以确保它们只创建格式正确的事件。同时，您的消费者应用程序可以针对相同的模式编写所有业务逻辑和测试，这样当它们接收和处理事件时，它们就不会抛出任何异常或错误计算结果。

测试集成到您的 CI/CD 管道中，以便您可以在部署应用程序和服务之前验证您的代码和模式是否协同工作。您还可以将 CI/CD 管道集成到您的模式注册表中，以验证您的模式与最新的模式，以确保您的应用程序与所有依赖的模式兼容，以防您错过了演变或更新。

## 3. 优先考虑事件设计

尽管努力防止不良数据进入流，但有时一个错字就足以破坏输入。事件设计在防止事件流中的不良数据方面发挥着另一个关键作用。经过深思熟虑的事件设计可以允许进行更正，例如通过发布具有正确数据的新的记录来覆盖以前的不良数据。在应用程序开发阶段优先考虑仔细、有意的事件设计可以显着缓解与不良数据修复相关的問題。

状态事件（也称为事件携带状态传输）提供了在给定时间点实体的完整视图。增量事件仅提供与先前增量事件的更改。下图显示增量事件类似于国际象棋中的移动，而状态事件则显示棋盘的完整当前状态。

![](https://cdn.thenewstack.io/media/2024/10/39697508-delta-vs-state-events-1024x561.png)

状态事件可以简化纠正先前发布的错误数据的过程。您只需发布一个包含更新的正确状态的新状态事件。然后，您可以使用 [压缩](https://docs.confluent.io/kafka/design/log_compaction.html) 来（异步）删除旧的错误数据。每个消费者都会收到正确状态的副本，并且可以通过将其与他们可能存储在其域边界中的任何先前状态进行比较来处理和推断其更改。

虽然增量提供较小的事件大小，但您无法将其压缩掉。您能做的最好的事情是发布一个撤消先前增量的增量，但问题是所有消费者都必须能够处理撤消事件。挑战在于有很多方法可以产生错误的增量（例如，非法移动，一名玩家连续移动几回合），并且每个撤消事件都必须是精确的修复。现实情况是，在任何有意义的规模上做到这一点都非常困难，并且您仍然会在事件流中保留所有先前的错误数据；如果您选择使用增量，您就无法清理它。

事件设计允许纠正错误，而无需删除所有内容并从头开始。但是，只有状态事件提供发出更正（包含已修复总状态的新事件）和删除旧的错误数据（压缩）的方法。

## 4. 万不得已，回溯、重建和重试

在数据流的世界中，预防永远胜于补救。作为最后的手段，请准备好深入研究事件流。虽然此过程可以应用于任何包含错误数据的主题——无论是状态、增量还是混合——但它很费力且容易出错。谨慎行事。

从外部来源重建数据需要搜索错误数据并生成包含已修复数据的新的流。您必须回溯到流程的开始并暂停消费者和生产者。之后，您可以修复并将数据重写到另一个流中，您最终将在其中迁移所有参与方。

虽然这种昂贵且复杂的解决方案应该是最后的手段，但它是您武器库中必不可少的策略。

## 降低错误数据的影响

处理事件流中的错误数据并不一定是一项艰巨的任务。通过了解错误数据的性质，防止其进入您的事件流，利用事件设计来覆盖错误数据，以及在必要时准备好回溯、重建和重试，您可以有效地降低错误数据的影响。良好的数据实践不仅可以节省时间和精力，还可以让您完成工作。
