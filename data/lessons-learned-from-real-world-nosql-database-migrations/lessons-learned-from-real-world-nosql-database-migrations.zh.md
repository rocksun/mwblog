在“[经过实战检验的NoSQL迁移秘诀](https://thenewstack.io/what-matters-most-for-nosql-migrations/)”一文中，我分享了我规划、执行NoSQL数据库迁移以及降低其风险的顶级策略。我讨论了关键步骤，例如模式和数据迁移、数据验证，以及技术切换、工具、边缘情况以及您可能不需要迁移所有数据的想法等重要考量。

现在，让我们分析团队如何实际迁移他们的数据——他们面临了哪些挑战、权衡取舍、他们如何进行以及吸取的教训。这些都是真实世界的案例，其中人名和识别细节已被模糊处理。

## 流式批量加载（DynamoDB 到 ScyllaDB）

第一个例子：一家大型媒体流公司决定从DynamoDB切换到ScyllaDB以降低成本。

这个用例的一个有趣之处在于，该团队有一个每日覆盖整个数据集的摄取过程。因此，没有必要将他们的数据从一个数据库整体迁移到另一个数据库。他们只需配置他们的摄取作业，使其除了写入DynamoDB外，也写入ScyllaDB。

[![](https://cdn.thenewstack.io/media/2026/01/b4c4b250-image1.png)](https://cdn.thenewstack.io/media/2026/01/b4c4b250-image1.png)

一旦作业启动，数据就会存储在两个数据库中。由于DynamoDB和ScyllaDB的数据模型非常相似，这极大地简化了过程。如果从文档存储或关系型数据库切换到宽列NoSQL，情况会更复杂。

正如我在上一篇文章中提到的，从一种技术迁移到另一种技术几乎总是需要进行一些更改。即使是相似的数据库，其功能和内部工作原理也各不相同。该团队的一些迁移顾虑与ScyllaDB处理乱序写入的方式、他们将如何实现记录版本控制以及数据压缩的效率有关。这些都是有效且有趣的顾虑。

这次迁移的主要教训是需要了解源数据库和目标数据库之间的差异。即使是许多方面都非常相似的数据库，例如ScyllaDB和DynamoDB，也确实存在您需要识别和应对的差异。当您探索这些差异时，您最终可能会发现改进的空间，这里发生的就是这种情况。

这个用例非常容易受到乱序写入的影响。在我们解释他们如何解决这个问题之前，让我们先了解一下乱序写入涉及什么。

### 理解乱序写入

当较新的更新在较旧的更新之前到达时，就会发生乱序写入。

例如，假设您正在运行双写设置，同时写入源数据库和目标数据库。然后您接入一个迁移工具（例如[ScyllaDB Migrator](https://migrator.docs.scylladb.com/stable/)），开始从源数据库读取数据并将其写入目标数据库。Spark作业从源数据库读取一些数据，然后客户端对同一数据写入一个更新。客户端首先将数据写入目标数据库，而Spark作业在其之后写入。Spark作业可能会覆盖较新的数据。这就是乱序写入。

[![](https://cdn.thenewstack.io/media/2026/01/8bb1d5a4-image2a.png)](https://cdn.thenewstack.io/media/2026/01/8bb1d5a4-image2a.png)

[Martin Fowler](https://martinfowler.com/eaaDev/RetroactiveEvent.html)是这样描述的：“乱序事件是指接收较晚的事件，晚到您已经处理了本应在乱序事件接收后处理的事件。”

对于[Cassand](https://thenewstack.io/benchmarking-apache-cassandra-40-nodes-vs-scylladb-4-nodes/)ra和ScyllaDB，您可以通过使用CQL（Cassandra查询语言）协议显式设置写入的时间戳来处理这些乱序写入。在我们的例子中，客户端更新将包含一个比Spark写入更晚的时间戳，因此它将“获胜”——无论哪个最后到达。

DynamoDB中不存在此功能。

### 团队如何在DynamoDB中处理乱序写入

该团队使用DynamoDB的条件表达式（Condition Expressions）处理乱序写入，这与Cassandra中的轻量级事务非常相似。然而，DynamoDB中的条件表达式（无论是在[性能还是成本](https://thenewstack.io/mongodb-vs-scylladb-performance-scalability-and-cost/)方面）都比常规的非条件表达式昂贵得多。

该团队如何尝试使用ScyllaDB规避乱序写入？最初，他们在每次写入之前都实现了读后写（read-before-write）。这有效地导致了他们的读取次数激增。

在我们与他们会面并分析他们的情况后，我们通过简单地操纵其写入的时间戳，显著提高了他们的应用程序和数据库性能。这与我们的另一个客户[Zillow处理乱序事件](https://www.scylladb.com/users/case-study-zillow-scales-background-and-real-time-workloads-with-scylla/)的方法相同。

## 互动平台：带TTL的数据（ScyllaDB 自管理到 ScyllaDB Cloud）

接下来，让我们看看同一数据库不同版本之间的迁移：ScyllaDB到ScyllaDB的迁移。一家互动平台公司决定从自管理的本地ScyllaDB部署迁移到ScyllaDB Cloud托管解决方案，因此我们帮助他们迁移数据。

不需要数据建模更改，这极大地简化了流程。虽然我们最初建议进行在线迁移，但他们选择采用离线方式。

[![](https://cdn.thenewstack.io/media/2026/01/6e9341e0-image3a.png)](https://cdn.thenewstack.io/media/2026/01/6e9341e0-image3a.png)

### 为什么选择离线迁移？

离线迁移有一些明显的缺点：存在一个数据丢失窗口，其长度等于迁移所需的时间，而且这个过程相当手动。您必须对每个节点进行快照，将快照复制到某个地方，然后将它们加载到目标系统。如果您选择不进行双写，切换客户端是单向操作；回滚将意味着数据丢失。

我们事先讨论了这些风险，但团队认为这些风险不会超过离线迁移的好处和简单性。（他们预计大部分数据最终会因TTL（生存时间）而过期）。

在生产迁移之前，我们测试了每个步骤，以更好地了解潜在的数据丢失窗口。

在大多数情况下，在进行离线迁移时，也可以完全从数据丢失转变为临时不一致。切换写入器后，您只需再次从源数据库（现在是只读系统）重复迁移步骤，从而恢复任何未作为初始快照一部分捕获的数据。

### 典型的基于TTL的迁移流程

该团队使用TTL数据来控制他们的数据过期，所以让我们讨论一下带有TTL数据的迁移通常是如何工作的。

首先，您配置应用程序客户端进行双写，但让客户端仅从现有真实数据源读取数据。最终，该真实数据源上的TTL会过期。此时，您可以将读取切换到新的目标数据库，所有数据都应该同步。

[![](https://cdn.thenewstack.io/media/2026/01/bbe70652-image4.png)](https://cdn.thenewstack.io/media/2026/01/bbe70652-image4.png)

### 迁移的实际情况

在这种情况下，客户端仅对单一现有真实数据源进行读取和写入。在应用程序仍在运行的情况下，团队对所有节点上的数据进行了在线快照。生成的快照被传输到目标集群，我们使用Load and Stream（一个基于Cassandra `nodetool refresh` 命令构建的ScyllaDB扩展）加载了数据。

[![](https://cdn.thenewstack.io/media/2026/01/b2c8a07b-image5a.png)](https://cdn.thenewstack.io/media/2026/01/b2c8a07b-image5a.png)

Load and Stream不仅仅是加载节点数据并丢弃该节点非副本的令牌，它实际上是将数据流式传输到其他集群成员。这极大地简化了整个迁移过程。Load and Stream不仅仅是加载数据并丢弃不需要的令牌，它实际上是将数据流式传输到集群中的其他节点。

在团队的Load and Stream完成后，客户端只需将读写切换到新的真实数据源。

# 消息应用：影子集群（Cassandra 到 ScyllaDB）

接下来，让我们探讨一家消息应用公司如何应对从Cassandra迁移超过万亿行数据到ScyllaDB的挑战。

由于Cassandra和ScyllaDB是API兼容的，此类迁移不应需要任何模式或应用程序更改。然而，考虑到他们数据的关键性和一致性要求，在线迁移方法是唯一可行的选择。他们需要零用户影响，并且对数据丢失零容忍。

### 使用影子集群进行在线迁移

团队选择创建一个“影子集群”。影子集群是生产集群的镜像，它拥有相同（大部分）数据并接收相同的读写操作。他们根据相应生产集群中节点的磁盘快照创建了它。生产流量（读和写）通过他们为此特定目的创建的数据服务镜像到影子集群。

[![](https://cdn.thenewstack.io/media/2026/01/c2439988-image6.png)](https://cdn.thenewstack.io/media/2026/01/c2439988-image6.png)

通过影子集群，他们可以在实际切换之前评估新平台的性能影响。它还允许他们彻底测试迁移的其他方面，例如长期稳定性和可靠性。

缺点是什么？它相当昂贵，因为它在您运行影子集群期间通常会使您的基础设施成本翻倍。拥有影子集群还会增加可观测性、检测、潜在代码更改等方面的复杂性。

### 迁移过程中权衡吞吐量和延迟

这次迁移中一个值得注意的[经验教训](https://thenewstack.io/lessons-learned-leading-high-stakes-data-migrations/)：确保源系统在实际数据迁移过程中的稳定性是多么重要。大多数团队都希望尽快迁移他们的数据。然而，尽快迁移可能会影响延迟，当低延迟对最终用户的满意度至关重要时，这可能成为一个问题。

在该团队的案例中，解决方案是尽可能快地迁移数据，但仅限于不影响源系统延迟的程度。

那么，您应该以每秒多少次操作来迁移？在哪个并发级别？这里没有简单的答案。实际上，您必须进行测试。

## 总结

“最佳”NoSQL迁移方法是什么？正如这些示例的广度和多样性所显示的，答案很简单：视情况而定。每日批量摄取让一个团队完全跳过了常规的迁移步骤。另一个团队不得不应对TTL和快照时序。还有另一个团队则真正专注于确保迁移不会损害他们严格的延迟要求。对一个团队有效的方法对下一个团队可能无效——您的特定要求也将塑造您自己的迁移路径。

我希望这些例子能让您对自己在迁移中将面临的权衡和技术考量类型有一个有趣的了解。如果您想了解更多，我鼓励您浏览ScyllaDB用户迁移案例库。例如：