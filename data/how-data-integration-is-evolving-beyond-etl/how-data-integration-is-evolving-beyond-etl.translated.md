# 数据集成如何超越 ETL 而不断发展

![如何超越 ETL 的数据集成特色图片](https://cdn.thenewstack.io/media/2024/05/2c1ec955-digital-8766930_1280-1024x574.png)

谈到数据集成，有些人可能想知道有什么可讨论的——这不就是 ETL 吗？也就是说，从各种数据库中提取、转换并最终加载到不同的数据仓库中。

然而，随着大数据、数据湖、实时数据仓库和大规模模型的兴起，数据集成的架构已经从数据仓库时代的 ETL 演变为大数据时代的 ELT，再到现在的 EtLT 阶段。在全球科技领域，FiveTran、Airbyte 和 Matillion 等新兴的 EtLT 公司已经出现，而 IBM 等巨头已经投资 23 亿美元收购 [StreamSets](https://thenewstack.io/data-analytics-and-ai-what-will-happen-in-2023/) 和 webMethods，以将其产品线从 ETL 升级到 webMethods（[DataOps](https://thenewstack.io/the-benefits-and-drawbacks-of-dataops-in-practice/))。

无论您是企业经理还是数据领域的专业人士，重新审视近期数据集成中的变化和未来趋势至关重要。

## ETL 架构

数据领域的多数专家都熟悉 ETL 这个术语。在 [数据仓库](https://thenewstack.io/why-the-self-adapting-data-warehouse-is-the-future/) 的鼎盛时期，[IBM DataStage](https://thenewstack.io/a-close-look-at-cloud-based-machine-learning-platforms-ibm-and-oracle/)、Informatica、Talend 和 Kettle 等 ETL 工具很流行。一些公司仍然使用这些工具从各种数据库中提取数据，对其进行转换，然后将其加载到不同的数据仓库中以进行报告和分析。ETL 架构的优缺点如下：

### ETL 架构的优点：

- 数据一致性和质量
- 复杂数据源的集成
- 清晰的技术架构
- 业务规则的实施

### ETL 架构的缺点：

- 缺乏实时处理
- 高昂的硬件成本
- 灵活度有限
- 维护成本
- 对非结构化数据的处理有限

## ELT 架构

随着大数据时代的到来，面对 ETL 无法加载复杂数据源及其实时性能不佳的挑战，ETL 架构的一种变体 ELT 应运而生。公司开始使用各种数据仓库供应商提供的 ELT 工具，例如 Teradata 的 BETQ/Fastload/TPT 和 Hadoop Hive 的 Apache Sqoop。ELT 架构的特点包括直接将数据加载到数据仓库或大数据平台中，而无需进行复杂的转换，然后使用 SQL 或 H-SQL 处理数据。

ELT 架构的优缺点如下：

### ELT 架构的优点：

- 处理海量数据
- 提高开发和运营效率
- 经济高效
- 灵活性和可扩展性
- 与新技术的集成

### ELT 架构的缺点：

- 实时支持有限
- 高昂的数据存储成本
- 数据质量问题
- 依赖于目标系统功能

## EtLT 架构

随着数据湖和实时数据仓库的普及，ELT 架构在实时处理和处理非结构化数据方面的弱点被凸显出来。因此，一种新的架构 EtLT 应运而生。EtLT 架构通过添加从 SaaS、Binlog 和云组件等来源实时提取数据，以及在将数据加载到目标存储之前纳入小规模转换，从而增强了 ELT。这一趋势导致了全球多家专业公司的出现，例如 StreamSets、Attunity（被 Qlik 收购）、Fivetran 和 Apache 基金会的 SeaTunnel。

EtLT 架构的优缺点如下：

### EtLT 架构的优点：

- 实时数据处理
- 支持复杂数据源
- 减少成本
- 灵活性和可扩展性
- 性能优化
- 支持大型模型
- 数据质量和治理

### EtLT 架构的缺点：

- 技术复杂性
- 依赖于目标系统功能
- 管理和监控挑战
- 数据变更管理复杂性增加
- 依赖于工具和平台

总体而言，近年来，随着数据、实时数据仓库和大规模模型的兴起，EtLT 架构已逐渐成为全球数据集成领域的的主流。有关具体的历史细节，您可以参考我在“ [ELT 已死，EtLT 将终结现代数据处理架构](https://blog.devgenius.io/elt-is-dead-and-etlt-will-be-the-end-of-modern-data-processing-architecture-154b87c1cce0)”一文中的相关内容。

在此总体趋势下，我们来解读整个数据集成轨迹的成熟度模型。总体而言，有四个明确的趋势：
**更正后的 Markdown 文本：**

**章节 1：ETL 演变为 EtLT**

- 在 ETL 演变为 EtLT 的趋势中，数据集成的重点已从传统的批处理转向实时数据收集和批流式集成数据集成。最热门的场景也已从过去的单数据库批处理集成场景转向混合云、SaaS 和以批流式方式集成的多个数据源。

- 数据复杂性转换已逐渐从传统的 ETL 工具转向在数据仓库中处理复杂转换。同时，在实时数据集成期间，DDL（字段定义）更改时，对自动模式更改（模式演变）的支持也已开始。即使适应轻量级转换中的 DDL 更改也已成为一种趋势。

- 对数据源类型的支持已从文件和传统数据库扩展到包括新兴数据源、开源大数据生态系统、非结构化数据系统、云数据库以及对大模型的支持。这些也是每个企业中最常见的场景，未来，实时数据仓库、湖泊、云和大模型将在每个企业内的不同场景中使用。

- 在核心功能和性能方面，大多数企业最优先考虑数据源的多样性、高准确性和易于故障排除。相反，对于高吞吐量和高实时性能等功能，没有太多检查点。

**章节 2：数据集成成熟度模型解读**

### 数据生产

数据生产部分是指在数据集成的背景下如何获取、分发、转换和存储数据。这部分在集成数据时提出了最大的工作量和挑战。

当业内用户使用数据集成工具时，他们首先考虑的是这些工具是否支持与他们的数据库、云服务和 SaaS 系统集成。如果这些工具不支持用户的专有系统，那么需要额外的成本来定制接口或将数据导出到兼容的文件中，这可能会对数据的及时性和准确性构成挑战。
**数据收集：**

目前，大多数数据集成工具都支持批处理收集、速率限制和 HTTP 收集。然而，实时数据获取 (CDC) 和 DDL 更改检测仍处于增长和普及阶段。特别是，处理源系统中 DDL 更改的能力至关重要。源系统结构的更改常常会中断实时数据处理。有效解决 DDL 更改的技术复杂性仍然是一个挑战，并且各个行业供应商仍在探索解决方案。

**数据转换：**

随着 ETL 架构的逐渐衰落，集成工具中的复杂业务处理（例如，Join、Group By）已逐渐成为历史。尤其是在实时场景中，用于流窗口 Join 和聚合等操作的可用内存有限。因此，大多数 ETL 工具正在向 ELT 和 EtLT 架构迁移。使用类似 SQL 的语言进行轻量级数据转换已成为主流，使开发人员无需学习各种数据集成工具即可执行数据清理。此外，数据内容监控和 DDL 更改转换处理与通知、警报和自动化相结合，使数据转换成为一个更智能的过程。

**数据分发：**

传统的 JDBC 加载、HTTP 和批量加载已成为每个主流数据集成工具的基本功能，竞争重点在于数据源支持的广度。自动 DDL 更改可减少开发人员的工作量，并确保数据集成任务的顺利执行。各种供应商采用自己的方法来处理数据表定义更改的复杂场景。与大型模型集成正在成为一种新趋势，允许内部企业数据与大型模型交互，尽管它目前是某些开源社区中爱好者的领域。

**数据存储：**

下一代数据集成工具具有缓存功能。以前，此缓存存在于本地，但现在使用分布式存储和分布式检查点/快照技术。有效利用云存储也正在成为一个新的方向，尤其是在涉及需要数据重放和记录的大型数据缓存的场景中。

**数据结构迁移：**

这部分涉及在数据集成过程中是否可以执行自动表创建和检查。自动表创建涉及在目标系统中自动创建与源系统中兼容的表/数据结构。这大大减少了数据开发工程师的工作量。自动模式推断是一个更复杂的场景。在 EtLT 架构中，在实时数据 DDL 更改或数据字段更改的情况下，自动推断其合理性允许用户在运行数据集成任务之前识别问题。业界仍在对此方面进行试验阶段。

### 计算模型

计算模型随着 ETL、ELT 和 EtLT 的不断变化而演变。它已从早期强调计算转变为中期专注于传输，现在强调实时传输中的轻量级计算：

**离线数据同步：**

这已成为每个企业最基本的数据集成要求。然而，在不同的架构下，性能会有所不同。总体而言，在处理大规模数据时，ETL 架构工具的性能远低于 ELT 和 EtLT 工具。

**实时数据同步：**

随着实时数据仓库和数据湖的普及，实时数据同步已成为每个企业在集成数据时需要考虑的一个重要因素。越来越多的公司开始使用实时同步。

**批处理流集成：**

新一代数据集成引擎从一开始就考虑了批处理流集成，为不同的企业场景提供了更有效的同步方法。相比之下，大多数传统引擎的设计重点是实时或离线场景，导致批处理数据同步性能不佳。在数据初始化和混合批处理流环境中，统一使用批处理和流可以表现得更好。

**云原生：**

海外数据集成工具在这方面更具侵略性，因为它们按即用即付的方式计费。因此，为每个任务快速获取/释放响应式计算资源是每家公司的核心竞争力和利润来源。相比之下，中国大数据云原生集成的进展仍然相对缓慢，因此它仍然是国内少数公司探索的主题。

### 数据类型和典型场景
**文件收集：**
这是每个集成工具的基本功能。但是，与过去不同的是，除了标准文本文件之外，以 Parquet 和 ORC 等格式收集数据已成为标准。

**大数据收集：**
随着 Snowflake、Redshift、Hudi、Iceberg、ClickHouse、Doris 和 StarRocks 等新兴数据源的普及，传统数据集成工具在这方面明显落后。中国和美国的用户在使用大数据方面总体上处于同一水平，因此需要供应商适应这些新兴数据源。

**Binlog 收集：**
这是中国一个新兴的行业，因为它在信息化过程中取代了 DataStage 和 Informatica 等传统工具。但是，Oracle 和 DB2 等数据库的替换速度并不快，导致大量专门的 Binlog 数据收集公司涌现，以解决海外的 CDC 问题。

**信息化数据收集：**
这是中国独有的场景。随着信息化进程，涌现出众多国产数据库。这些数据库的批处理和实时收集能否适应，对中国供应商提出了更高的挑战。

**分片：**
在大多数大型企业中，分片通常用于减轻数据库的压力。因此，数据集成工具是否支持分片已成为专业数据集成工具的标准功能。

**消息队列：**
在数据湖和实时数据仓库的推动下，与实时相关的一切都在蓬勃发展。消息队列作为企业实时数据交换中心的代表，已成为先进企业的必备选择。数据集成工具是否支持足够数量的内存/磁盘消息队列类型已成为最热门的功能之一。

**非结构化数据：**
MongoDB 和 Elasticsearch 等非结构化数据源已成为企业必不可少的工具。数据集成也相应地支持此类数据源。

**大模型数据：**
全球许多初创公司都在致力于快速与企业数据和大型

**SaaS 集成：**
这是海外非常流行的功能，但在中国尚未产生重大需求。

**数据统一调度：**
将数据集成与调度系统集成，特别是通过调度系统和后续数据仓库任务协调实时数据，对于构建实时数据仓库至关重要。

**实时数据仓库/数据湖：**
这些是目前企业最流行的场景。将实时数据输入仓库/湖泊可以实现下一代数据仓库/湖泊的优势。

**数据灾难恢复备份：**
随着数据集成实时功能的增强和 CDC 支持，传统灾难恢复领域的集成应运而生。一些数据集成和灾难恢复供应商已开始在彼此的领域开展工作。但是，由于灾难恢复和集成场景在细节上存在显着差异，因此供应商相互渗透的领域可能缺乏功能，需要随着时间的推移进行迭代改进。

### 运营和监控
在数据集成中，运营和监控是必不可少的功能。有效运营和监控可以显著减轻数据问题时系统操作和开发人员的工作量。
### 流程控制

现代数据集成工具从任务并行、单任务 JDBC 并行和单 JDBC 读取量等多个方面控制流量，确保对源系统的影响最小。

### 任务/表级统计信息

任务级和表级同步统计信息对于管理数据集成过程中的运维人员至关重要。

### 逐步试运行

由于支持实时数据、SaaS 和轻量级转换，直接运行复杂的数据流变得更加复杂。因此，一些先进的公司引入了逐步试运行功能，以实现高效的开发和运维。

### 表变更事件捕获

这是实时数据处理中的一项新兴功能，允许用户在源系统中发生表变更时以预定义的方式进行变更或发出警报，从而最大程度地提高实时数据的稳定性。

### 批流式集成调度

在实时 CDC 和流处理之后，与传统批数据仓库任务集成是不可避免的。但是，确保批数据的准确启动而不影响数据流操作仍然是一个挑战。这就是集成和批流式集成调度相关的原因。

### 智能诊断/调优/资源优化

在集群和云原生场景中，有效利用现有资源并在出现问题时推荐正确的解决方案是大多数先进数据集成公司关注的热点话题。但是，实现生产级智能应用程序可能需要一些时间。

### 核心功能

数据集成中有许多重要的功能，但以下几点是最关键的。在企业使用期间，缺少这些功能可能会产生重大影响。

- **全量/增量同步：**单独的全量/增量同步已成为每个数据集成工具的必要功能。但是，从全量模式自动切换到增量模式尚未在中小型供应商中普及，需要用户手动切换。
- **CDC 捕获：**随着企业对实时数据的需求增加，CDC 捕获已成为数据集成的核心竞争优势。对来自多个数据源的 CDC 的支持、要求以及 CDC 对源数据库的影响通常成为数据集成工具的核心竞争力。
- **数据多样性：**支持多个数据源已成为数据集成工具中的“红海竞争”。更好地支持用户现有系统的数据源通常会导致在业务竞争中处于更有利的地位。
- **检查点恢复：**实时和批数据集成是否支持检查点恢复有助于在许多场景中快速从错误数据场景中恢复，或在某些特殊情况下协助恢复。但是，目前只有少数工具支持此功能。
- **并发性/限速：**数据集成工具在需要速度时需要高度并发，并在速度慢时有效减少对源系统的影响。这已成为集成工具的必要功能。
- **多表同步/全数据库迁移：**这不仅指界面中的方便选择，还指 JDBC 或现有集成任务是否可以在引擎级别重用，从而更好地利用现有资源并快速完成数据集成。

### 性能优化

除了核心功能之外，性能通常代表用户是否需要更多资源，或者数据集成工具的硬件和云成本是否足够低。但是，目前极端的性能是不必要的，通常在界面支持和核心功能之后被认为是第三个因素。

- **及时性：**分钟级集成已逐渐退出历史舞台，支持秒级数据集成已成为一项非常流行的功能。但是，毫秒级数据集成场景仍然相对较少，主要出现在灾难恢复特殊场景中。
- **数据规模：**目前大多数场景涉及 Tb 级数据集成，而 Pb 级数据集成是由互联网巨头使用的开源工具实现的。Eb 级数据集成在短期内不会出现。
- **高吞吐量：**高吞吐量主要取决于集成工具是否可以有效利用网络和 CPU 资源，以实现理论数据集成的最大值。在这方面，基于 ELT 和 EtLT 的工具明显优于 ETL 工具。
**分布式集成：**动态容错比动态扩展和云原生更重要。大数据集成任务在硬件和网络故障情况下自动容错的能力，是大规模数据集成时的一个基本功能。可扩展性和云原生是此场景中的派生需求。

**准确性：**数据集成如何保证一致性是一项复杂的任务。除了使用多种技术来保证“Exactly Once”之外，还要进行 CRC 校验。还需要第三方数据质量检查工具，而不仅仅是“自我认证”。因此，数据集成工具通常与数据调度工具配合使用，以验证数据准确性。

**稳定性：**这是多项功能的结果。在可用性、任务隔离、数据隔离、权限和加密控制方面确保单个任务的稳定性非常重要。当单个任务或部门出现问题时，不应影响其他任务和部门。

**生态：**优秀的数据集成工具拥有庞大的生态系统，支持与多个数据源同步以及与上游和下游调度和监控系统集成。此外，工具可用性也是一个重要的指标，涉及企业人员成本。

## 第三章 趋势

未来几年，随着 EtLT 架构的普及，数据集成中将涌现出许多新场景，而数据虚拟化和 DataFabric 也将对未来的数据集成产生重大影响：

**多云集成：**这在全球范围内已经很普遍，大多数数据集成都具有跨云集成能力。在中国，由于云的普及程度有限，这方面仍处于早期孵化阶段。

**ETL 集成：**随着 ETL 周期的下降，大多数企业将逐渐从 Kettle、Informatica、Talend 等工具迁移到新兴的 EtLT 架构，从而支持批流式集成数据集成和更多新兴数据源。

**ELT：**目前，大多数主流大数据架构都基于 ELT。随着实时数据仓库和数据湖的兴起，ELT 相关工具将逐步升级为 EtLT 工具，或添加实时 EtLT 工具来弥补 ELT 架构中对实时数据支持的不足。

**EtLT：**在全球范围内，摩根大通、希音、Shopee 等公司正在将自己嵌入到 EtLT 架构中。更多公司将把其内部数据集成工具集成到 EtLT 架构中，结合批流式集成调度系统，以满足企业 DataOps 相关需求。

**自动化治理：**随着数据源和实时数据的增加，传统治理流程无法满足实时分析的时效性要求。自动化治理将在未来几年内在企业中逐步兴起。

**大模型支持：**随着大模型渗透到企业应用中，为大模型提供数据成为数据集成的一项必要技能。传统的 ETL 和 ELT 架构较难适应实时、大批量数据场景，因此 EtLT 架构将随着大模型的普及而加深对大多数企业的渗透。

**ZeroETL：**这是亚马逊提出的一个概念，它表明存储在 S3 上的数据可以直接被各种引擎访问，而无需在不同引擎之间进行 ETL。从某种意义上说，如果数据场景不复杂，数据量小，少量引擎就可以满足 OLAP 和 OLTP 的要求。但是，由于场景支持有限且性能较差，更多公司需要一段时间才能认识到这种方法。

**DataFabric：**目前，许多公司提出使用 DataFabric 元数据来管理所有数据，从而在查询期间无需 ETL/ELT，并直接访问底层数据。这项技术仍处于实验阶段，在查询响应和场景适应方面面临着重大挑战。它可以满足小数据查询的简单场景需求，但对于复杂的大数据场景，在可预见的未来，EtLT 架构仍然是必要的。

**数据虚拟化：**基本思想类似于 DataFabric 的执行层。数据不需要移动；而是通过临时查询接口和计算引擎（例如 Presto、TrinoDB）直接查询，以转换存储在底层数据存储或数据引擎中的数据。但是，在数据量大的情况下，引擎查询效率和内存消耗往往达不到预期，因此仅用于数据量小的场景中。
### 从整体趋势来看

随着全球数据爆炸式增长、大模型的出现以及各种场景下数据引擎的激增，实时数据的兴起让数据集成重新回到数据领域的中心。

如果把数据比作一种新的能源，那么数据集成就是这种新能源的管道。数据引擎越多，对管道的效率、数据源兼容性和可用性要求就越高。

虽然数据集成最终会面临零 ETL、数据虚拟化和 DataFabric 的挑战，但在可见的未来，这些技术的性能、准确性和 ROI 一直未能达到数据集成的普及程度。

否则，美国最流行的数据引擎就不应该是 SnowFlake 或 DeltaLake，而是 TrinoDB。当然，我相信在未来 10 年，在 DataFabric x 大模型的情况下，虚拟化 + EtLT + 数据路由可能是数据集成的终极解决方案。

总之，只要数据量增长，数据之间的管道就永远存在。

## 如何使用数据集成成熟度模型

首先，成熟度模型提供了对未来 10 年内可能用于数据集成的当前和潜在未来技术的全面视图。它为个人提供了个人技能发展的见解，并帮助企业设计和选择适当的技术架构。此外，它还提供了数据集成行业内关键发展领域的指导。

对于企业而言，技术成熟度有助于评估对特定技术的投资水平。对于成熟的技术，它可能已经使用了多年，有效地支持业务运营。然而，随着技术进步达到一个平台期，可以考虑采用更新、更有前途的技术来实现更高的业务价值。处于下降期的技术在支持业务运营方面可能会面临越来越多的限制和问题，在 3-5 年内逐渐被更新的技术所取代。

在引入此类技术时，必须考虑其业务价值和企业的当前状态。另一方面，流行技术由于在早期采用者中得到广泛验证，因此受到企业的优先考虑，大多数企业和技术公司都认可它们。它们的业务价值已经得到验证，预计它们将在未来 1-2 年内主导市场。

增长型技术需要根据其业务价值进行考虑，已经过了早期采用阶段，其技术和业务价值得到了早期采用者的验证。由于品牌和推广等原因，它们尚未在市场上得到充分接受，但有可能成为流行技术和未来的行业标准。

前瞻性技术通常是前沿技术，由早期采用者使用，提供一定的业务价值。然而，它们的普遍适用性和 ROI 尚未得到充分验证。企业可以考虑在它们提供显着业务价值的领域进行有限采用。

对于个人而言，成熟和下降的技术提供有限的学习和研究价值，因为它们已经被广泛采用。专注于流行技术对就业前景有利，因为它们在行业中备受追捧。

然而，该领域的竞争非常激烈，需要一定的理解深度才能脱颖而出。增长型技术值得深入研究，因为它们未来可能会变得流行，并且早期的经验可以在它们达到顶峰时带来专业知识。

前瞻性技术虽然可能带来突破性创新，但也可能失败。个人可以选择根据个人兴趣投入时间和精力。虽然这些技术可能远未达到工作要求和实际应用，但有前瞻性思维的公司可能会在面试中询问它们，以评估候选人的远见。

[YOUTUBE.COM/THENEWSTACK](https://youtube.com/thenewstack?sub_confirmation=1)

技术发展迅速，不要错过任何一集。订阅我们的 YouTube 频道以流式传输我们所有的播客、采访、演示等。