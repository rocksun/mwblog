# Query Apache Kafka with SQL
![Featued image for: Query Apache Kafka with SQL](https://cdn.thenewstack.io/media/2024/04/91a6c0b1-kafka-postgresql-1024x576.png)
Apache Kafka is largely used in larger organizations to store and exchange data, but it has a big problem: You can’t easily query this data. Someone must always duplicate the data to a regular database to query it. This slows data innovation and forces enterprises to build pipelines where anything can happen.
Enabling every team member across an organization to access and utilize real-time data using the solution they want is a transformative strategy that drives widespread adoption and operational efficiency. This empowers not only developers but also business analysts, data scientists and decision-makers building a data-driven culture.
## Is Kafka Just for Streaming ETL?
Kafka was open sourced in 2011 when massive databases and big data were king. Since then, we’ve learned much about using this new way to move and transform data while keeping it in motion.
Today, Kafka is mostly used to move data reliably to a destination everyone can work with. This might be a database, data warehouse or data lake users can query (such as PostgreSQL, ClickHouse, Elasticsearch or
[Snowflake](https://www.snowflake.com/?utm_content=inline+mention)) and the analytics team can work with and that can be used to build dashboards and machine learning models. Kafka is often used only for its real-time capabilities and does not contain historical data — default data retention in Kafka is just a few days, after which the data is automatically deleted.
Kafka is combined with stream processing technologies like Kafka Streams, Apache Spark or Apache Flink to do transformations, filter the data, enrich it with users’ data and maybe do some joins between various sources. Kafka is great for building streaming extract, transform and load (ETL), which captures, transforms and loads data to another place in real-time, in contrast to traditional batch processing, which is defined on a scheduled basis (every X minutes).
All looks good, but Kafka has a big drawback: It does not make data accessible.
## Kafka Is Not Very Good for Querying
Apache Kafka is often the place where all the data in an organization is created before being moved into other applications. Then all the applications communicate and produce data through Kafka. But somehow, this data is barely accessible to non-developers including data scientists, analysts and product owners.
Exploring and working with data is not even straightforward for developers because there is no simple language like
[SQL](https://roadmap.sh/sql) for talking about data with Kafka. You often need external tooling like [Conduktor](https://www.conduktor.io/) or advanced command line tooling in your terminal to look at and analyze the data — but that can only go so far.
Not everyone in an organization is tech-savvy, and organizations want to provide a consistent experience for everyone to communicate equally. For example, they want the entire team, no matter how comfortable they are with tech, to be able to work on a new project without having to learn complex new tools.
In the Kafka space, organizations depend upon data engineering teams to build the necessary pipelines and ETL to make data accessible. These teams also use change data capture (CDC) tools like Debezium to move the data outside of Kafka, which dilutes data ownership, security and responsibilities.
## But Apache Kafka Is Not a Database… Is It?
As Martin Kleppmann discussed in his talk “
[Is Kafka a Database?”](https://www.youtube.com/watch?v=v2RJQELoM6Y) at Kafka Summit SF 2018: Kafka can achieve all of the atomicity, consistency, isolation and durability (ACID) requirements of a database by building stream processors. Kafka also has full support for exactly-once transactions, and Apache’s [KIP-939](https://cwiki.apache.org/confluence/display/KAFKA/KIP-939%3A+Support+Participation+in+2PC) proposal is emerging to support the two-phase commit (2PC) protocol to do distributed transactions with other databases.
Interestingly enough, Kleppman concluded that there are “no ad-hoc queries, for sure,” and you must move your data to a real database for such matters. Six years later, this is the one caveat still present and slowing down everyone who wants to work with Kafka.
## Dealing with the Data Mess
Organizations have tons of data in Kafka and databases. The quality of the data varies. The rules are not the same everywhere. Nobody has the same view of everything. It’s difficult to know what data is where or where the source of truth is. It’s what we call a data mess.
The duplication of data from Kafka to databases adds a thick layer of complexity. Because of fundamentally different security models, the ownership and security of the data become brittle and possibly inconsistent.
[Kafka has its way of protecting data,](https://thenewstack.io/protect-sensitive-data-and-prevent-bad-practices-in-apache-kafka/) and databases have another way. This security mismatch is hard to fix, and if you add requirements such as data masking or field-level encryption, it’s almost impossible.
This is how data leaks happen. For example, in March a breach of the French government
[exposed up to 43 million people’s data](https://www.theregister.com/2024/03/14/mega_data_breach_at_french/). These incidents underpin a clear deficiency in skills, consistency and maturity in the ecosystem.
The rapid multiplication of data products aggravates the fragmentation of the data landscape within organizations. This proliferation creates data silos, each operating in isolation, diluting the potential for a unified data strategy. The ad-hoc development of data pipelines, built outside of a cohesive governance framework, leaves organizations vulnerable to inaccuracies and inconsistencies.
## Is SQL the End Game?
SQL is a very well-known and popular programming language, ranking 6th on the
[TIOBE Index](https://www.tiobe.com/tiobe-index/sql/) and used by 40% of developers globally — with a notable 78% of them frequently utilizing SQL in their work.
When we talk about SQL, we have to talk about
[PostgreSQL](https://thenewstack.io/a-cheat-sheet-to-database-access-control-postgresql/). It has emerged as the leading database protocol, and every vendor involved in data wants to be compatible with it. Tools like [Grafana](https://thenewstack.io/grafana-seeks-to-correct-observabilitys-historic-terrible-job/), Metabase, Tableau, DBeaver and [Apache Superset](https://thenewstack.io/explore-and-visualize-data-the-apache-superset-way/) can all connect to services offering a PostgreSQL-compatible endpoint. Having a Kafka platform providing such an endpoint for any topic enables using these tools for data visualization and straightforward introspection.
SQL offers a solid foundation for building a unified data ecosystem, with Kafka serving as the single source of truth at its core. PostgreSQL stands out for its broad compatibility and ease of getting started thanks to many accessible vendors. Its open source nature, seamless integration with development environments and straightforward setup and management make it a preferred database choice for scalability, versatility, flexibility and robustness.
By modernizing Kafka to incorporate SQL capabilities, we can significantly reduce the need for data pipelines and replication. It will also lead to overall efficiency, cost efficiency, simpler governance and fewer security failures.
This would also integrate data engineers directly into product teams, rather than keeping them isolated in their own silos with their own data road maps, strengthening collaboration between developers and analytics teams.
## Going Further with AI and Machine Learning (ML)
SQL is perfect for ad-hoc analysis, dashboarding or building data pipelines. But it’s not the best at handling the massive volumes of data required for data science and AI/ML. This is where technologies like Apache Parquet and Apache Iceberg shine.
They provide columnar-based systems and pushdown filter optimizations that efficiently query large quantities of data. Many data scientists love them because they can be queried with their tools, such as Apache Spark, Pandas, Dask and Trino. This improves data accessibility and simplifies how to build AI/ML applications.
As we shared in our
[review of Kafka Summit London 2024](https://www.conduktor.io/blog/kafka-summit-london-2024/), Kafka’s ability to serve as the single source of truth — as organizations seek to expose data in Kafka across multiple formats — is becoming a reality. Confluent has announced [TableFlow](https://www.confluent.io/en-gb/blog/introducing-tableflow/), which seamlessly materializes Apache Kafka topics as Apache Iceberg tables without having to build and maintain data pipelines.
## Paving the Way to Liberate Data
Conduktor and Kafka, with capabilities to support real-time data needs, cater to online analytical processing (OLAP) and business requirements using SQL. They also fulfill AI/ML demands with file formats such as Parquet and Iceberg. In these ways, they are paving the way for a future where data is truly accessible and optimized for various consumption preferences. Building true data products and eliminating the need for technical duplication across different data stores will lead to a more efficient and secure data ecosystem.
If you want to liberate your data and make it more accessible in your organization while removing governance and security frictions, book a
[Conduktor demo](https://www.conduktor.io/contact/demo/). [
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)