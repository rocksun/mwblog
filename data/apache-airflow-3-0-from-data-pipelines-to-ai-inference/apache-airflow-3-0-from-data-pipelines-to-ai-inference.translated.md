# Apache Airflow 3.0：从数据管道到 AI 推理

![Featued image for: Apache Airflow 3.0: From Data Pipelines to AI Inference](https://cdn.thenewstack.io/media/2025/04/95ab7678-apache_airflow-1024x768.png)

大约 10 年前，[Apache Airflow](https://github.com/apache/airflow/) 推出时，其前提相对简单但又永恒。它最初被设计为允许开发人员和数据工程师将数据管道编写为代码的一种手段。

随着 3.0 版本的最新发布，这个日益流行的开源工作流管理资源现在提供了许多新功能，以支持企业级应用程序。

数据管道有版本控制（称为[有向无环图](https://www.geeksforgeeks.org/introduction-to-directed-acyclic-graph/) (DAG)），增强的安全功能以及支持 AI 推理执行的结构。

在活跃的开源社区的推动下，该平台的新发展正在显着扩大其支持的用例范围。

虽然它仍然是数据集成和数据编排工作的主要支柱，但现在它正在扩展到数据科学和机器学习的部署。

据 [Vikram Koka](https://www.linkedin.com/in/vikramkoka)（[Astronomer](https://www.astronomer.io/) 首席战略官和 [Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) Committer）称，“随着 Airflow 采用率的增长，我们现在看到 30% 的用户使用 Airflow 进行 MLOps。我们看到 10% 的用户将其用于生成式 AI 应用程序。”

3.  0 版本具有多种功能，可支持这些发展，同时通过基于 Python 的代码来加强其服务于数据驱动型工作流的核心价值主张。

## DAG 版本控制

[Airflow](https://airflow.apache.org/) 3.0 版本最广泛适用的功能之一是它为数据管道或 DAG 提供的版本控制。

在此版本之前，该系统的功能就好像用户只关心这些任务的最新版本的代码一样。

该平台的新版本让开发人员可以查看 DAG 的先前版本，以及许多其他相关问题，包括“所有操作元素”，Koka 说。“日志、诊断、指标……关于它的所有内容，你现在实际上可以返回并查看。”

此功能对于处理 DAG 的多个开发团队，甚至处理同一 DAG 的各个部分的开发团队至关重要。当数据管道的原始作者更换工作或项目时，它也有助于继承数据管道。

最普遍的用例可能需要调试尝试，并了解数据管道的哪些部分已损坏，或者如何改进它们以最大限度地提高效率。

Airflow 的 DAG 版本控制非常详细，包括 DAG 历史记录的各个方面，例如“该历史记录的日志是什么；该历史记录的结构是什么”，Koka 评论道。“历史版本需要多长时间才能运行？能够查看基于该管道或 DAG 先前版本的所有 DAG 运行变得更加重要。”

## 解耦的安全增强

Airflow 3.0 还增强了其安全功能，使该平台值得企业级生产机会。其主要的安全升级是将任务执行功能与解决方案提供的管理、调度和整体编排功能分开。

Airflow 的服务器组件现在包括“一个 API 服务器，它可以基本上读取和写入 Airflow 元数据数据库”，Koka 说。“然后，我们提供一个名为任务 SDK 的客户端组件，该组件最初是用 Python 编写的。因此，所有用户定义的代码仅在此任务 SDK 的上下文中运行。”

使用此范例，任务 SDK 的代码不会直接连接到 Airflow 的元数据数据库，从而防止工作进程直接写入它。相反，任务 SDK 中指定的作业与 API 服务器交互以报告和接收作业的状态。结果是“更强的安全访问控制姿态”，Koka 解释说。Koka 还提到，[Golang](https://go.dev/) 的任务 SDK 即将推出，并且社区成员一直在要求支持 Rust 的任务 SDK。

## 远程执行

将 Airflow 的任务执行功能与其他核心功能分离，一个更引人注目的结果是，它有效地允许任务在用户希望的任何地方运行。在某些情况下，这种自由可以加强安全性和[数据治理控制](https://thenewstack.io/make-data-governance-automation-suck-less-with-a-supergraph/)——例如在符合金融行业法规的数据上在私有云中运行作业，这样它就不会离开特定的数据中心。
对于这种用例，Koka表示，这些数据“可以集中编排，但仍然完全保留在特定的数据中心，以保证数据主权”。

API服务器提供了Koka引用的集中编排，而它与任务SDK的分离使得作业可以在完全不同的集群中、在公共或私有云中，或在组织指定的任何其他地方运行。“你可能有一些ML作业会受益于GPU，”Koka评论道。

“你可以在一个完全独立的GPU集群上运行这些作业。你不需要在同一个集群上增加GPU的开销。你可以在需要的时候租用GPU集群。”

## 调度选项

3.  0版本仍然支持传统的批处理模式来调度数据管道作业，并且在此基础上进行了大幅扩展。现在有几种调度任务的模式可用，包括：

**事件驱动调度：** 通过此选项，组织可以根据外部系统中的数据更改触发工作流。这里也存在低延迟的影响，例如依赖此调度方法来触发某些管道组件，这些组件基于[Kafka](https://thenewstack.io/the-new-look-and-feel-of-apache-kafka-4-0/)中到达的数据。 “它使Airflow能够对生态系统中数据的变化做出反应，”Koka提到。“它更接近于实时事件处理。”

**同时DAG执行：** 这种调度方法对于机器学习模型推理很有用。“你真的希望能够同时运行很多这样的模型，”Koka说。“我们增加了对推理执行的支持，因此你可以同时运行多个传入数据的管道。”

**Ad-Hoc调度：** 从概念上讲，这种调度方式与事件驱动调度有一些重叠。但是，“我通常倾向于认为它是基于几乎像人类事件或人类触发的事件来触发的，”Koka说。“比如抵押贷款申请的出现，或者有人说我想运行一个DAG，这是由来自其他系统的API触发的，而该API是由人类行为触发的。”

## 企业成熟

Apache Airflow 3.0的DAG版本控制、安全升级、远程执行能力和作业调度灵活性使其可用于越来越多的用例。最新版本还允许回填，因此组织可以异步重新运行错过的任务，监控其进度并取消它们。

这些发展都标志着管道编写和部署工具从开发者和工程师在幕后使用的工具，转变为部署在企业应用程序中的工具。

[
YOUTUBE.COM/THENEWSTACK
技术发展迅速，不要错过任何一集。订阅我们的YouTube频道，观看我们所有的播客、访谈、演示等。
](https://youtube.com/thenewstack?sub_confirmation=1)