# The Security Paradox of AI in Cloud Native Development
![Featued image for: The Security Paradox of AI in Cloud Native Development](https://cdn.thenewstack.io/media/2025/05/dc4631f3-oleksandr-chumak-zguburggmdy-unsplash-1024x612.jpg)
[Oleksandr Chumak](https://unsplash.com/@olalandro?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)on
[Unsplash](https://unsplash.com/photos/black-framed-eyeglasses-on-computer-screen-zGuBURGGmdY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash).
As AI and other familial technologies like generative AI (GenAI) continue to seep into environments across the enterprise, business leaders are at a constant simmer to find, build, and deploy new use cases, from data collection assistants for marketers to analyst agents for sales teams.

For security teams tasked with safeguarding broad and often disparate cloud ecosystems, monitoring potentially hundreds if not thousands of AI projects, all at various stages of complexity and maturity, can feel like drinking water from a hose — a huge mess moving incredibly fast.

Recent research from nearly [3,000 ](https://www.paloaltonetworks.com/resources/research/state-of-cloud-native-security-2024)cloud security and DevOps professionals found that more than half (54%) cited complexity and fragmentation in cloud environments as a top data security problem, and 71% of organizations faced vulnerabilities due to rushed deployments. Unsurprisingly, one in three respondents said they struggle to [keep up with rapid technology changes and evolving](https://thenewstack.io/ai-is-evolving-rapidly-heres-how-developers-can-keep-pace/) threats.

**AI in Cloud Development: Mind the Gaps**
Undoubtedly, AI — GenAI in particular — has been a digital B12 shot for [software engineers and developers](https://thenewstack.io/three-software-development-challenges-slowing-ai-progress/), boosting productivity and accelerating code generation and the overall pace of cloud native innovation. Sundar Pichai, CEO of Google and Alphabet, disclosed during their Q3 earnings in October that[ more than a quarter of their new code is generated by AI,](https://blog.google/inside-google/message-ceo/alphabet-earnings-q3-2024/#search) still under the watchful eye of their engineers.

As Sundar said, while the rapid adoption of AI for coding assistance and beyond *“helps our engineers do more and move faster,”* it also introduces dynamic [security concerns and cloud](https://thenewstack.io/what-we-can-learn-from-the-top-cloud-security-breaches/) vulnerabilities at scale, regardless of whether usage is sanctioned. This can be anything from insecure code that increases threat exposure to AI-automated cloud intrusions that evade traditional detection tools. A [recent report ](https://www.paloaltonetworks.com/resources/research/state-of-cloud-native-security-2024)found that despite 38% of respondents identifying AI-driven attacks as a top cloud security risk, all (100%) already use AI-assisted development.

When it comes to AI’s influence in cloud development, the risk factors can be separated into three equally concerning pillars:

**Don’t know what you don’t know:**With so many AI projects, from experimentation through production, running across cloud environments, it’s a herculean task for[security teams to maintain](https://thenewstack.io/open-source-needs-maintainers-but-how-can-they-get-paid/)a holistic view of the landscape, let alone its potential impact. This lack of visibility leads to a paucity of governance, even if comprehensive policies exist, and can be further compounded by costly regulatory and legal implications for unsanctioned usage.**More models, more problems:**AI-generated code and broader AI infrastructure introduce new elements into cloud environments, all of which potentially introduce new risks such as prompt injections or poisoned[data that can alter model](https://thenewstack.io/pulumi-templates-for-genai-stacks-pinecone-langchain-first/)behavior. Given that model interactions are highly unstructured, attack and evasion variations can simply be a battle of attrition with overworked and understaffed security teams.**Don’t forget your data:**like peanut butter and jelly, you simply can’t have AI without data, and that data, often used to finetune[models through processes like Retrieval-Augmented Generation](https://thenewstack.io/advanced-retrieval-augmented-generation-rag-techniques/)(RAG), must be secured.[It’s been demonstrated](https://www.scworld.com/brief/hugging-face-compromised-with-malicious-ai-models)that if bad actors access a model, they can reach its original proprietary or sensitive organizational and customer data. So, the rule of thumb: any model trained on sensitive[data should be treated as if it contains](https://thenewstack.io/container-security-a-troubling-tale-but-hope-on-the-horizon/)that sensitive data.
**AI in Cloud Development: Risk Doesn’t Mean Retreat**
While the risks of AI in cloud environments are irrefutable, so are the rewards. Regarding cloud security, think of AI as a digital double agent — the potential to breach perimeters with the power to bolster them. This is why the pace of organizational innovation and [investment must be in tandem with your security](https://thenewstack.io/want-to-mitigate-risk-invest-in-automation/) posture — if your right leg wants to run, your left leg can’t want to walk.

For security teams, there’s a simple trifecta of best practices to help ensure safe AI [adoption and integration into cloud](https://thenewstack.io/5-things-to-know-before-adopting-cloud-native/) workflows:

**Maintain an AI inventory:**You must have complete visibility of your AI infrastructure — models, agents, applications, workspaces — and how it maps to data. This can be done by regularly syncing with developers to understand and catalog what assets they’re using or automating it through specific tools and solutions.**Perform risk assessments:**you know what they say about assumptions… put the work in and map out conceivable risks associated with each AI pipeline, project, ecosystem, and infrastructure — what’s the potential impact? What are the threats? Does it touch sensitive data? Is it internal only or customer-facing? What is it used for? What business processes are reliant on it?**Invest in threat protection:**Leverage end-to-end platforms and advanced products like AI agents that can protect model interaction in real time. Detect, respond, and mitigate threat disruptions as they occur and help understand organizational context (who needs to know, who needs to approve actions, who has ownership, who will this impact) to quickly triage problems and reduce risk.
**AI in Cloud Development: A Responsible Way Forward**
Security teams have been branded as the *Department of No* for decades — an unfortunate moniker rooted in exaggerated truths. Gartner[ predicts](https://www.gartner.com/en/webinar/566448/1276680#:~:text=By%202028%2C%20cloud%20computing%20will,execution%20will%20impact%20business%20effectiveness.) that by 2028, the adoption of AI will culminate in more than 50% of cloud computing resources, which puts a more heightened focus on the need for organizations to strike a balance between AI-driven efficiency and robust security in their cloud environments.

This will take an equal commitment from engineers and developers to embrace openness and shine a brighter light on their cloud-based workstreams and from security teams to shed this obstructionist label and enable their organizations and colleagues to move swiftly and safely. By maintaining a 360-degree view of the threat environment, understanding every piece of hardware and software in your AI infrastructure, and investing in advanced threat protection, security teams can avoid the mess and just focus on the ride.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)