<!--
title: LLM六大安全风险：平台工程师指南
cover: https://cdn.thenewstack.io/media/2025/06/60cb6ba0-image1a.png
summary: 随着AI应用增加，安全风险也随之增加。平台工程师需要了解LLM的关键威胁，包括提示注入、模型窃取、数据泄露、资源管理、API安全和性能问题。采取输入验证、速率限制、PII检测、监控、身份验证和优化等措施来应对这些风险。
-->

随着AI应用增加，安全风险也随之增加。平台工程师需要了解LLM的关键威胁，包括提示注入、模型窃取、数据泄露、资源管理、API安全和性能问题。采取输入验证、速率限制、PII检测、监控、身份验证和优化等措施来应对这些风险。

> 译自：[6 Key Security Risks in LLMs: A Platform Engineer's Guide](https://thenewstack.io/6-key-security-risks-in-llms-a-platform-engineers-guide/)
> 
> 作者：Ron Northcutt

越来越多的组织正在部署 AI 模型，无论是通过托管云服务还是自托管解决方案。根据云安全提供商 Wiz 发布的[《2025 年云中 AI 状态》](https://www.wiz.io/reports/the-state-of-ai-in-the-cloud-2025)报告，目前 85% 的组织都在使用 AI 服务，高于 2024 年的 70%。随着这种增长，集成的工具和安全风险也随之增加。

平台工程师站在保护这些 AI 系统的第一线，尤其是在[大型语言模型 (LLM)](https://roadmap.sh/guides/introduction-to-llms)投入生产之际。虽然云托管的 LLM 带来了严峻的安全挑战，但自托管模型（例如 [Meta 的 LLaMa](https://thenewstack.io/why-open-source-developers-are-using-llama-metas-ai-model/)）引入了更大的复杂性。自托管可以更好地控制安全性、自定义和成本，但也扩大了攻击面。

[平台工程师](https://thenewstack.io/platform-engineering/)必须了解 LLM 构成的关键威胁，并应用有效的防御措施来安全地部署 AI。我们将探讨的六个最紧迫的风险是：

1.  提示注入攻击
2.  模型提取和盗窃
3.  私人数据泄露
4.  资源管理和成本控制挑战
5.  基础设施和 API 安全漏洞
6.  性能和可扩展性限制

让我们分解每个风险以及如何缓解它们。

## 1. 注意提示注入

提示注入是 AI 时代的 SQL 注入。攻击者精心设计恶意输入来操纵 LLM、绕过安全措施或提取敏感数据。这些攻击范围从覆盖安全规则的简单越狱提示，到影响后端系统的更高级的漏洞利用。

例如，攻击者可能会将类似 SQL 的语法注入到提示中，以操纵连接数据库的应用程序，或利用模型漏洞来揭示内部系统指令。如果不加以检查，这些风险可能会暴露机密流程并危及 AI 驱动的工作流程。

为了降低提示注入的风险，请实施以下关键防御措施：

*   **输入验证：** 在 API 网关处过滤和清理用户提示。
*   **上下文感知过滤：** 使用基于 AI 的检测来识别操纵企图。
*   **Web 应用程序防火墙 (WAF) 规则：** 部署 WAF 规则以检测和阻止恶意提示模式。
*   **速率限制：** 阻止可能暴露漏洞的重复对抗性查询。

## 2. 保护您的模型智能

[模型提取](https://thenewstack.io/microsoft-machine-learning-models-can-be-easily-reverse-engineered/)攻击允许攻击者系统地查询 LLM，以重建其知识库或训练数据，本质上是克隆其能力。这些攻击通常依赖于自动化脚本，提交数百万个查询来映射模型的响应。

一种常见的技术是模型反演，它涉及战略性地构建输入，以提取嵌入在模型中的敏感或专有信息。攻击者还可以使用重复的、增量的查询（略有变化）来积累一个模仿原始训练数据集的数据集。

为了防止未经授权的模型提取，请实施以下关键防御措施：

*   **高级速率限制：** 控制每个用户、IP 和 API 密钥的用量，以防止自动抓取。
*   **基于令牌的速率限制：** 限制每个用户会话的请求数量，以遏制过度查询。
*   **响应随机化：** 在输出中引入细微的变化，以破坏模式分析。
*   **行为分析：** 检测和阻止指示提取企图的可疑查询模式。

## 3. 保护私人数据

LLM 从用户输入和模型输出两方面引入了重大的隐私和监管风险。用户可能会无意中提交敏感数据，而模型可能会生成暴露训练数据或过去交互中的机密详细信息的响应。

在输出方面，LLM 可能会无意中泄露嵌入在其数据集或先前输入的用户数据中的私人信息。一个常见的风险场景涉及用户在不知情的情况下将财务记录或密码提交到 AI 驱动的聊天机器人中，这可能会以不可预测的方式存储、检索或暴露此数据。对于基于云的 LLM，风险会进一步扩大。一个组织的数据可能会出现在另一个组织的响应中。

为了减轻私人数据泄露，请应用以下关键保护措施：

*   **个人身份信息 (PII) 检测和编辑：** 自动扫描和清理输入和输出中的 PII。
*   **数据泄露防护 (DLP)：** 监控响应中是否存在敏感内容暴露，并执行合规性策略。
*   **严格的访问控制：** 限制谁可以与模型响应进行交互和检索。
*   **对话历史记录管理：** 防止多租户环境跨用户暴露数据。

## 4. 管理资源和控制成本

运行 LLM 的成本很高。基于云的 LLM 账单可能会迅速增加，而自托管需要部署高性能 GPU 和大量额外的基础设施投资。如果没有适当的控制，资源消耗可能会降低平台性能并推高成本。

自托管模型需要严格执行使用限制，以防止过度的令牌消耗和性能下降。与此同时，如果用户提交触发高令牌处理成本的长提示，则使用云 API 的组织面临意外超额的风险。

为了优化资源使用和控制成本，请实施以下关键策略：

*   **实时监控：** 跟踪 GPU 利用率和令牌消耗。
*   **请求配额：** 设置每个租户的使用限制，以防止资源过度使用。
*   **基于令牌的速率限制：** 限制过度的令牌处理以控制支出。
*   **GPU 感知调度和负载平衡：** 在多个实例之间有效地分配推理工作负载。

## 5. 保护您的基础设施和 API

配置错误的基础设施是主要的攻击媒介。暴露的调试端点、弱身份验证和不安全的 API 可能会导致未经授权的访问。影子 API（未记录或被遗忘的端点，无意中暴露了模型功能）是一种特别危险的风险。

例如，配置错误的 Kubernetes Ingress 控制器可能会无意中将内部 API 暴露给公共流量，从而允许未经授权的查询绕过安全控制。

为了保护 LLM 基础设施和 API，请实施以下关键保护措施：

*   **强大的身份验证：** 使用 OAuth2 或 JWT 令牌来强制执行 API 访问控制。
*   **相互 TLS (mTLS)：** 加密服务到服务通信以实现安全的数据交换。
*   **网络分段：** 隔离模型端点以最大程度地减少暴露于攻击。
*   **定期安全审计：** 持续扫描配置错误和未经授权的访问。

## 6. 优化性能并高效扩展

LLM 需要大量资源，并且随着使用量的增长，性能瓶颈将不可避免地出现。冷启动延迟是一个主要挑战；在流量高峰期间，模型实例可能需要太长时间才能初始化。低效的 GPU 利用率进一步降低了性能，尤其是在资源有限的自托管部署中，并增加了不必要的成本。此外，高度并发的查询可能会使系统不堪重负，从而导致延迟并降低吞吐量。

为了在任何规模上优化性能，请实施以下关键策略：

*   **GPU 感知自动缩放：** 使用 KEDA 等工具动态缩放资源。
*   **缓存策略：** 减少冗余推理调用以缩短响应时间。
*   **模型优化：** 部署多个模型以用于不同的工作负载，并相应地路由用户。

## LLM 安全的未来

保护 AI 系统是一场持续的战斗。随着新威胁的出现，保持领先需要持续监控、定期安全审计和采用最新的防御策略。挑战在于找到适当的平衡——过多的安全性可能会降低性能并增加成本，而过少的安全性会将敏感数据置于风险之中。

通过采取积极主动的战略方法，平台工程师可以保持 AI 部署的安全、高效和有价值，而无需不必要的权衡。