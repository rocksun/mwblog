
<!--
title: 工程团队AI落地应用的复杂现实
cover: https://cdn.thenewstack.io/media/2025/06/9401ae3f-webfaster-nbzhm2uwkjs-unsplash-scaled.jpg
summary: 人工智能成功的关键在于深刻理解工作流程，而非技术。质量评估、领域专家主导和用户信任至关重要。优先考虑领域知识、可量化质量指标和逐步自动化是成功模式。组织结构应弥合技术与领域专业知识的差距。
-->

人工智能成功的关键在于深刻理解工作流程，而非技术。质量评估、领域专家主导和用户信任至关重要。优先考虑领域知识、可量化质量指标和逐步自动化是成功模式。组织结构应弥合技术与领域专业知识的差距。

> 译自：[The Complicated Reality of AI Implementation in Engineering Teams](https://thenewstack.io/the-complicated-reality-of-ai-implementation-in-engineering-teams/)
> 
> 作者：Tyler Phillips

我职业生涯的大部分时间都认为，技术上的精湛是解决复杂问题的关键。作为一名人工智能创始人，后来成为人工智能产品负责人，我深信最先进的模型、最复杂的架构总会胜出。但我在 [Apollo.io](https://www.apollo.io/) 扩展人工智能解决方案的经验教会了我一个截然不同的道理：人工智能成功的关键因素不是技术，而是对你想用人工智能增强的人工工作流程的深刻理解。

我的职业生涯始于外呼销售的第一线，我看到团队在数小时的手动研究和外联任务中挣扎。销售专业人员被数据淹没，但缺乏有意义的联系。传统的自动化工具感觉像是粗钝的工具：速度快，但从根本上与人类沟通的细微艺术脱节。

在将 Apollo 的人工智能功能扩展到 54,000 名每周活跃用户（2024 年增长 5 倍）后，我们的团队有了一个惊人的发现。我们人工智能实施的成功与我们选择的模型或架构的复杂性关系不大。相反，决定性因素是我们的组织结构、评估方法以及销售专业人员的领域专业知识的整合。

## **质量评估基础设施的差距**

我们在人工智能之旅中发现的最令人惊讶的现实是什么？构建成功的人工智能应用程序需要一个评估基础设施，而大多数团队完全忽略了这一点。

像许多团队一样，我们最初是凭感觉进行交付的：如果在测试中看起来不错，我们就交付它。一旦你扩展到内部用户或早期采用者之外，这种方法就不可避免地会失败。在 Apollo，我们开发了一个四维评分系统，评估每个 AI 输出的：

* **准确性 (Accuracy)**：它是否与经过验证的信息相符？
* **相关性 (Relevance)**：它是否解决了特定的用例？
* **清晰度 (Clarity)**：它是否简洁易懂？
* **语气 (Tone)**：它是否与适当的沟通方式相匹配？

每个维度都由领域专家以 1-3 的等级进行评分。没有达到 2.5/3 的最低综合分数就不能交付。这个评估框架成为我们的竞争优势，确保了我们从数百名用户扩展到数万名用户时的一致质量。

## **有效的组织模式：领域专家领导，工程师支持**

通过痛苦的迭代，我们发现了一个违反直觉的真理：工程师是编写提示的错误人选，尽管他们具有人工智能系统的技术专长。

[机器学习](https://thenewstack.io/category/machine-learning/)（ML）工程师擅长系统架构和 API 集成，但在提示中捕捉特定领域的知识方面却步履维艰。当主题专家领导提示创建时，奇迹就会发生。

这个洞察力促使我们创建了一个专门的“提示编写者”角色，由销售领域的专家而非工程师担任。在这种组织结构转变之后，用户满意度提高了 1.34 倍。

对我们来说最有效的混合团队结构：

| **角色** | **主要职责** |
| --- | --- |
| 工程师 | 基础设施、API、监控 |
| 提示编写者 | 领域特定的提示创建 |
| 质量团队 | 评估框架和指标 |
| 领域专家 | 真实世界的测试和注释 |

## **用户体验挑战：通过透明度建立信任**

尽管技术领导者可能希望如此，但用户默认情况下并不信任人工智能系统。当我们最初的人工智能产品需要提示工程技能时，采用完全停滞了。

我们意识到，人工智能的采用遵循信任曲线，而不是能力曲线。用户需要验证输出，然后才会依赖它们。

我们用以下方式重建：

1. 简明的英语提示
2. 所有声明的来源引用
3. 输出的置信度指标
4. 一键式手动覆盖选项

这些变化导致每周活跃用户增长了 3 倍，不是因为底层的人工智能能力提高了，而是因为界面创造了信任。

## **实施经验：三个成功的模式**

在将我们的人工智能平台扩展到 54,000 名每周活跃用户并与数百家实施人工智能的公司合作后，我们发现了三个一致的模式，这些模式区分了成功的实施：

**赢得自动化的权利。** 大多数工程团队都急于构建完全自主的人工智能系统，但我们的数据显示，用户始终更喜欢拥有手动审查选项。当我们实施一键式覆盖机制和置信度指标时，用户采用率提高了 3 倍。首先建立信任机制，然后随着用户信心的增长逐渐增加自动化程度。

**优先考虑领域专业知识而不是技术复杂性。** 构建人工智能产品的优势不在于采用最新的代理框架或[大型语言模型](https://thenewstack.io/llm/)（LLM），而在于让主题专家将最佳实践融入到你的系统中。我们的盲模型测试表明，Claude Haiku 3.5 在我们特定的销售用例中始终优于更大的模型，尽管它更快、更便宜。模型的重要性不如你的团队对问题领域的理解程度。

**将你对质量的定义运作起来。** 大多数团队在早期使用主观的“感觉”来评估 AI 输出，但这种方法在大规模时会崩溃。通过实施我们的四维评分系统（准确性、相关性、清晰度、语气）和特定的阈值（最低 2.5/3），我们创建了一个客观的质量框架，即使我们的用户群增长了 5 倍，也能实现一致的结果。如果没有可量化的质量指标，人工智能产品不可避免地会朝着不一致的方向发展。

## **人工智能工程团队的未来**

新兴的现实？人工智能的实施失败不是因为技术上的限制，而是因为组织没有构建他们的团队来弥合能力和领域专业知识之间的差距。

下一波成功的人工智能产品不会由竞相实施最新模型架构的团队构建。它们将由创建正确的组织结构、评估框架和信任机制以大规模提供一致价值的团队构建。