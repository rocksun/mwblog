On the 4th of July, [Nathan Lambert](https://www.linkedin.com/in/natolambert/) launched “[The American DeepSeek Project](https://www.interconnects.ai/p/the-american-deepseek-project),” a plan to counter the open-weight AI [large language models](https://thenewstack.io/llm/) (LLMs) from [China’s DeepSeek](https://thenewstack.io/icymi-deepseek-is-an-open-source-success-story/) with support for an American “fully open source model at the scale and performance of current (publicly available) frontier models, within two years.”

It’s an issue dear to his heart. Lambert is a former Hugging Face research scientist (who’s also worked at [Google](https://cloud.google.com/?utm_content=inline+mention) [DeepMind](https://thenewstack.io/googles-deepmind-extends-ai-with-faster-sort-algorithms/) and Facebook AI Research), and is now a post-training lead at the nonprofit Allen Institute for AI ([Ai2](https://allenai.org/)). “I want to do this at Ai2,” Lambert wrote on his blog, “but it takes far more than just us to make it happen. We need advocates, peers, advisors, and compute.”

“Our strategy was never to seek broad public support,” Lambert told TNS in an email interview last week, “but instead to target key people in the AI/ML [machine learning] community. We’ve had numerous professors, founders and influential voices in AI sign on, as well as some surprises like OpenAI C-suite executives. We’ve also spoken to some key DC policy makers.” (“There are many avenues to obtain and allocate these resources across multiple stakeholders,” the site explains, including private companies, philanthropic institutions, government agencies, private sector partnerships, “and potentially new public-private partnership models, similar to how other critical national infrastructure projects are funded.”)

Lambert’s site said he’s pursuing the initiative “extracurricular” with his work on AI-research newsletter Interconnects.ai, and one month later, his campaign had evolved into “[The Atom Project](https://atomproject.ai/),” which hopes to trigger “an inflection point in the investment into the American open AI model ecosystem.”

Ironically, the day after Lambert’s project launched, OpenAI released its new [gpt-oss open-weight models](https://openai.com/index/introducing-gpt-oss/) — under the Apache 2.0 license — which the Project’s site acknowledges as “a positive step.” And last Sunday, Aug. 17, NVIDIA [released](https://blogs.nvidia.com/blog/speech-ai-dataset-models/) a [new dataset and models](https://huggingface.co/datasets/nvidia/Granary) to support the development of high-quality speech recognition and translation AI for 25 European languages.

“Both have been great steps forward,” Lambert told me last week, adding that “Additionally, over $150 million in investment has been announced in the open model ecosystem.”

But Lambert’s site still makes the case that “it doesn’t fully address the fundamental challenge,” arguing that “One model release doesn’t establish the sustained infrastructure, research culture and long-term commitment needed to compete with systematic efforts like DeepSeek’s.”

To compete with China’s open models, Lambert said America “needs a comprehensive approach to open AI development, not just occasional releases.”

## ‘Serious Computing Power’

Lambert said his goals are similar to the [National Artificial Intelligence Research Resource Pilot](https://www.nsf.gov/focus-areas/ai/nairr), led by the U.S. National Science Foundation and 12 other federal agencies and 26 nongovernmental partners to “make available government-funded, industry and other contributed resources in support of the nation’s research and education community.”

But Lambert said his project remains “focused on building the right kind of models; we believe very deeply that not all ‘open’ models are created equally, and we need to make sure that not only the caliber of models made in America match the foreign alternatives, but also that the decisions made to have fully open models are being respected.

“Our vision is an open model ecosystem without compromise — we want American true open models to lead the world.”

The Washington Post notes Lambert’s “ambitious” campaign envisioned by the Project involves “[access to serious computing power](https://www.msn.com/en-us/technology/artificial-intelligence/an-ambitious-new-project-aims-to-win-back-the-u-s-lead-in-open-source-ai-from-china/ar-AA1JWQ9H), with upward of 10,000 of the cutting-edge GPU chips used to power corporate AI development” — and at a cost of $100 million.

[![Screenshot from ATOM Project executive over (quote from Jensen Huang)](https://cdn.thenewstack.io/media/2025/08/31ad19c3-screenshot-from-atom-project-executive-over-quote-from-jensen-huang.png)](https://cdn.thenewstack.io/media/2025/08/31ad19c3-screenshot-from-atom-project-executive-over-quote-from-jensen-huang.png)

Screenshot from ATOM Project executive summary.

“A lot of it is a coordination problem,” Lambert told The Washington Post, noting that existing U.S.-based efforts at open models were impacted by a lack of funding, including Hugging Face’s Bloom and Pythia from [EleutherAI](https://www.eleuther.ai/about), an AI thinktank. “There are groups of people in the country that are doing it, but they haven’t been able to scale up.”

And in the end, Lambert envisions not just open weights, but also a model that’s sharing its data, training code and logs — all of the knowledge and materials needed to fully train an AI model. This could even include intermediate checkpoints and base models — and also, of course, permissive licenses.

The official website for the ATOM Project now boasts an impressive list of “Notable Signatories” who support its goal of building truly open American models “and to ensure that the United States maintains its lead in AI.”

## Open for the Win?

“If you believe that open source LLMs are going to outpace the other LLMs, then it could be a big deal,” said TNS analyst [Lawrence Hecht](https://www.linkedin.com/in/lawrence-hecht/), while adding, “I don’t think there is any reason to think that is a likely outcome.” A [late-July report from Menlo Ventures](https://menlovc.com/perspective/2025-mid-year-llm-market-update/#18aaeef7-0c05-404c-b36f-01edbc154d0f-link) found that 13% of AI workloads today use open source models, “down slightly from 19% [six months ago](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/).” Hecht acknowledges “that can change very quickly,” and there is the possibility that an open source model could in the future gain a critical mass of users. But on the other hand, in the world of today, “ChatGPT is not open source and it already has a critical mass of users.”

Lambert counters that when it comes to open models, “There are [other](https://www.linuxfoundation.org/hubfs/Research%20Reports/lfr_marketimpacts25_052725a.pdf?hsLang=en) [reports](https://www.mckinsey.com/~/media/mckinsey/business%20functions/quantumblack/our%20insights/open%20source%20technology%20in%20the%20age%20of%20ai/open-source-technology-in-the-age-of-ai_final.pdf) that show far higher adoption numbers.”

[![Respondents to a survey by global management consulting firm McKinsey & Company found 63% of organizations saying they used open models. ](https://cdn.thenewstack.io/media/2025/08/9eee033e-mckinsey-stats-on-open-model-use-1024x439.png)](https://cdn.thenewstack.io/media/2025/08/9eee033e-mckinsey-stats-on-open-model-use-1024x439.png)

Respondents to a survey by global management consulting firm McKinsey & Company found 63% of organizations said they used open models.

In [another blog post in May](https://www.interconnects.ai/p/what-people-get-wrong-about-the-leading?utm_source=publication-search), Lambert also notes a reluctance to deploy China’s open models in enterprise solutions, suggesting the concern is “the information hazards of indirect influence of Chinese values on Western business systems,” as well as questions about whether it’s been proven that they’re safe to run. But the project’s site makes another argument: that America “must lead AI research globally, and we must invest in making the tools our researchers need to do their job here in America: a suite of leading, open foundation models that can re-establish the strength of the research ecosystem.” (This will “drive research into fundamental AI advances,” while also securing America’s AI stack and maximizing its share of the AI market.)

“America’s AI leadership was built by being the global hub and leading producer of open AI research,” explained the website, “research which led directly to innovations like the Transformer architecture, ChatGPT, and the latest innovations in reasoning models and agents.” So a lack of open models would lead to a lack of leadership — and the dire consequences that would follow.

“AI leadership is increasingly tied to economic competitiveness, military capabilities, and technological sovereignty. Countries that lead in AI will have significant advantages in everything from economic productivity to defense capabilities, making this a critical national security issue.”

The website calls open-language models “crucial for long-term competition within American industry,” since closed AI labs “can only cover so many of the potential ideas … the broader, open research community focuses on innovations that’ll be transformative in 2, 5, 10, or more years.”

TNS analyst Hecht thinks the importance of foundational AI research is overblown, since in practice “it has taken second stage to engineering solutions in recent years.”

But the ATOM Project argues that closed models like ChatGPT and Claude “limit research in several critical ways: you can’t inspect their architecture, modify their behavior, finetune them for specific tasks, or understand their training process. Researchers need full access to model weights and code to conduct meaningful AI safety research, develop new capabilities, and build upon existing work.”

## OpenAI’s gpt-oss

The day after announcing the ATOM Project, Lambert found himself writing [a blog post about OpenAI’s two new gpt-oss open-weight models](https://www.interconnects.ai/p/gpt-oss-openai-validates-the-open), released under the Apache 2.0 license. Lambert called the release “a dramatic change in open model performance and strategy” and “a major moment for the ecosystem … I would give OpenAI a very strong grade on their first open release in a while — they definitely listened to the [feedback](https://natolambert.substack.com/p/some-thoughts-on-openai-returning) given by the community.”

“It’s a phenomenal step for the open ecosystem, especially for the West and its allies, that the most known brand in the AI space has returned to openly releasing models. This is momentum and could be the start of the turning point of adoption and impact of open models relative to China. The U.S. and its allies will no longer be falling further and further behind, which was the main story of 2025, but we need to build on this momentum if we want to have competitive open models for all use cases in the order of months rather than years.”

The new OpenAI models revealed “far more of their technical stack than any release to date,” Lambert writes, and they were “sized to run efficiently on a range of hardware from consumer GPUs to the cloud.” But Lambert notes OpenAI didn’t release the model’s training data, base model, code or a technical report. And he points out that “Many people make noise about creating open models” — but always as a secondary objective. “The goal of The ATOM Project is to give an outlet for people like myself that want to make this project their number one priority.

“Models that are designed from first principles to be modifiable, interpretable and extendable is what will enable a new decade of AI research to be born. This needs base models, training details, convenient sizes and other little details that are missing from many recent open model releases, including OpenAI’s.”

And whatever form the future takes, Lambert sees a larger movement that we can all participate in. “The call to action here is simple,” Lambert wrote in his July 4th blog post. “Consider how you can slightly shift your decision-making to make The American DeepSeek more likely.

“This approach succeeds just as much by having one model at the end of it, as it does by having the community form better habits and norms around the way AI models are conceived, built, shared, and used.”

And the ATOM Project’s site is still asking its visitors to “consider how your expertise or resources might contribute to building the infrastructure America needs.”

VIDEO

[YOUTUBE.COM/THENEWSTACK

Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.

SUBSCRIBE](https://youtube.com/thenewstack?sub_confirmation=1)

*TNS Analyst Lawrence Hecht contributed to this post.*

[YOUTUBE.COM/THENEWSTACK

Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.

SUBSCRIBE](https://youtube.com/thenewstack?sub_confirmation=1)

Group
Created with Sketch.

[![](https://cdn.thenewstack.io/media/2023/11/82081813-7zddypfe_400x400.jpg)

David Cassel is a proud resident of the San Francisco Bay Area, where he's been covering technology news for more than two decades. Over the years his articles have appeared everywhere from CNN, MSNBC, and the Wall Street Journal Interactive...

Read more from David Cassel](https://thenewstack.io/author/destiny/)