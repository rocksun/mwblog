<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"
      xmlns:og="http://ogp.me/ns#"
      xmlns:fb="https://www.facebook.com/2008/fbml">
<head>
    <title>ML in Go with a Python sidecar - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="https://eli.thegreenplace.net/favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://eli.thegreenplace.net/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="https://eli.thegreenplace.net/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://eli.thegreenplace.net/theme/css/pygments/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="https://eli.thegreenplace.net/theme/css/style.css" type="text/css"/>

        <link href="https://eli.thegreenplace.net/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Eli Bendersky's website ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://eli.thegreenplace.net/" class="navbar-brand">
                <img src="https://eli.thegreenplace.net/images/logosmall.png" width="32" height="32"/>
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="https://eli.thegreenplace.net/pages/about">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="https://eli.thegreenplace.net/pages/projects">
                        <i class="fa fa-github"></i>
                        <span class="icon-label">Projects</span>
                    </a>
                </li>
                <li>
                    <a href="https://eli.thegreenplace.net/archives/all">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://eli.thegreenplace.net/2024/ml-in-go-with-a-python-sidecar/"
                       rel="bookmark"
                       title="Permalink to ML in Go with a Python sidecar">
                        ML in Go with a Python sidecar
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> November 11, 2024 at 06:13</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="https://eli.thegreenplace.net/tag/go">Go</a>
        ,
    <a href="https://eli.thegreenplace.net/tag/machine-learning">Machine Learning</a>
        ,
    <a href="https://eli.thegreenplace.net/tag/python">Python</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>Machine learning models are rapidly becoming more capable; how can we make
use of these powerful new tools in our Go applications?</p>
<p>For top-of-the-line commercial LLMs like ChatGPT, Gemini or Claude, the models
are exposed as language agnostic REST APIs. We can hand-craft
HTTP requests or use client libraries (SDKs) provided by the LLM vendors.
If we need more customized solutions, however, some challenges arise. Completely
bespoke models are typically trained in Python using tools like TensorFlow,
JAX or PyTorch that don't have real non-Python alternatives.</p>
<p>In this post, I will present some approaches for Go developers to use ML models
in their applications - with increasing level of customization. The summary up
front is that it's pretty easy, and we only have to deal with Python very
minimally, if at all - depending on the circumstances.</p>
<img alt="Go gopher with Python logo inside" class="align-center" src="https://eli.thegreenplace.net/images/2024/gopherpythonlogo.png" style="width: 219px;" />
<div class="section" id="internet-llm-services">
<h2>Internet LLM services</h2>
<p>This is the easiest category: multimodal services from Google, OpenAI
and others are available as REST APIs with convenient client libraries for
most leading languages (including Go), as well as third-party packages that
provide abstractions on top (e.g. <a class="reference external" href="https://github.com/tmc/langchaingo">langchaingo</a>).</p>
<p>Check out the official Go blog titled <a class="reference external" href="https://go.dev/blog/llmpowered">Building LLM-powered applications in
Go</a> that was published earlier this year.
I've written about it before on this blog as well:
<a class="reference external" href="https://eli.thegreenplace.net/2024/gemini-cli-access-gemini-models-from-the-command-line/">#1</a>,
<a class="reference external" href="https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/">#2</a>,
<a class="reference external" href="https://eli.thegreenplace.net/2023/using-gemini-models-from-go/">#3</a> etc.</p>
<p>Go is typically as well supported as other programming languages in this domain;
in fact, it's uniquely powerful for such applications because of its
network-native nature; quoting from the Go blog post:</p>
<blockquote>
Working with LLM services often means sending REST or RPC requests to a
network service, waiting for the response, sending new requests to other
services based on that and so on. Go excels at all of these, providing great
tools for managing concurrency and the complexity of juggling network
services.</blockquote>
<p>Since this has been covered extensively, let's move on to the more challenging
scenarios.</p>
</div>
<div class="section" id="locally-running-llms">
<h2>Locally-running LLMs</h2>
<p>There's a plethora of high-quality open models <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a> one can choose from to
run locally: Gemma, Llama, Mistral and many more. While these models aren't
quite as capable as the strongest commercial LLM services, they are often
surprisingly good and have clear benefits w.r.t. cost and privacy.</p>
<p>The industry has begun standardizing on some common formats for shipping and
sharing these models - e.g. GGUF from <a class="reference external" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>,
<a class="reference external" href="https://huggingface.co/docs/safetensors/en/index">safetensors from Hugging Face</a>
or the older <a class="reference external" href="https://github.com/onnx/">ONNX</a>.
Additionally, there are a number of excellent OSS tools that let us run such
models locally and expose a REST API for an experience that's very similar to
the OpenAI or Gemini APIs, including dedicated client libraries.</p>
<p>The best known such tool is probably <a class="reference external" href="https://ollama.com/">Ollama</a>; I've
written extensively about it in the past:
<a class="reference external" href="https://eli.thegreenplace.net/2024/the-life-of-an-ollama-prompt/">#1</a>,
<a class="reference external" href="https://eli.thegreenplace.net/2023/using-ollama-with-langchaingo/">#2</a>,
<a class="reference external" href="https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/">#3</a>.</p>
<img alt="Internals of ollama, showing service connecting to clients and loading GGUF" class="align-center" src="https://eli.thegreenplace.net/images/2024/ollama-internals.png" />
<p>Ollama lets us customize an LLM through a <a class="reference external" href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">Modelfile</a>,
which includes things like setting model parameters, system prompts etc. If we
fine-tuned a model <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a>, it can also be loaded into Ollama by specifying our
own GGUF file.</p>
<p>If you're running in a cloud environment, some vendors already have
off-the-shelf solutions like <a class="reference external" href="https://cloud.google.com/run/docs/tutorials/gpu-gemma2-with-ollama">GCP's Cloud Run integration</a> that
may be useful.</p>
<p>Ollama isn't the only player in this game, either; recently a new tool
emerged with a slightly different approach. <a class="reference external" href="https://github.com/Mozilla-Ocho/llamafile">Llamafile</a>
distributes the entire model as a single binary, which is portable across
several OSes and CPU architectures. Like Ollama, it provides REST APIs for the
model.</p>
<p>If such a customized LLM is a suitable solution for your project, consider just
running Ollama or Llamafile and using their REST APIs to communicate with the
model. If you need higher degrees of customization, read on.</p>
</div>
<div class="section" id="a-note-about-the-sidecar-pattern">
<h2>A note about the sidecar pattern</h2>
<p>Before we proceed, I want to briefly discuss the <a class="reference external" href="https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/">sidecar pattern</a>
of application deployment. That k8s link talks about <em>containers</em>, but the
pattern isn't limited to these. It applies to any software architecture in which
functionality is isolated across processes.</p>
<p>Suppose we have an application that requires some library functionality; using
Go as an example, we could find <a class="reference external" href="https://pkg.go.dev/">an appropriate package</a>, import it and be on our way. Suppose there's no
suitable Go package, however. If libraries exist with a C interface, we could
alternatively use <a class="reference external" href="https://go.dev/blog/cgo">cgo</a> to import it.</p>
<p>But say there's no C API either, for example if the functionality is only
provided by a language without a convenient exported interface. Maybe it's
in Lisp, or Perl, or... Python.</p>
<p>A very general solution could be to wrap the code we need in some kind of server
interface and run it as a separate process; this kind of process is called a
<em>sidecar</em> - it's launched specifically to provide additional functionality for
another process. Whichever inter-process communication (IPC) mechanism we use,
the benefits of this approach are many - isolation, security, language
independence, etc. In today's world of containers and orchestration this
approach is becoming increasingly more common; this is why many of the links
about sidecars lead to k8s and other containerized solutions.</p>
<img alt="Depiction of a motorcycle with a Gopher, with Python in a sidecar" class="align-center" src="https://eli.thegreenplace.net/images/2024/sidecar-go-py.png" style="width: 256px;" />
<p>The Ollama approach outlined in the previous section is one example of using
the sidecar pattern. Ollama provides us with LLM functionality but it runs as
a server in its own process.</p>
<p>The solutions presented in the rest of this post are more explicit and fully
worked-out examples of using the sidecar pattern.</p>
</div>
<div class="section" id="locally-running-llm-with-python-and-jax">
<h2>Locally-running LLM with Python and JAX</h2>
<p>Suppose none of the existing open LLMs will do for our project, even
fine-tuned. At this point we can consider training our own LLM - this
is hugely expensive, but perhaps there's no choice. Training usually involves
one of the large ML frameworks like TensorFlow, JAX or PyTorch. In this section
I'm not going to talk about how to train models; instead, I'll show how to run
local inference of an already trained model - in Python with JAX, and use that
as a sidecar server for a Go application.</p>
<p>The sample (<a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/jax-gemma-server">full code is here</a>)
is based on the
<a class="reference external" href="https://github.com/google-deepmind/gemma/">official Gemma repository</a>,
using its <em>sampler</em> library <a class="footnote-reference" href="#footnote-3" id="footnote-reference-3">[3]</a>. It comes with a README that explains how
to set everything up. This is the relevant code instantiating a Gemma
sampler:</p>
<div class="highlight"><pre><span></span><span class="c1"># Once initialized, this will hold a sampler_lib.Sampler instance that</span>
<span class="c1"># can be used to generate text.</span>
<span class="n">gemma_sampler</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">def</span> <span class="nf">initialize_gemma</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Initialize Gemma sampler, loading the model into the GPU.&quot;&quot;&quot;</span>
    <span class="n">model_checkpoint</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;MODEL_CHECKPOINT&quot;</span><span class="p">)</span>
    <span class="n">model_tokenizer</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;MODEL_TOKENIZER&quot;</span><span class="p">)</span>

    <span class="n">parameters</span> <span class="o">=</span> <span class="n">params_lib</span><span class="o">.</span><span class="n">load_and_format_params</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Parameters loaded&quot;</span><span class="p">)</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">()</span>
    <span class="n">vocab</span><span class="o">.</span><span class="n">Load</span><span class="p">(</span><span class="n">model_tokenizer</span><span class="p">)</span>
    <span class="n">transformer_config</span> <span class="o">=</span> <span class="n">transformer_lib</span><span class="o">.</span><span class="n">TransformerConfig</span><span class="o">.</span><span class="n">from_params</span><span class="p">(</span>
        <span class="n">parameters</span><span class="p">,</span>
        <span class="n">cache_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">transformer</span> <span class="o">=</span> <span class="n">transformer_lib</span><span class="o">.</span><span class="n">Transformer</span><span class="p">(</span><span class="n">transformer_config</span><span class="p">)</span>

    <span class="k">global</span> <span class="n">gemma_sampler</span>
    <span class="n">gemma_sampler</span> <span class="o">=</span> <span class="n">sampler_lib</span><span class="o">.</span><span class="n">Sampler</span><span class="p">(</span>
        <span class="n">transformer</span><span class="o">=</span><span class="n">transformer</span><span class="p">,</span>
        <span class="n">vocab</span><span class="o">=</span><span class="n">vocab</span><span class="p">,</span>
        <span class="n">params</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sampler ready&quot;</span><span class="p">)</span>
</pre></div>
<p>The model weights and tokenizer vocabulary are files downloaded
<a class="reference external" href="https://www.kaggle.com/models/google/gemma">from Kaggle</a>, per the
instructions in the Gemma repository README.</p>
<p>So we have LLM inference up and running in Python; how do we use it from
Go?</p>
<p>Using a sidecar, of course. Let's whip up a quick web server around this model
and expose a trivial REST interface on a local port that Go (or any other
tool) can talk to. As an example, I've set up a Flask-based web server around
this inference code. The web server is invoked with <a class="reference external" href="https://gunicorn.org/">gunicorn</a> - see the
<a class="reference external" href="https://github.com/eliben/code-for-blog/blob/main/2024/go-py-ml/jax-gemma-server/run-gemma-server.sh">shell script</a> for details.</p>
<p>Excluding the imports, here's the entire application code:</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_app</span><span class="p">():</span>
    <span class="c1"># Create an app and perform one-time initialization of Gemma.</span>
    <span class="n">app</span> <span class="o">=</span> <span class="n">Flask</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">app</span><span class="o">.</span><span class="n">app_context</span><span class="p">():</span>
        <span class="n">initialize_gemma</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">app</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">create_app</span><span class="p">()</span>

<span class="c1"># Route for simple echoing / smoke test.</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s2">&quot;/echo&quot;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;POST&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">echo</span><span class="p">():</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">json</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;echo_prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>


<span class="c1"># The real route for generating text.</span>
<span class="nd">@app</span><span class="o">.</span><span class="n">route</span><span class="p">(</span><span class="s2">&quot;/prompt&quot;</span><span class="p">,</span> <span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;POST&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">prompt</span><span class="p">():</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">json</span><span class="p">[</span><span class="s2">&quot;prompt&quot;</span><span class="p">]</span>

    <span class="c1"># For total_generation_steps, 128 is a default taken from the Gemma</span>
    <span class="c1"># sample. It&#39;s a tradeoff between speed and quality (higher values mean</span>
    <span class="c1"># better quality but slower generation).</span>
    <span class="c1"># The user can override this value by passing a &quot;sampling_steps&quot; key in</span>
    <span class="c1"># the request JSON.</span>
    <span class="n">sampling_steps</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;sampling_steps&quot;</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>

    <span class="n">sampled_str</span> <span class="o">=</span> <span class="n">gemma_sampler</span><span class="p">(</span>
        <span class="n">input_strings</span><span class="o">=</span><span class="p">[</span><span class="n">prompt</span><span class="p">],</span>
        <span class="n">total_generation_steps</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">sampling_steps</span><span class="p">),</span>
    <span class="p">)</span><span class="o">.</span><span class="n">text</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;response&quot;</span><span class="p">:</span> <span class="n">sampled_str</span><span class="p">}</span>
</pre></div>
<p>The server exposes two routes:</p>
<ul class="simple">
<li><tt class="docutils literal">prompt</tt>: a client sends in a textual prompt, the server runs Gemma
inference and returns the generated text in a JSON response</li>
<li><tt class="docutils literal">echo</tt>: used for testing and benchmarking</li>
</ul>
<p>Here's how it all looks tied together:</p>
<img alt="Flask server wrapping Gemma sampling and exposing REST" class="align-center" src="https://eli.thegreenplace.net/images/2024/jax-gemma-server.png" style="width: 500px;" />
<p>The important takeaway is that this is just an example. Literally any part of
this setup can be changed: one could use a different ML library (maybe PyTorch
instead of JAX); one could use a different model (not Gemma, not even an LLM)
and one can use a different setup to build a web server around it. There are
many options, and each developer will choose what fits their project best.</p>
<p>It's also worth noting that we've written less than 100 lines of Python code
in total - much of it piecing together snippets from tutorials. This tiny amount
of Python code is sufficient to wrap an HTTP server with a simple REST interface
around an LLM running locally through JAX on the GPU. From here on, we're safely
back in our application's actual business logic and Go.</p>
<p>Now, a word about performance. One of the concerns developers may have with
sidecar-based solutions is the performance overhead of IPC
between Python and Go. I've added a simple <tt class="docutils literal">echo</tt> endpoint to measure this
effect; take a look at the <a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/jax-gemma-server/measure-request-latency">Go client that exercises it</a>;
on my machine the
latency of sending a JSON request from Go to the Python server and getting back
the echo response is about 0.35 ms on average. Compared to the time it takes
Gemma to process a prompt and return a response (typically measured in seconds,
or maybe hundreds of milliseconds on very powerful GPUs), this is entirely
negligible.</p>
<p>That said, not every custom model you may need to run is a full-fledged LLM.
What if your model is small and fast, and the overhead of 0.35 ms becomes
significant? Worry not, it can be optimized. This is the topic of the next
section.</p>
</div>
<div class="section" id="locally-running-fast-image-model-with-python-and-tensorflow">
<h2>Locally-running fast image model with Python and TensorFlow</h2>
<p>The final sample of this post mixes things up a bit:</p>
<ul class="simple">
<li>We'll be using a simple image model (instead of an LLM)</li>
<li>We're going to train it ourselves using TensorFlow+Keras (instead of JAX)</li>
<li>We'll use a different IPC method between the Python sidecar
server and clients (instead of HTTP+REST)</li>
</ul>
<p>The model is still implemented in Python, and it's still driven as a sidecar
server process by a Go client <a class="footnote-reference" href="#footnote-4" id="footnote-reference-4">[4]</a>. The idea here is to show the versatility of
the sidecar approach, and to demonstrate a lower-latency way to communicate
between the processes.</p>
<p>The full code of the sample <a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/tf-cifar-server">is here</a>.
It trains a simple CNN
(convolutional neural network) to classify images from the
<a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>:</p>
<img alt="CIFAR-10 dataset sample" class="align-center" src="https://eli.thegreenplace.net/images/2024/cifar10.png" />
<p>The neural net setup with TensorFlow and Keras was taken from an official
tutorial. Here's the entire network definition:</p>
<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">MaxPooling2D</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Flatten</span><span class="p">())</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
<p>CIFAR-10 images are 32x32 pixels, each pixel being 3 values for red, green
and blue. In the original dataset, these values are bytes in the inclusive
range 0-255 representing color intensity. This should explain the
<tt class="docutils literal">(32, 32, 3)</tt> shape appearing in the code. The full code for training the
model is in the <tt class="docutils literal">train.py</tt> file in the sample; it runs for a bit and saves the
serialized model along with the trained weights into a local file.</p>
<p>The next component is an &quot;image server&quot;: it loads the trained model+weights
file from disk and runs inference on images passed into it, returning the
label the model thinks is most likely for each.</p>
<p>The server doesn't use HTTP and REST, however. It creates a
<a class="reference external" href="https://en.wikipedia.org/wiki/Unix_domain_socket">Unix domain socket</a>
and uses a simple length-prefix encoding protocol to communicate:</p>
<img alt="Length-prefix packet format" class="align-center" src="https://eli.thegreenplace.net/images/2024/length-prefix-packet.png" style="width: 663px;" />
<p>Each packet starts with a 4-byte field that specifies the length of the rest
of the contents. A type is a single byte, and the body can be anything <a class="footnote-reference" href="#footnote-5" id="footnote-reference-5">[5]</a>.
In the sample image server two commands are currently supported:</p>
<ul class="simple">
<li>0 means &quot;echo&quot; - the server will respond with the same packet back to
the client. The contents of the packet body are immaterial.</li>
<li>1 means &quot;classify&quot; - the packet body is interpreted as a 32x32 RGB image,
encoded as the red channel for each pixel in the first 1024 bytes (32x32,
<a class="reference external" href="https://eli.thegreenplace.net/2015/memory-layout-of-multi-dimensional-arrays">row major</a>),
then green in the next 1024 bytes and finally blue in the last 1024
bytes. Here the server will run the image through the model, and reply with
the label the model thinks describes the image.</li>
</ul>
<p>The sample also includes <a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml/tf-cifar-server/client">a simple Go client</a>
that can take a PNG file
from disk, encode it in the required format and send it over the domain socket
to the server, recording the response.</p>
<p>The client can also be used to benchmark the latency of a roundtrip message
exchange. It's easier to just show the code instead of explaining what it does:</p>
<div class="highlight"><pre><span></span><span class="kd">func</span><span class="w"> </span><span class="nx">runBenchmark</span><span class="p">(</span><span class="nx">c</span><span class="w"> </span><span class="nx">net</span><span class="p">.</span><span class="nx">Conn</span><span class="p">,</span><span class="w"> </span><span class="nx">numIters</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="c1">// Create a []byte with 3072 bytes.</span><span class="w"></span>
<span class="w">  </span><span class="nx">body</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nb">make</span><span class="p">([]</span><span class="kt">byte</span><span class="p">,</span><span class="w"> </span><span class="mi">3072</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">body</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">body</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nb">byte</span><span class="p">(</span><span class="nx">i</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">256</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="nx">t1</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">Now</span><span class="p">()</span><span class="w"></span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="k">range</span><span class="w"> </span><span class="nx">numIters</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="nx">sendPacket</span><span class="p">(</span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="nx">messageTypeEcho</span><span class="p">,</span><span class="w"> </span><span class="nx">body</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="nx">cmd</span><span class="p">,</span><span class="w"> </span><span class="nx">resp</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">readPacket</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">cmd</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">resp</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="nx">body</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="nx">log</span><span class="p">.</span><span class="nx">Fatal</span><span class="p">(</span><span class="s">&quot;bad response&quot;</span><span class="p">)</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="nx">elapsed</span><span class="w"> </span><span class="o">:=</span><span class="w"> </span><span class="nx">time</span><span class="p">.</span><span class="nx">Since</span><span class="p">(</span><span class="nx">t1</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&quot;Num packets: %d, Elapsed time: %s\n&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">numIters</span><span class="p">,</span><span class="w"> </span><span class="nx">elapsed</span><span class="p">)</span><span class="w"></span>
<span class="w">  </span><span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&quot;Average time per request: %d ns\n&quot;</span><span class="p">,</span><span class="w"> </span><span class="nx">elapsed</span><span class="p">.</span><span class="nx">Nanoseconds</span><span class="p">()</span><span class="o">/</span><span class="nb">int64</span><span class="p">(</span><span class="nx">numIters</span><span class="p">))</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
<p>In my testing, the average latency of a roundtrip
is about 10 Î¼s (that's <em>micro</em>-seconds). Considering the size of the message
and it being Python on the other end, this is roughly in-line with my
<a class="reference external" href="https://eli.thegreenplace.net/2019/unix-domain-sockets-in-go/">earlier benchmarking of Unix domain socket latency in Go</a>.</p>
<p>How long does a single image inference take with this model? In my measurements,
about 3 ms. Recall that the communication latency for the HTTP+REST approach was
0.35 ms; while this is only 12% of the image inference time, it's close enough
to be potentially worrying. On a beefy server-class GPU the time can be much
shorter <a class="footnote-reference" href="#footnote-6" id="footnote-reference-6">[6]</a>.</p>
<p>With the custom protocol over domain sockets, the latency - being 10 Î¼s -
seems quite negligible no matter what you end up running
on your GPU.</p>
</div>
<div class="section" id="code">
<h2>Code</h2>
<p>The full code for the samples in this post is <a class="reference external" href="https://github.com/eliben/code-for-blog/tree/main/2024/go-py-ml">on GitHub</a>.</p>
<hr class="docutils" />
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>To be pedantic, these models are not entirely open: their inference
architecture is open-source and their weights are available, but the
details of their training remain proprietary.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>The details of fine-tuning models are beyond the scope of this post,
but there are plenty resources about this online.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[3]</a></td><td>&quot;Sampling&quot; in LLMs means roughly &quot;inference&quot;. A trained model is
fed an input prompt and then &quot;sampled&quot; to produce its output.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-4" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-4">[4]</a></td><td>In my samples, the Python server and Go client simply run in different
terminals and talk to each other. How service management is structured
is very project-specific. We could envision an approach wherein the
Go application launches the Python server to run in the background and
communicates with it. Increasingly likely these days, however, would
be a container-based setup, where each program is its own container
and an orchestration solution launches and manages these containers.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-5">[5]</a></td><td>You may be wondering why I'm implementing a custom protocol here instead
of using something established. In real life, I'd definitely recommend
using something like gRPC. However, for the sake of this sample I wanted
something that would be (1) simple without additional libraries and
(2) very fast. FWIW, I don't think the latency numbers would be very
much different for gRPC. Check out my
<a class="reference external" href="https://eli.thegreenplace.net/2019/unix-domain-sockets-in-go/">earlier post about RPC over Unix domain sockets in Go</a>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-6">[6]</a></td><td>On the other hand, the model I'm running here is <em>really</em> small.
It's fair to say realistic models you'll use in your application
will be much larger and hence slower.</td></tr>
</tbody>
</table>
</div>

            </div>
            <!-- /.entry-content -->
<hr/>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            &copy; 2003-2024 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="//code.jquery.com/jquery-2.2.4.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://eli.thegreenplace.net/theme/js/bootstrap.min.js"></script>

<!--
  Using goatcounter to count visitors. The count.js script is vendored in.
-->
<script data-goatcounter="https://stats.thegreenplace.net/count"
        async src="https://eli.thegreenplace.net/theme/js/count.js"></script>
</body>
</html>