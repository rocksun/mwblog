# The Case for Telemetry Pipelines
![Featued image for: The Case for Telemetry Pipelines](https://cdn.thenewstack.io/media/2025/01/320af1bd-pipelines-1024x576.jpeg)
In a [survey last year](https://chronosphere.io/learn/observability-log-data-trends/), observability and security professionals reported a 250% year-over-year increase in logs alone.

This isn’t surprising to those working with containerized apps in clusters across clouds — these types of systems emit so much data. What it means for you is that understanding what is happening in production becomes a headache as you wade through more and more metrics, logs, traces and events.

A telemetry pipeline can make sure that you have relevant data at your fingertips. Scaling [observability](https://thenewstack.io/observability/) isn’t just about collecting more data; it’s about making sure that data is useful and [cost-effective](https://thenewstack.io/4-unexpected-costs-of-unreliable-observability/).

Without a telemetry pipeline, you’re dealing with brittle dependencies, ingestion failures leading to dropped data, inconsistent third-party telemetry and [skyrocketing costs](https://www.youtube.com/watch?v=UAxvLK_ZYYU). A pipeline gives you the flexibility to route telemetry where you need it, buffer it for reliability, standardize formats, and optimize what you store so you get the insights you need without the baggage.

Below is my stab at walking you through what you need to know about telemetry pipelines: what they are, why they matter and how they can solve common observability challenges.

## What Is a Telemetry Pipeline?
At first, I assumed “telemetry pipeline” was just another trendy buzzword. After all, we have been getting logs and metrics from our apps and into monitoring systems for years before this term burst on the scene. So what is it really?

A telemetry pipeline is a system that collects, processes and routes telemetry data (logs, metrics and traces) from various sources to the right monitoring and analysis tools. Instead of managing separate agents or collectors for different signals, a telemetry pipeline unifies data handling, making observability more efficient and scalable.

Before diving in further, let’s rewind to get crystal clear on what “telemetry” even means.

At its core, telemetry is the automated collection and transmission of data from remote systems for monitoring and analysis. You see telemetry in action all the time — for example, a fitness tracker monitoring your heart rate and sending that data to an app. In software, telemetry refers to the logs, metrics, traces and events generated by applications, infrastructure and cloud services that are sent onto [monitoring and observability](https://thenewstack.io/monitoring-vs-observability-whats-the-difference/) systems.

Today’s cloud native systems generate a constant stream of telemetry. Microservices, containers, clusters and serverless functions all emit events, logs, metrics and traces, each in different formats, at varying rates and with unique processing needs. This data often needs to be filtered, enriched and routed to multiple monitoring, alerting and analytics tools. Managing it manually or with fragmented solutions quickly becomes unsustainable.

That’s where a telemetry pipeline comes in.

**Support for Multiple Signals**
In the past, you had to run different components to collect each signal type — one for logs, another for metrics and yet another for traces. A telemetry pipeline simplifies things by handling all these data types in a single binary. You can still run separate pipelines for logs, metrics and traces. But instead of learning three different technologies, you’re using the same telemetry pipeline for all of them. So when you learn how to configure and deploy it once, that knowledge carries over, making things a lot easier.

**Collect, Process, Route**
A telemetry pipeline does three main things:

**Collects**data from anywhere — your apps, databases, cloud services or whatever else you’re running. The more ways it can ingest data, the better. Think of it as a universal receiver that speaks everyone’s language.**Processes**data by cleaning it up, adding context, filtering noise and masking sensitive bits. This is where raw data becomes actually useful.**Routes**data delivering everything where it needs to go, whether that’s analytics tools, long-term storage, alerting systems or anywhere else your team needs it.
At its core, a telemetry pipeline is about getting the right data to the right place in the right format. That’s powerful, but what does it enable in practice?

## Why Telemetry Pipelines Matter
### Decoupling Your Observability From Any Single Vendor
Sending telemetry directly to a vendor might seem simple until you need to switch tools, control costs or route data differently. Without a telemetry pipeline, you’re either locked into a single vendor’s ecosystem or managing a patchwork of agents and collectors that don’t always work well together.

A telemetry pipeline acts as a middleware layer, giving you the flexibility to migrate between vendors without changing application instrumentation. You can add or remove destinations through configuration and route telemetry to multiple backends so different teams can use the tools that fit their needs. It also allows you to transform data formats on the fly, ensuring compatibility across observability platforms without modifying application code.

### Building Resiliency Into Your Observability Stack
Telemetry needs to be reliable, especially during incidents, but direct ingestion can introduce points of failure, whether from network disruptions, rate limits or temporary outages. A telemetry pipeline adds resiliency by buffering telemetry with queues that can handle traffic spikes, retry failed deliveries to prevent data loss, and enable local observability tooling for redundancy. This ensures that even if an endpoint is temporarily unreachable, you still have access to critical data to understand your system when you need it most.

### Managing Telemetry You Don’t Control
Today’s applications rely on more than just your own code. They pull in libraries from third-party services, run on managed cloud platforms and integrate with legacy systems, each with its own format and level of detail. Since you can’t always control how this telemetry is generated, making sense of it can be a challenge.

A telemetry pipeline standardizes data across sources, enriching it with missing metadata and filtering out noise so only the most relevant signals remain. Instead of wrestling with fragmented, inconsistent telemetry, you get a clean, structured data set that’s easy to work with, no matter where it came from.

### Managing Costs Without Sacrificing Visibility
Observability is only useful if you have the right data when you need it. Yet, when costs start to spiral, organizations tend to overcorrect, limiting what gets collected and making developers hesitant to add necessary instrumentation. The problem isn’t just cost; it’s losing critical visibility.

A telemetry pipeline gives you control over the volume and shape of your data, so you’re paying for telemetry that’s actually valuable, not just whatever gets emitted by default. By filtering out noise, reducing high-cardinality metrics and sampling traces intelligently, you get the insights you need while cutting down on the data that doesn’t serve you. Instead of worrying about cost constraints, you can focus on instrumenting your code properly, knowing that your pipeline ensures the most relevant data is always available.

Observability isn’t just a technical concern; it’s a business investment in system reliability and diagnosability. A telemetry pipeline ensures that investment is working for you — not against you — by making telemetry data more flexible, resilient and valuable. With the right foundation in place, the next step is understanding how it helps teams solve real-world observability challenges.

## Getting Started With Telemetry Pipelines
So what to do now? The next step is putting your knowledge into action. Whether you’re aiming to reduce vendor lock-in, improve data quality or get better control over costs, a telemetry pipeline doesn’t have to be an all-or-nothing effort. You can start small and evolve alongside your applications, giving you more flexibility and insight with each iteration.

If you’re considering implementing a telemetry pipeline, a great first step is to talk with your fellow engineers in development, operations and security. Look for shared friction points — maybe debugging is slowed down by inconsistent logs, or maybe monitoring costs are rising without clear visibility into what’s actually useful. Once you’ve identified a common pain point, start with a single telemetry type, like logs, and build from there. If you’re itching to get started and want to experiment locally, this [Cloud Native Telemetry Pipelines Workshop](https://o11y-workshops.gitlab.io/workshop-fluentbit/) is a great hands-on tutorial.

[
YOUTUBE.COM/THENEWSTACK
Tech moves fast, don't miss an episode. Subscribe to our YouTube
channel to stream all our podcasts, interviews, demos, and more.
](https://youtube.com/thenewstack?sub_confirmation=1)